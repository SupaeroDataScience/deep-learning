{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVEEazxFZgr2",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjpQefesZgr4"
   },
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1VXs8S8Zgr7"
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGMK6qekZgr9"
   },
   "source": [
    "# 1. Text classification in English\n",
    "\n",
    "## 1.1 Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26qQSlBNZpZY"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/SupaeroDataScience/deep-learning\n",
    "#!mv deep-learning/data .\n",
    "#!mv deep-learning/NLP/datasets .\n",
    "#!pip install nltk unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYwNdKZiZgr9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = 'data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = 'data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBPuFTw2ZgsA"
   },
   "source": [
    "## 1.2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKLxbfC-ZgsB"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eLR5hi3ZgsC"
   },
   "outputs": [],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H9GRQxXZgsC"
   },
   "source": [
    "## 1.3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbwk9Mp0ZgsD"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLBm1kLIZgsE"
   },
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXUNGD9AZgsG"
   },
   "source": [
    "The LemmaTokenizer defined above will be applied further in this example. The next step is to define the Count Vectorization pipeline using this Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa66gFMlZgsG"
   },
   "outputs": [],
   "source": [
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "bow = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIKuiDWSZgsH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb-3Z74eZgsI"
   },
   "source": [
    "## 1.4. Using the bag of words (BOW) object to classify spam\n",
    "\n",
    "Let's start by splitting the data into train and test sets, using 20% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf-54hwSZgsI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow,email_label,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rS7EDB-ZgsI"
   },
   "source": [
    "In this simple example we will use a Logistic Regression Classifier. Let's fit it to our Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxD1T5OiZgsJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr_classifier=LogisticRegression()\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKsqSjg0ZgsJ"
   },
   "source": [
    "In many cases, Bag of Words can provide sufficient information for classification. In this case, the accuracy reached by our classifier is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naewZmg0ZgsK"
   },
   "source": [
    "## 1.5. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9imM6DqZgsK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "\n",
    "emailBagOfWords = {feat2word[i]: bow[mail_number, i] for i in bow[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", bow[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(bow[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCV1BC1RZgsL"
   },
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNuCQwo8ZgsL"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(bow)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhKhbIoeZgsM"
   },
   "source": [
    "Let's run the classification process again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NkGrarpZgsM"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf,email_label,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_icBmOuZgsM"
   },
   "source": [
    "In this simplae case, additional filtering is unecessary and even removed some information. There is indeed likely a link between the abundance of words/long emails and the fact that this email is a spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcZVAEwsZgsN"
   },
   "source": [
    "# 2. Text classification in French\n",
    "\n",
    "The previously used dataset is a widely used dataset for introductory text classification. \n",
    "\n",
    "The field of Natural Language Understanding, and Natural Language Classification in particular, suffers from two challenges :\n",
    "- Adapting the features and methodologies to various and more complex datasets\n",
    "- Adapting the process to languages other than english\n",
    "\n",
    "Concerning the latter, one has to take into account that most of NLU research is currently performed on english. Datasets are rarely available for other languages, and the algorithms proposed for better NLU are often left untested on foreign data. \n",
    "French, for instance, has less efficient lemmatization (french is a richly flected language). In the following section, we will reuse the same methodologies on a french dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TzD5FKrZgsN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load video games reviews\n",
    "vgr = pd.read_csv(\"datasets/jvc.csv\")\n",
    "vgr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17I9wrn8ZgsN"
   },
   "outputs": [],
   "source": [
    "#convert rating to numeric values, and plot the histogram of values\n",
    "rating=vgr.website_rating.apply(lambda k: k[:-3])\n",
    "vgr['rating']=pd.to_numeric(rating)\n",
    "vgr.rating.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB1HYixvZgsO"
   },
   "source": [
    "Most games seem to have a rating between 11 and 16. In this exercise, we will try to determine if we can determine if a game is very good (rating above 16) or very bad (rating below 11) based only on the summary of its review.\n",
    "\n",
    "Let's start by splitting the dataset between good and bad games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZWhweYuZgsO"
   },
   "outputs": [],
   "source": [
    "bad=vgr[(vgr.rating<=11) & (vgr.platform==\"PC\")]\n",
    "bad['quality']=pd.Series([\"bad\"]*len(bad.index),index=bad.index)\n",
    "good=vgr[(vgr.rating>=16) & (vgr.platform==\"PC\")]\n",
    "good['quality']=pd.Series([\"good\"]*len(good.index),index=good.index)\n",
    "selected_games=pd.concat([good,bad]).dropna()\n",
    "\n",
    "#Keep only reviews and \n",
    "game_reviews=selected_games['description']\n",
    "game_quality=selected_games['quality']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRuc6EaNZgsO"
   },
   "source": [
    "Lemmatization in French is a tricky issue.\n",
    "\n",
    "One example : the verb finir can be expressed as finissons, finirez, finisse, finit, etc...\n",
    "Lemmatization is typically less efficient in french than in english. \n",
    "\n",
    "Another alternative is to use Stemming instead. Stemming uses RegEx rules to truncate the end of a word that would normally correspond to conjugations, inflections, etc...\n",
    "Stemming destructs the readability of the words by truncating their end, but runs faster than Lemmatization\n",
    "\n",
    "In the next cell, we adapt the LemmaTokenizer that we defined earlier using a FrenchStemmer instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA9TEMYXZgsO"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class FrenchStemTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.st = FrenchStemmer()\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.st.stem(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(tokenizer=FrenchStemTokenizer(remove_non_words=True))\n",
    "bow_games = countvect.fit_transform(game_reviews)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirJF0KjZgsP"
   },
   "source": [
    "### Classify with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbgEZqdpZgsP"
   },
   "outputs": [],
   "source": [
    "print(\"Number of documents:\", len(game_reviews))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow_games.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54z4k0VpZgsP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_games,game_quality,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLqa6tFTZgsQ"
   },
   "source": [
    "### Classify using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoDUgqKgZgsQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_games = TfidfTransformer().fit_transform(bow_games)\n",
    "tfidf_games.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLCm-xlzZgsQ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_games,game_quality,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYmgT33VZgsR"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "up7NxbxnZgsR"
   },
   "source": [
    "Tfidf and Bow are usually very efficient features to manipulate during cassification. However, please note that their size is directly related to the size of the vocabulary of our corpus. \n",
    "\n",
    "In our current example, even when using stemming, the dimensionnality of our bow or tfidf vectors is still very high (3161). This is not maintaintable with increasing corpora sizes.\n",
    "\n",
    "In this sction, we will use the word2vec embeddings, that address this issue by proposing an architecture that learns individual representations for words in a vector space of given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L38iGShaZgsS"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "\n",
    "class FrenchTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [unidecode.unidecode(t) for t in word_list]\n",
    "\n",
    "tok=FrenchTokenizer()\n",
    "\n",
    "text_for_word2vec=[tok(sent) for sent in game_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRaGsoecZgsS"
   },
   "source": [
    "The operation above will tokenize all texts by keeping stemmed tokens. Please note the following choices :\n",
    "- we have applied stemming in order to reduce the dimensionality of our feature space\n",
    "- we have removed stop words, in order to not let context be learned with it. (depending on the use case, you may want to keep them or remove them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOWsPngIZgsS"
   },
   "source": [
    "We can now train the Word2Vec model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XliGqhWMZgsT"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model=Word2Vec(text_for_word2vec,size=200,window=5,min_count=1)\n",
    "model.save(\"word2vec.model\")\n",
    "w2v=dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXaVjIVMZgsT"
   },
   "source": [
    "Let's check word similarity in our trained data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0y6VW1dZgsT"
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=\"jeu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDKaY4dJZgsU"
   },
   "source": [
    "Let's now try again to classify our samples using these embddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVj4nK80ZgsU"
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self,word2vec,dim):\n",
    "        self.word2vec=word2vec\n",
    "        self.dim=dim\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_n-EJW5ZgsV"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(game_reviews,game_quality,test_size=0.2)\n",
    "\n",
    "pipe=Pipeline([('vectorizer',MeanEmbeddingVectorizer(w2v,200)),('classifier',lr_classifier)])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tg9TYPRdZgsV"
   },
   "outputs": [],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNCJSWtaZgsW"
   },
   "source": [
    "What we observe here is that word2vec embeddings perform worse than what we learned from BOW or TFIDF. \n",
    "\n",
    "In our case, the training corpus for the embeddings was not large enough to ensure proper convergence and representation of the words.\n",
    "\n",
    "It is also common that for smaller corpora (<10.000 docs approximately), TFIDF usually performs better for classification, whereas Word2Vec produces better results with larger corpora and across domains (e.g. training on data from Wikipedia, and then using the vectors on data from another field)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
