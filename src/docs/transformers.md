# Transformers

* [Home](https://supaerodatascience.github.io/deep-learning/)
* [Github repository](https://github.com/SupaeroDataScience/deep-learning/)

This class introduces attention mechanisms in detail and presents the Transformer architecture, with an example in NLP. It is very tied to the [NLP class](https://supaerodatascience.github.io/deep-learning/NLP.html).

[Attention notebook source](https://github.com/SupaeroDataScience/deep-learning/blob/main/transformers/Neural_Self_Attention_.ipynb)

[Attention notebook on Colab](https://colab.research.google.com/github/SupaeroDataScience/deep-learning/blob/main/transformers/Neural_Self_Attention_.ipynb)

[Mini Transformer notebook source](https://github.com/SupaeroDataScience/deep-learning/blob/main/transformers/Mini_Transformer.ipynb)

[Mini Transformer notebook on Colab](https://colab.research.google.com/github/SupaeroDataScience/deep-learning/blob/main/transformers/Mini_Transformer.ipynb)

## Additional Resources

+ [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
+ [Illustrated Transformer notebook](https://github.com/SupaeroDataScience/deep-learning/blob/main/transformers/Transformer_Architecture_2025.ipynb), version [on Colab](https://colab.research.google.com/github/SupaeroDataScience/deep-learning/blob/main/transformers/Transformer_Architecture_2025.ipynb)
+ Vaswani, Ashish, et al. "Attention is all you need." Advances in Neural Information Processing Systems 30, 2017. [pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
+ Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901. [pdf](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
+ Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." J. Mach. Learn. Res. 21.140 (2020): 1-67. [pdf](https://jmlr.org/papers/volume21/20-074/20-074.pdf)
+ Hoffmann, Jordan, et al. "Training Compute-Optimal Large Language Models." arXiv preprint arXiv:2203.15556 (2022). [pdf](https://arxiv.org/pdf/2203.15556.pdf)

## Vision Transformers

+ Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020). [pdf](https://arxiv.org/pdf/2010.11929.pdf)
+ [ViT - Papers with code](https://paperswithcode.com/method/vision-transformer)
+ Wightman, Ross, Hugo Touvron, and Hervé Jégou. "Resnet strikes back: An improved training procedure in timm." arXiv preprint arXiv:2110.00476 (2021). [pdf](https://arxiv.org/pdf/2110.00476.pdf)
+ Liu, Zhuang, et al. "A convnet for the 2020s." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. [pdf](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf)
