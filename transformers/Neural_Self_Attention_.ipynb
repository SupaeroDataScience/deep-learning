{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XQ6NsIuDtgr"
   },
   "source": [
    "# Neural Self-Attention\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson<br> https://supaerodatascience.github.io/deep-learning/\n",
    "\n",
    "\n",
    "Based on [medium article](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) by [Raimi Karim](https://towardsdatascience.com/@remykarem)\n",
    "\n",
    "Colab version by [Manuel Romero](https://twitter.com/mrm8488)\n",
    "\n",
    "Additional illustrations by [Jay Alammar](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tYtjfCEKUhJ"
   },
   "source": [
    "## Background\n",
    "\n",
    "Attention modules were first introduced in neural networks as a way to index the latent space of a recurrent network, although similar ideas were used in Neural Turing Machines to index an external memory. The idea in both is the same: a mask which specifies which part of a sequence to pay **attention** to. \n",
    "\n",
    "+ Alex Graves, Greg Wayne, and Ivo Danihelka. \"Neural turing machines.\" CoRR, abs/1410.5401, 2014. [pdf](https://arxiv.org/pdf/1410.5401.pdf)\n",
    "\n",
    "+ Bahdanau, Dzmitry, Kyung Hyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\" 3rd International Conference on Learning Representations, ICLR 2015. [pdf](https://arxiv.org/pdf/1409.0473.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYace5LeMjTq"
   },
   "source": [
    "![Self-attention example](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ObDFoE2KUnj"
   },
   "source": [
    "While originally used as part of recurrent or convolutional networks, Transformer networks rely entirely on attention modules to analyze sequences. Specifically, they use self-attention, where an entire sequence is presented for analysis. A self-attention module takes in n inputs, and returns n outputs. What happens in this module? Put simply, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\n",
    "\n",
    "+ Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems 30, 2017. [pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmelXHYHNK0t"
   },
   "source": [
    "A self-attention head has three weight matrices (represented as fully-connected NN layers) which compute a **query**, **key**, and **value** per input. The key and values can be though of as a dictionary the network has learned - for each key, it will save associations with the list of possible vaues. In a language model, this is often associated words, either based on order or context. The query is similar to the key, but it is for one input at a time, while the key is calculated over all inputs. The query will be used to calculate a score based on each key to find keys which have similar values. The score then weights the value vectors to output relevant values corresponding to each input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqaZmsgNOiyX"
   },
   "source": [
    "![Query, key and value](https://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAs7EotAO7Xh"
   },
   "source": [
    "The following animation illustrates the whole process of the self-attention head. A sequence of input tokens are used to compute keys and values, which are then compared with each query, calculated per input. For each query, the scores of all keys are calculated and then used to weight the sum of values, resulting in an output value vector per input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U76qWlrbOmx7"
   },
   "source": [
    "![texto alternativo](https://miro.medium.com/max/1973/1*_92bnsMJy8Bl539G4v93yg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDMmHAaSTE6P"
   },
   "source": [
    "Following, we are going to explain and implement:\n",
    "1. Prepare inputs\n",
    "2. Initialise weights\n",
    "3. Derive key, query and value\n",
    "4. Calculate attention scores for Input 1\n",
    "5. Calculate softmax\n",
    "6. Multiply scores with values\n",
    "7. Sum weighted values to get Output 1\n",
    "8. Repeat steps 4–7 for Input 2 & Input 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1UxPJlHBVmS"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENdzUZqSBsiB"
   },
   "source": [
    "### Step 1: Prepare inputs\n",
    "\n",
    "In text-based models, an encoding matrix is used to turn tokens into a specific encoding, often floating-point based with high dimensionality (768 for example.) For this tutorial, for the sake of simplicity, we use 3 inputs, each with dimension 4, with an integer encoding.\n",
    "\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*hmvdDXrxhJsGhOQClQdkBA.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKYrJsljBhnv",
    "outputId": "1e13d280-fcc5-4606-8bc0-30dfa29545cc"
   },
   "outputs": [],
   "source": [
    "x = [\n",
    "  [1, 0, 1, 0], # Input 1\n",
    "  [0, 2, 0, 2], # Input 2\n",
    "  [1, 1, 1, 1]  # Input 3\n",
    " ]\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ96EoE1Bvat"
   },
   "source": [
    "### Step 2: Initialise weights\n",
    "\n",
    "Every input must have three representations (see diagram below). These representations are called **key** (orange), **query** (red), and **value** (purple). For this example, let’s take that we want these representations to have a dimension of 3. Because every input has a dimension of 4, this means each set of the weights must have a shape of 4×3.\n",
    "\n",
    "![texto del enlace](https://miro.medium.com/max/1975/1*VPvXYMGjv0kRuoYqgFvCag.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUTNr15JBkSG",
    "outputId": "f445a036-1df6-494e-9f32-a0e04a61f384"
   },
   "outputs": [],
   "source": [
    "w_key = [\n",
    "  [0, 0, 1],\n",
    "  [1, 1, 0],\n",
    "  [0, 1, 0],\n",
    "  [1, 1, 0]\n",
    "]\n",
    "w_query = [\n",
    "  [1, 0, 1],\n",
    "  [1, 0, 0],\n",
    "  [0, 0, 1],\n",
    "  [0, 1, 1]\n",
    "]\n",
    "w_value = [\n",
    "  [0, 2, 0],\n",
    "  [0, 3, 0],\n",
    "  [1, 0, 3],\n",
    "  [1, 1, 0]\n",
    "]\n",
    "w_key = torch.tensor(w_key, dtype=torch.float32)\n",
    "w_query = torch.tensor(w_query, dtype=torch.float32)\n",
    "w_value = torch.tensor(w_value, dtype=torch.float32)\n",
    "\n",
    "print(\"Weights for key: \\n\", w_key)\n",
    "print(\"Weights for query: \\n\", w_query)\n",
    "print(\"Weights for value: \\n\", w_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pr9XZF9X_Ed"
   },
   "source": [
    "Note: *In a neural network setting, these weights are usually small numbers, initialised randomly using an appropriate random distribution like Gaussian, Xavier and Kaiming distributions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxGT5awVB1Xw"
   },
   "source": [
    "### Step 3: Derive key, query and value\n",
    "\n",
    "Now that we have the three sets of weights, let’s actually obtain the **key**, **query** and **value** representations for every input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQwhDIi7aGXp"
   },
   "source": [
    "Obtaining the keys:\n",
    "```\n",
    "               [0, 0, 1]\n",
    "[1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]\n",
    "[0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]\n",
    "[1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]\n",
    "```\n",
    "![texto alternativo](https://miro.medium.com/max/1975/1*dr6NIaTfTxEWzxB2rc0JWg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi0EblXTamFz"
   },
   "source": [
    "Obtaining the values:\n",
    "```\n",
    "               [0, 2, 0]\n",
    "[1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] \n",
    "[0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]\n",
    "[1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]\n",
    "```\n",
    "![texto alternativo](https://miro.medium.com/max/1975/1*5kqW7yEwvcC0tjDOW3Ia-A.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTp2izu1bLNq"
   },
   "source": [
    "Obtaining the querys:\n",
    "```\n",
    "               [1, 0, 1]\n",
    "[1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]\n",
    "[0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]\n",
    "[1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]\n",
    "```\n",
    "![texto alternativo](https://miro.medium.com/max/1975/1*wO_UqfkWkv3WmGQVHvrMJw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qegb9M0KbnRK"
   },
   "source": [
    "Notes: *Notes\n",
    "In practice, a bias vector may be added to the product of matrix multiplication.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rv2NXynOB7oG",
    "outputId": "b07f5313-f2cb-494f-844f-6ea82c23836e"
   },
   "outputs": [],
   "source": [
    "keys = torch.matmul(x, w_key)\n",
    "querys = torch.matmul(x, w_query)\n",
    "values = torch.matmul(x, w_value)\n",
    "\n",
    "print(\"Keys: \\n\", keys)\n",
    "# tensor([[0., 1., 1.],\n",
    "#         [4., 4., 0.],\n",
    "#         [2., 3., 1.]])\n",
    "\n",
    "print(\"Querys: \\n\", querys)\n",
    "# tensor([[1., 0., 2.],\n",
    "#         [2., 2., 2.],\n",
    "#         [2., 1., 3.]])\n",
    "print(\"Values: \\n\", values)\n",
    "# tensor([[1., 2., 3.],\n",
    "#         [2., 8., 0.],\n",
    "#         [2., 6., 3.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pmf0OQhCnD8"
   },
   "source": [
    "### Step 4: Calculate attention scores\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*u27nhUppoWYIGkRDmYFN2A.gif)\n",
    "\n",
    "To obtain **attention scores**, we start off with taking a dot product between Input 1’s **query** (red) with **all keys** (orange), including itself. Since there are 3 key representations (because we have 3 inputs), we obtain 3 attention scores (blue).\n",
    "\n",
    "```\n",
    "            [0, 4, 2]\n",
    "[1, 0, 2] x [1, 4, 3] = [2, 4, 4]\n",
    "            [1, 0, 1]\n",
    "```\n",
    "Notice that we only use the query from Input 1. Later we’ll work on repeating this same step for the other querys.\n",
    "\n",
    "Note: *The above operation is known as dot product attention, one of the several score functions. Other score functions include scaled dot product and additive/concat.*            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GDhKEl0Cokw",
    "outputId": "53254332-d148-41b8-c6ee-60a947e4268f"
   },
   "outputs": [],
   "source": [
    "attn_scores = torch.matmul(querys, keys.T)\n",
    "print(attn_scores)\n",
    "\n",
    "# tensor([[ 2.,  4.,  4.],  # attention scores from Query 1\n",
    "#         [ 4., 16., 12.],  # attention scores from Query 2\n",
    "#         [ 4., 12., 10.]]) # attention scores from Query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO3NmnbvCxpX"
   },
   "source": [
    "### Step 5: Calculate softmax\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*jf__2D8RNCzefwS0TP1Kyg.gif)\n",
    "\n",
    "Take the **softmax** across these **attention scores** (blue).\n",
    "```\n",
    "softmax([2, 4, 4]) = [0.0, 0.5, 0.5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDNzdZHVC1ys",
    "outputId": "9568fde2-d7bb-40f0-dfa7-aa54e8184515"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "attn_scores_softmax = softmax(attn_scores, dim=-1)\n",
    "print(attn_scores_softmax)\n",
    "# tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
    "#         [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
    "#         [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
    "\n",
    "# For readability, approximate the above as follows\n",
    "attn_scores_softmax = [\n",
    "  [0.0, 0.5, 0.5],\n",
    "  [0.0, 1.0, 0.0],\n",
    "  [0.0, 0.9, 0.1]\n",
    "]\n",
    "attn_scores_softmax = torch.tensor(attn_scores_softmax)\n",
    "print(attn_scores_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBe71nseDBhb"
   },
   "source": [
    "### Step 6: Multiply scores with values\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*9cTaJGgXPbiJ4AOCc6QHyA.gif)\n",
    "\n",
    "The softmaxed attention scores for each input (blue) is multiplied with its corresponding **value** (purple). This results in 3 alignment vectors (yellow). In this tutorial, we’ll refer to them as **weighted values**.\n",
    "```\n",
    "1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]\n",
    "2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]\n",
    "3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNnx-Fx5DFDi",
    "outputId": "a2823e50-833e-4ca5-988d-f3f424a287d4"
   },
   "outputs": [],
   "source": [
    "weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]\n",
    "print(weighted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU6w0U9ADQIc"
   },
   "source": [
    "### Step 7: Sum weighted values\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*1je5TwhVAwwnIeDFvww3ew.gif)\n",
    "\n",
    "Take all the **weighted values** (yellow) and sum them element-wise:\n",
    "\n",
    "```\n",
    "  [0.0, 0.0, 0.0]\n",
    "+ [1.0, 4.0, 0.0]\n",
    "+ [1.0, 3.0, 1.5]\n",
    "-----------------\n",
    "= [2.0, 7.0, 1.5]\n",
    "```\n",
    "\n",
    "The resulting vector ```[2.0, 7.0, 1.5]``` (dark green) **is Output 1**, which is based on the **query representation from Input 1** interacting with all other keys, including itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3yNYDUEgAos"
   },
   "source": [
    "### Step 8: Repeat for Input 2 & Input 3\n",
    "![texto alternativo](https://miro.medium.com/max/1973/1*G8thyDVqeD8WHim_QzjvFg.gif)\n",
    "\n",
    "Note: *The dimension of **query** and **key** must always be the same because of the dot product score function. However, the dimension of **value** may be different from **query** and **key**. The resulting output will consequently follow the dimension of **value**.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6excNSUDRRj",
    "outputId": "3519c2d8-608b-4ee5-e9d1-6d83057e617c"
   },
   "outputs": [],
   "source": [
    "outputs = weighted_values.sum(dim=0)\n",
    "print(outputs)\n",
    "\n",
    "# tensor([[2.0000, 7.0000, 1.5000],  # Output 1\n",
    "#         [2.0000, 8.0000, 0.0000],  # Output 2\n",
    "#         [2.0000, 7.8000, 0.3000]]) # Output 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4tyrwqqY73n"
   },
   "source": [
    "The following image summarizes the different steps. One step used in practice is to normalize the score, before the softmax, by the square root of the dimension of the key vectors. This is done only to stabilize gradients, as it leads to less drastic differences in the softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB9XD-eGY2gr"
   },
   "source": [
    "![Self-attention steps](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MXKpEpVaCNG"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Implement a division by the square root of the key vector dimensionality. How does this change the softmax values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJVY7YYgaO3T"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Make the first and the third input vectors identical. Then, implement a simple positional encoding scheme to add to the inputs. Verify that the outputs are different when the first and third input vectors are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5g7N4ghbU5-"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Using identical vectors for the first and third inputs, change any of the weight vectors to try to get the first and third outputs to be as close as possible."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
