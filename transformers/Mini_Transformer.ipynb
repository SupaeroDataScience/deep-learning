{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqRHyNzdeXaS"
      },
      "source": [
        "# Minimal Transformer: Next Token Prediction\n",
        "\n",
        "In this notebook, we build a minimal decoder-only transformer architecture (similar to GPT) and train it on next token prediction. This follows your previous class on attention mechanisms.\n",
        "\n",
        "**Key differences from encoder-decoder transformers:**\n",
        "- Decoder-only architecture (no encoder)\n",
        "- Causal self-attention (can only attend to previous tokens)\n",
        "- Next token prediction objective\n",
        "- Character-level tokenization for simplicity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl40akxCeXaU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPu9uTHTeXaU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "print(f'PyTorch version: {torch.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgcbOVzneXaV"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "We keep the model small so it can train quickly in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlf2t7UpeXaV"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "class Config:\n",
        "    # Model architecture\n",
        "    d_model = 128          # Embedding dimension\n",
        "    n_heads = 4            # Number of attention heads\n",
        "    n_layers = 4           # Number of transformer blocks\n",
        "    d_ff = 512             # Feed-forward dimension\n",
        "    dropout = 0.1          # Dropout rate\n",
        "\n",
        "    # Training\n",
        "    block_size = 64        # Maximum context length\n",
        "    batch_size = 64        # Batch size\n",
        "    learning_rate = 3e-4   # Learning rate\n",
        "    max_iters = 3000       # Training iterations\n",
        "    eval_interval = 300    # Evaluate every N iterations\n",
        "    eval_iters = 100       # Number of iterations for evaluation\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR0RtnS0eXaV"
      },
      "source": [
        "## Data: Choose Your Dataset\n",
        "\n",
        "Select one of the following datasets by uncommenting the corresponding option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_tHC_aheXaV"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTION 1: TinyStories (Recommended - Modern, Simple English)\n",
        "# ============================================================\n",
        "# Download a subset of TinyStories dataset\n",
        "print(\"Downloading TinyStories dataset...\")\n",
        "url = 'https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt'\n",
        "urllib.request.urlretrieve(url, 'dataset.txt')\n",
        "\n",
        "# Read only first ~1MB (similar to Shakespeare size)\n",
        "with open('dataset.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read(1_000_000)  # Read first 1 million characters\n",
        "\n",
        "print(f'Dataset length: {len(text):,} characters')\n",
        "print(f'\\nFirst 500 characters:')\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsKL3PKueXaV"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTION 2: Shakespeare (Classic - Early Modern English)\n",
        "# ============================================================\n",
        "# Uncomment these lines to use Shakespeare instead:\n",
        "\n",
        "# print(\"Downloading Shakespeare dataset...\")\n",
        "# url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "# urllib.request.urlretrieve(url, 'dataset.txt')\n",
        "#\n",
        "# with open('dataset.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "#\n",
        "# print(f'Dataset length: {len(text):,} characters')\n",
        "# print(f'\\nFirst 500 characters:')\n",
        "# print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9BUMz4xeXaW"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTION 3: Python Code\n",
        "# ============================================================\n",
        "# Uncomment these lines to train on Python code:\n",
        "\n",
        "# print(\"Downloading Python code dataset...\")\n",
        "# url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/linux/input.txt'\n",
        "# urllib.request.urlretrieve(url, 'dataset.txt')\n",
        "#\n",
        "# with open('dataset.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read(1_000_000)  # Read first 1 million characters\n",
        "#\n",
        "# print(f'Dataset length: {len(text):,} characters')\n",
        "# print(f'\\nFirst 500 characters:')\n",
        "# print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74BplMxLeXaW"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTION 4: Modern Novel (e.g., The Adventures of Sherlock Holmes)\n",
        "# ============================================================\n",
        "# Uncomment these lines to use Sherlock Holmes:\n",
        "\n",
        "# print(\"Downloading Sherlock Holmes...\")\n",
        "# url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
        "# urllib.request.urlretrieve(url, 'dataset.txt')\n",
        "#\n",
        "# with open('dataset.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "#\n",
        "# # Remove Project Gutenberg header/footer\n",
        "# start = text.find('I.')\n",
        "# end = text.find('End of the Project Gutenberg')\n",
        "# if start != -1 and end != -1:\n",
        "#     text = text[start:end]\n",
        "#\n",
        "# print(f'Dataset length: {len(text):,} characters')\n",
        "# print(f'\\nFirst 500 characters:')\n",
        "# print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3rMxw8TeXaW"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# OPTION 5: Wikipedia Article (Simple English Wikipedia)\n",
        "# ============================================================\n",
        "# Uncomment these lines to use Wikipedia:\n",
        "\n",
        "# print(\"Downloading Wikipedia dump...\")\n",
        "# # This is a small subset of Simple English Wikipedia\n",
        "# url = 'https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles.xml.bz2'\n",
        "# # Note: This requires additional processing with packages like mwparserfromhell\n",
        "# # For simplicity, you might want to manually download a few Wikipedia articles as .txt\n",
        "# # Or use a pre-processed Wikipedia dataset\n",
        "#\n",
        "# # For a quick demo, here's a manual approach:\n",
        "# text = \"\"\"Wikipedia is a free online encyclopedia.\n",
        "# [Paste several Wikipedia articles here as plain text]\n",
        "# \"\"\"\n",
        "#\n",
        "# print(f'Dataset length: {len(text):,} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAg1Ed3DeXaW"
      },
      "source": [
        "## Character-Level Tokenization\n",
        "\n",
        "For simplicity, we use character-level tokenization. Each unique character becomes a token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNRV7YDYeXaW"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f'Vocabulary size: {vocab_size} unique characters')\n",
        "print(f'Characters: {\"\".join(chars)}')\n",
        "\n",
        "# Create character-to-integer and integer-to-character mappings\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Encoder: string -> list of integers\n",
        "def encode(s):\n",
        "    return [char_to_idx[c] for c in s]\n",
        "\n",
        "# Decoder: list of integers -> string\n",
        "def decode(ids):\n",
        "    return ''.join([idx_to_char[i] for i in ids])\n",
        "\n",
        "# Test\n",
        "test_str = \"Hello, world!\"\n",
        "encoded = encode(test_str)\n",
        "decoded = decode(encoded)\n",
        "print(f'\\nOriginal: {test_str}')\n",
        "print(f'Encoded: {encoded}')\n",
        "print(f'Decoded: {decoded}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qo1LW5teXaW"
      },
      "source": [
        "## Prepare Training Data\n",
        "\n",
        "Split into train and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7dMypRueXaW"
      },
      "outputs": [],
      "source": [
        "# Encode the entire dataset\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(f'Encoded data shape: {data.shape}')\n",
        "\n",
        "# Train/validation split (90/10)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f'Train data: {len(train_data):,} tokens')\n",
        "print(f'Val data: {len(val_data):,} tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3RyRVUFeXaX"
      },
      "source": [
        "## Data Batching\n",
        "\n",
        "Create batches of sequences for training. Each sequence is `block_size` tokens long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVGjjJw6eXaX"
      },
      "outputs": [],
      "source": [
        "def get_batch(split, config):\n",
        "    \"\"\"\n",
        "    Generate a small batch of data of inputs x and targets y.\n",
        "\n",
        "    Args:\n",
        "        split: 'train' or 'val'\n",
        "        config: Configuration object\n",
        "\n",
        "    Returns:\n",
        "        x: Input sequences [batch_size, block_size]\n",
        "        y: Target sequences [batch_size, block_size]\n",
        "    \"\"\"\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Randomly select starting positions\n",
        "    ix = torch.randint(len(data) - config.block_size, (config.batch_size,))\n",
        "\n",
        "    # Create input and target sequences\n",
        "    x = torch.stack([data[i:i+config.block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+config.block_size+1] for i in ix])\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Test batch generation\n",
        "x_batch, y_batch = get_batch('train', config)\n",
        "print(f'Input batch shape: {x_batch.shape}')\n",
        "print(f'Target batch shape: {y_batch.shape}')\n",
        "print(f'\\nExample sequence:')\n",
        "print(f'Input:  {decode(x_batch[0].tolist())}')\n",
        "print(f'Target: {decode(y_batch[0].tolist())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMQUsaI4eXaX"
      },
      "source": [
        "## Model Components\n",
        "\n",
        "Now we build the transformer architecture from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92Q5uwjeXaX"
      },
      "source": [
        "### 1. Multi-Head Attention\n",
        "\n",
        "You've already learned about attention! This is the causal (masked) version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pc_JggyeXaX"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head causal self-attention.\n",
        "\n",
        "    Key difference from your previous class:\n",
        "    - Uses a causal mask to prevent attending to future tokens\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads  # Dimension per head\n",
        "\n",
        "        # Linear projections for Q, K, V (all heads at once)\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            Output tensor [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Split into multiple heads and reshape\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, n_heads, d_k]\n",
        "        # -> [batch_size, n_heads, seq_len, d_k]\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # Apply causal mask (prevent attending to future tokens)\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).bool()\n",
        "        scores = scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # Apply softmax\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        # [batch_size, n_heads, seq_len, d_k]\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        out = self.W_o(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtzEzEBceXaX"
      },
      "source": [
        "### 2. Feed-Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TegCotmAeXaX"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feed-Forward Network.\n",
        "    Two linear transformations with GELU activation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrXl0BdfeXaX"
      },
      "source": [
        "### 3. Transformer Block\n",
        "\n",
        "Combines attention and feed-forward with residual connections and layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "golNGPNNeXaX"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single transformer block:\n",
        "    - Multi-head attention with residual connection\n",
        "    - Feed-forward network with residual connection\n",
        "    - Layer normalization (pre-norm architecture)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm architecture (more stable training)\n",
        "        # Attention with residual\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        # Feed-forward with residual\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr1LD_n9eXaX"
      },
      "source": [
        "### 4. Complete Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9pl_dLteXaY"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A minimal GPT-style transformer for next token prediction.\n",
        "\n",
        "    Architecture:\n",
        "    1. Token embeddings + positional embeddings\n",
        "    2. Stack of transformer blocks\n",
        "    3. Layer norm\n",
        "    4. Linear head to predict next token\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Token embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, config.d_model)\n",
        "\n",
        "        # Positional embeddings (learned)\n",
        "        self.pos_embedding = nn.Embedding(config.block_size, config.d_model)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "\n",
        "        # Language modeling head\n",
        "        self.lm_head = nn.Linear(config.d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying (share weights between token embeddings and lm_head)\n",
        "        self.token_embedding.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # Report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(f\"Number of parameters: {n_params/1e6:.2f}M\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx: Input token indices [batch_size, seq_len]\n",
        "            targets: Target token indices [batch_size, seq_len] (optional)\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits [batch_size, seq_len, vocab_size]\n",
        "            loss: Cross-entropy loss (if targets provided)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = idx.shape\n",
        "\n",
        "        # Token embeddings\n",
        "        tok_emb = self.token_embedding(idx)  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Positional embeddings\n",
        "        pos = torch.arange(0, seq_len, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.pos_embedding(pos)  # [seq_len, d_model]\n",
        "\n",
        "        # Add embeddings\n",
        "        x = tok_emb + pos_emb  # Broadcasting happens automatically\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Final layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Language modeling head\n",
        "        logits = self.lm_head(x)  # [batch_size, seq_len, vocab_size]\n",
        "\n",
        "        # Compute loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Reshape for cross-entropy\n",
        "            B, T, C = logits.shape\n",
        "            logits_flat = logits.view(B * T, C)\n",
        "            targets_flat = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Generate new tokens autoregressively.\n",
        "\n",
        "        Args:\n",
        "            idx: Starting sequence [batch_size, seq_len]\n",
        "            max_new_tokens: Number of tokens to generate\n",
        "            temperature: Sampling temperature (higher = more random)\n",
        "            top_k: If set, only sample from top k tokens\n",
        "\n",
        "        Returns:\n",
        "            Generated sequence [batch_size, seq_len + max_new_tokens]\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context if needed (to fit block_size)\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits, _ = self(idx_cond)\n",
        "\n",
        "            # Focus on last time step\n",
        "            logits = logits[:, -1, :] / temperature  # [batch_size, vocab_size]\n",
        "\n",
        "            # Optionally crop logits to only top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7cO8INdeXaY"
      },
      "source": [
        "## Create Model Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB1caQEYeXaY"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = GPTModel(vocab_size, config).to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7WpUSDReXaY"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdUzOKiLeXaY"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, config):\n",
        "    \"\"\"\n",
        "    Estimate loss on train and validation sets.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(config.eval_iters)\n",
        "        for k in range(config.eval_iters):\n",
        "            X, Y = get_batch(split, config)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vPYav8veXaY"
      },
      "source": [
        "## Test Generation Before Training\n",
        "\n",
        "Let's see what the untrained model generates (should be gibberish)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXb0hwe0eXaY"
      },
      "outputs": [],
      "source": [
        "# Generate from untrained model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=200, temperature=1.0, top_k=10)\n",
        "print('\\n=== Untrained Model Generation ===')\n",
        "print(decode(generated[0].tolist()))\n",
        "print('=' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HyKi3XzeXaZ"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Train for a few thousand iterations. Even this limited training should show improvement!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_c_4azWeXaZ"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "iterations = []\n",
        "\n",
        "print('Starting training...')\n",
        "print(f'Training for {config.max_iters} iterations')\n",
        "print(f'Evaluating every {config.eval_interval} iterations\\n')\n",
        "\n",
        "for iter in tqdm(range(config.max_iters)):\n",
        "    # Evaluate loss periodically\n",
        "    if iter % config.eval_interval == 0 or iter == config.max_iters - 1:\n",
        "        losses = estimate_loss(model, config)\n",
        "        print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        train_losses.append(losses['train'])\n",
        "        val_losses.append(losses['val'])\n",
        "        iterations.append(iter)\n",
        "\n",
        "    # Sample a batch\n",
        "    xb, yb = get_batch('train', config)\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print('\\nTraining complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD_0vqfleXaZ"
      },
      "source": [
        "## Plot Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqGP8XgTeXaZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(iterations, train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(iterations, val_losses, label='Validation Loss', marker='s')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(f'Initial train loss: {train_losses[0]:.4f}')\n",
        "print(f'Final train loss: {train_losses[-1]:.4f}')\n",
        "print(f'Improvement: {train_losses[0] - train_losses[-1]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nVqySQNeXaZ"
      },
      "source": [
        "## Test Generation After Training\n",
        "\n",
        "Now let's see what the trained model generates!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBZHhCcxeXaZ"
      },
      "outputs": [],
      "source": [
        "# Generate from trained model\n",
        "model.eval()\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=500, temperature=1.0, top_k=10)\n",
        "print('\\n=== Trained Model Generation ===')\n",
        "print(decode(generated[0].tolist()))\n",
        "print('=' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALD6FR6PeXaZ"
      },
      "source": [
        "## Generation with Different Seeds\n",
        "\n",
        "Try starting with different prompts. Adjust these based on your dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ezJoOZeXaZ"
      },
      "outputs": [],
      "source": [
        "def generate_with_prompt(prompt, max_new_tokens=300, temperature=0.8, top_k=10):\n",
        "    \"\"\"\n",
        "    Generate text starting from a prompt.\n",
        "    \"\"\"\n",
        "    context = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\n",
        "    generated = model.generate(context, max_new_tokens=max_new_tokens,\n",
        "                             temperature=temperature, top_k=top_k)\n",
        "    return decode(generated[0].tolist())\n",
        "\n",
        "# Choose prompts based on your dataset:\n",
        "\n",
        "# For TinyStories:\n",
        "prompts = [\n",
        "    \"Once upon a time\",\n",
        "    \"One day, a little\",\n",
        "    \"There was a\",\n",
        "    \"The girl was\"\n",
        "]\n",
        "\n",
        "# For Shakespeare:\n",
        "# prompts = [\n",
        "#     \"ROMEO:\\n\",\n",
        "#     \"To be or not to be\",\n",
        "#     \"First Citizen:\\n\",\n",
        "#     \"The king\"\n",
        "# ]\n",
        "\n",
        "# For Python code:\n",
        "# prompts = [\n",
        "#     \"def \",\n",
        "#     \"import \",\n",
        "#     \"class \",\n",
        "#     \"for i in\"\n",
        "# ]\n",
        "\n",
        "# For Sherlock Holmes:\n",
        "# prompts = [\n",
        "#     \"Sherlock Holmes\",\n",
        "#     \"The detective\",\n",
        "#     \"Watson said\",\n",
        "#     \"It was a\"\n",
        "# ]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f'\\n=== Prompt: \"{prompt}\" ===')\n",
        "    generated_text = generate_with_prompt(prompt, max_new_tokens=200)\n",
        "    print(generated_text)\n",
        "    print('=' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcbOR0dteXaZ"
      },
      "source": [
        "## Temperature and Top-K Effects\n",
        "\n",
        "Experiment with different generation parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PATeFcuCeXaZ"
      },
      "outputs": [],
      "source": [
        "# Choose a prompt from your dataset:\n",
        "prompt = \"Once upon a time\"  # For TinyStories\n",
        "# prompt = \"ROMEO:\\n\"  # For Shakespeare\n",
        "# prompt = \"def \"  # For Python code\n",
        "# prompt = \"Sherlock Holmes\"  # For Sherlock Holmes\n",
        "\n",
        "print(\"=== Low Temperature (0.5) - More focused ===\")\n",
        "print(generate_with_prompt(prompt, max_new_tokens=150, temperature=0.5, top_k=10))\n",
        "\n",
        "print(\"\\n=== Medium Temperature (1.0) - Balanced ===\")\n",
        "print(generate_with_prompt(prompt, max_new_tokens=150, temperature=1.0, top_k=10))\n",
        "\n",
        "print(\"\\n=== High Temperature (1.5) - More random ===\")\n",
        "print(generate_with_prompt(prompt, max_new_tokens=150, temperature=1.5, top_k=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vguoqWQ1eXaZ"
      },
      "source": [
        "## Analysis: What Did We Learn?\n",
        "\n",
        "Even with this minimal training:\n",
        "1. **Structure**: The model learns basic text structure (words, punctuation, capitalization)\n",
        "2. **Patterns**: It picks up on common patterns in the data\n",
        "   - TinyStories: narrative structure, character actions\n",
        "   - Shakespeare: dialogue format, character names\n",
        "   - Python code: syntax, indentation, function structure\n",
        "3. **Context**: The causal attention allows it to use context from previous tokens\n",
        "4. **Limitations**: It's not perfect - longer coherence requires much more training\n",
        "\n",
        "**Key Architecture Points:**\n",
        "- **Decoder-only**: Unlike encoder-decoder models for translation, this is decoder-only (like GPT)\n",
        "- **Causal attention**: The mask ensures we only attend to past tokens\n",
        "- **Next token prediction**: Simple but powerful objective\n",
        "- **Scalability**: This same architecture scales to billions of parameters (GPT-3, GPT-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzmVKYZheXaZ"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "To improve this model further, try one or multiple of the following:\n",
        "1. Train for more iterations (10k-50k)\n",
        "2. Increase model size (more layers, larger d_model)\n",
        "3. Combine datasets together to have more text (be careful of style differences)\n",
        "4. Add learning rate scheduling\n",
        "5. Modify the optimizer and hyperparameters\n",
        "6. Implement gradient clipping\n",
        "7. Try different tokenization (BPE instead of character-level)\n",
        "8. Add dropout for better regularization\n",
        "9. Experiment with different attention patterns (e.g., Flash Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVFo9a-HeXaa"
      },
      "source": [
        "## Save Model (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdQWTUaleXaa"
      },
      "outputs": [],
      "source": [
        "# Save model checkpoint\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'config': config,\n",
        "    'vocab_size': vocab_size,\n",
        "    'char_to_idx': char_to_idx,\n",
        "    'idx_to_char': idx_to_char,\n",
        "}, 'minimal_transformer_checkpoint.pt')\n",
        "\n",
        "print('Model saved to minimal_transformer_checkpoint.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l1wz2CPeXaa"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we built a minimal transformer from scratch:\n",
        "- Character-level tokenization\n",
        "- Causal multi-head attention\n",
        "- Position-wise feed-forward networks\n",
        "- Residual connections and layer normalization\n",
        "- Next token prediction training\n",
        "- Autoregressive generation\n",
        "\n",
        "This architecture is the foundation of modern LLMs like GPT!\n",
        "\n",
        "**Questions to consider:**\n",
        "1. Why is causal masking necessary for language generation?\n",
        "2. What would happen without positional embeddings?\n",
        "3. How does temperature affect generation quality?\n",
        "4. What are the tradeoffs between character-level and subword tokenization?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}