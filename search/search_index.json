{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Deep Learning","text":"<p>Deep Learning section of the Algorithms in Machine Learning class at ISAE-Supaero</p> <ul> <li>Home</li> <li>Github repository</li> </ul> <p>Adapted from Emmanuel Rachelson's Machine Learning class</p>"},{"location":"index.html#syllabus","title":"Syllabus","text":"<p>This class covers deep learning from a theoretical basis to example applications. We start with simple multi-layer perceptrons, backpropogation, and gradient descent, exploring at the fundamental aspects of deep learning in depth. We cover a wide range of deep learning topics, from Natural Language Processing to Generative Adversarial Networks; the full schedule is below. The goal is that students understand the capacities of deep learning, the current state of the field, and the challenges of using and developing deep learning algorithms. By the end of this class, we expect students that students will be able to understand recent literature in deep learning, implement novel neural network architectures, use and understand the PyTorch library in many ways, and apply deep learning to different domains.</p>"},{"location":"index.html#2025-schedule","title":"2025 Schedule","text":"Schedule 12/11 Artificial Neural Networks ANNs, backpropagation, Stochastic Gradient Descent 12/11 Deep Learning layers, convolution, architectures, training 18/11 Deep Learning for Computer Vision, pt 1 Convolutional Neural Networks, satellite imagery 18/11 Deep Learning for Computer Vision, pt 2 25/11 Image generation VAEs, GANs, and Diffusion Models 02/12 RNNs Recurrent Neural Networks, LSTM, GRU 02/12 Transformers Transformers 09/12 Model Analysis Autoencoders, t-SNE, SAEs 09/12 LLMs Large Language Models"},{"location":"ANN.html","title":"Artificial Neural Networks","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class gives a biological and historical overview of artificial neural networks and walks through the theoretical foundations of backpropagation and gradient descent, using an example in numpy.</p> <p>Notebook [source] [Colab]</p> <p>As an additional exercise, this notebook runs the provided backpropagation code, visualizing neural activations and error at each step.</p> <p>Visualizing backprop</p>"},{"location":"ANN.html#additional-resources","title":"Additional Resources","text":"<p>In class we present a Universal Approximation Theorem for single-layer networks with sigmoid activation functions. The slides for that proof are here: Universal Approximation Theorem</p> <p>This online book has a nice interactive visualization of this property of neural networks.</p> <p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here</p> <p>Stanford CS229 Lectures 11 and 12 introduce ANNs, backpropagation, and gradient descent, with lecture 12 going further in depth on how to resolve common training problems in ANNs.</p>"},{"location":"DR.html","title":"Model Analysis","text":"<p>Previously dimensionality reduction</p> <ul> <li>Home</li> <li>Github repository</li> </ul> <p>Model Analysis</p> <p>Uncertainty Calibration</p>"},{"location":"DR.html#dimensionality-reduction-resources","title":"Dimensionality Reduction resources","text":"<p>This class goes deeper in the mechanism and usage of Dimensionality Reduction. You will learn to exploit the effects of dimensionality on your machine learning models, with illustrated intuitions, and practical examples. You will start with an already very known technique in a linear setup, Principal Components Analysis, to move progressively towards non-linear manifold learning with t-SNE, to end up with Deep Learning techniques to reduce the dimension. Through concrete application of Autoencoders you will know what are their main interests and pitfalls, to make the best architecture choice given your problem to solve.</p> <p>Lecture notes</p> <p>Notebook for class exercises (colab)</p> <p>Solutions (colab)</p>"},{"location":"GAN.html","title":"Generative Models","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>In this class, we cover three types of generative models: Variational Autoencoders, Generative Adversarial Networks, and Score-based models (also called diffusion models).</p> <p>VAEs, Colab version</p> <p>GANs, Colab version</p> <p>Score-based models, Colab version</p>"},{"location":"GAN.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Intuitively Understanding Variational Autoencoders</li> <li>Understanding Variational Autoencoders (VAEs)</li> <li>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 20 covers generative models and section 20.10.4 specifically covers GANs.</li> <li>Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \"Score-Based Generative Modeling through Stochastic Differential Equations.\" International Conference on Learning Representations, 2021.</li> <li>Jonathan Ho, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in Neural Information Processing Systems. 2020.</li> <li>Yang Song, and Stefano Ermon. \"Generative modeling by estimating gradients of the data distribution.\" Advances in Neural Information Processing Systems. 2019.</li> </ul>"},{"location":"NLP.html","title":"Natural Language Processing","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class is closely tied to the Transformers class, which focuses on the architecture. In this two-part class, we present a historical and recent view of NLP in AI.</p> <ol> <li> <p>Introduction to NLP slides</p> </li> <li> <p>Sequence to Sequence Learning, version colab</p> </li> <li> <p>Attention in Seq2Seq, version colab</p> </li> <li> <p>Large Language Models and Generative AI slides</p> </li> </ol>"},{"location":"NLP.html#additional-resources","title":"Additional Resources","text":"<ul> <li>LLM Visualization</li> <li>HuggingFace LLM Course</li> <li>Claude 3 features</li> <li>Text Processing and Classification, version colab</li> <li>Entity Recognition and Chatbots, version colab</li> <li>Word2Vec Exercise, version colab</li> <li>MOOC of 10-20h on linguistics</li> <li>Explosion.ai, creators of spacy</li> <li>blog of Jay Alamar</li> </ul>"},{"location":"RNN.html","title":"Recurrent Neural Networks","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>Introduction</p> <ul> <li>Notebook</li> <li>Colab</li> </ul> <p>Time-series Forecasting</p> <ul> <li>Notebook</li> <li>Colab</li> </ul>"},{"location":"RNN.html#additional-resources","title":"Additional Resources","text":"<p>Time series forecasting tutorial in Tensorflow.</p> <p>Stanford's CS231n lecture on Recurrent Neural Networks, covering RNNs, backpropagation through time, and LSTMs.</p> <p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 10 covers recurrent neural networks.</p>"},{"location":"deep.html","title":"Deep Learning","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>In this class, we construct simple deep ANNs using the PyTorch library on the Fashion MNIST example.</p> <p>Notebook [source] [Colab]</p> <p>Neural Architecture Slides</p> <p>Instead of calculating backpropagation by hand as in the first class, this class uses automatic differentiation built in to PyTorch. This is built on the autograd package which has its own tutorial.</p> <p>One important concept is convolutional neural network layers. The Stanford CS231N class has a good interactive demonstration of convolution. This page shows demonstrations of stride, padding, and dilation.</p> <p>To facilitate the use of Torch, we introduce ignite at the end of class. An example of using ignite is given in this notebook</p>"},{"location":"deep.html#additional-resources","title":"Additional Resources","text":"<p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 9 specifically covers convolutional neural networks.</p>"},{"location":"evaluation.html","title":"Deep Learning Demo","text":"<p>Evaluation 2025-2026</p>"},{"location":"evaluation.html#principle-and-pedagogical-objective","title":"Principle and Pedagogical Objective","text":"<p>The purpose of this exercise is to create an educational video and code demonstration presenting the topic, as if you were presenting it to your peers (SDD students, work colleagues, knowledgeable clients, etc.). We never learn as well as when we teach, so this is an opportunity to master one more topic and collectively build competence by critically reviewing each other's work. We will use peer evaluation, and the goal is for the time you dedicate to this assessment to be a time where you continue to discover new things and deepen your mastery of deep learning. You will choose a specific topic to present, either from the list of topics or of your choice, if your proposed subject is validated.</p>"},{"location":"evaluation.html#starting-point","title":"Starting Point","text":"<p>Each topic is associated with one or two online references. These are starting points, not content to summarize! It is expected that you will read complementary documents, build your understanding, and make choices about what to present in your video and demo (or in some cases, choose to specifically address one aspect of the topic and not another - in this case, please discuss it with me).</p>"},{"location":"evaluation.html#instructions-and-evaluation-criteria","title":"Instructions and Evaluation Criteria","text":""},{"location":"evaluation.html#video-requirements","title":"Video Requirements","text":"<p>Your video must be educational, engaging, and have a good balance between formal and practical aspects. The suggested length is 10 minutes, with a minimum of 7 minutes and a maximum of 12 minutes. This is sufficient time to fully cover deep learning concepts if the video is well planned.</p> <p>You can use OBS, Camtasia, DaVinci Resolve, or other tools to create your video. You are encouraged to plan out your video and create slides to make it truly educational. The language can be French or English, according to your preference (language quality contributes to a pleasant viewing experience and to your grade).</p> <p>Your video must be rigorous: this is not scientific popularization, you must be precise and rigorous. This doesn't require you to systematically provide mathematical proofs, but it does require formulating and discussing ideas and results in a precise and well-argued manner.</p> <p>Your video must clearly explain the deep learning concepts in the article, making them accessible and understandable to your peers.</p>"},{"location":"evaluation.html#demo-requirements","title":"Demo Requirements","text":"<p>Your code demonstration must be useful and reusable, featuring elements that will allow viewers to quickly become functional with the topic. The demo should provide results and support for the concepts explained in your video.</p> <p>Ease of use is important: other students will try to run your demo, so make sure it is well-documented with clear installation and execution instructions (e.g., a README file with dependencies, setup steps, and usage examples).</p> <p>Your demo must be well-documented: code should be clear, and any necessary references or resources should be included.</p>"},{"location":"evaluation.html#extension-and-originality","title":"Extension and Originality","text":"<p>Beyond reproducing the results or concepts from the article, you must explore original ideas. How can you improve on the idea or modify it? Even if your modification doesn't improve performance, exploring alternative approaches, testing hypotheses, or applying the concept to new domains demonstrates deep understanding and creativity.</p> <p>This exploratory component is an essential part of the assignment and will be evaluated based on the originality and execution of your extension idea.</p>"},{"location":"evaluation.html#evaluation-criteria","title":"Evaluation Criteria","text":"Criterion Points Video clarity and pedagogy 5 Video comprehensiveness - technical accuracy, quality, and level of detail 5 Demo code quality and ease of use 5 Extension idea originality and execution 5 Total 20"},{"location":"evaluation.html#submission","title":"Submission","text":"<p>You must submit: 1. Video: Upload to YouTube, Vimeo or Nextcloud 2. Code Demo: Upload to GitHub</p> <p>Submission process: Via the LMS (Learning Management System), submit only the links to both your video and your GitHub repository. The LMS link will be provided separately.</p>"},{"location":"evaluation.html#schedule","title":"Schedule","text":"Date Event 05/01/26 Subject selection 30/01/26 Submission deadline 02/02/26 Peer evaluation <p>It is important to submit the links beforehand to ensure that everyone has a video and demo submitted before the peer evaluation.</p>"},{"location":"evaluation.html#peer-evaluation","title":"Peer Evaluation","text":"<p>After submission, other students will evaluate your work by: 1. Watching your video 2. Running your demo</p> <p>They will assess the clarity of your explanations, the technical depth, the quality and usability of your demo, and the originality of your extension work. This peer evaluation process is an opportunity to learn from each other's approaches and to provide constructive feedback.</p>"},{"location":"evaluation.html#work-recommendations","title":"Work Recommendations","text":"<p>Creating a quality video and demo takes time, but it's also one of the best ways to learn deeply. Here's a recommended workflow:</p> <p>Week 1: Research sources and efficient reading. Each topic has bibliographic references - read them efficiently and search for complementary sources to better understand or gain different perspectives.</p> <p>Week 2: Deep reading and practical experience with the topic (coding, personal exploration of theory).</p> <p>Week 3: Decide on your video structure, create slides, and draft your script. Begin developing your demo code.</p> <p>Week 4: Record your video and finalize your demo. Review, polish, and test everything. Make sure your demo runs smoothly and is well-documented.</p>"},{"location":"evaluation.html#your-cv","title":"Your CV","text":"<p>This video and demo are part of your portfolio. Put them on your GitHub, mention them in interviews, and highlight them when you apply somewhere. This is a unique and valuable exercise that is completely worth showcasing in your CV! Both the video and demonstration code can be excellent additions to your professional portfolio.</p>"},{"location":"transformers.html","title":"Transformers","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class introduces attention mechanisms in detail and presents the Transformer architecture, with an example in NLP. It is very tied to the NLP class.</p> <p>Attention notebook source</p> <p>Attention notebook on Colab</p> <p>Mini Transformer notebook source</p> <p>Mini Transformer notebook on Colab</p>"},{"location":"transformers.html#additional-resources","title":"Additional Resources","text":"<ul> <li>The Illustrated Transformer</li> <li>Illustrated Transformer notebook, version on Colab</li> <li>Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems 30, 2017. pdf</li> <li>Brown, Tom, et al. \"Language models are few-shot learners.\" Advances in neural information processing systems 33 (2020): 1877-1901. pdf</li> <li>Raffel, Colin, et al. \"Exploring the limits of transfer learning with a unified text-to-text transformer.\" J. Mach. Learn. Res. 21.140 (2020): 1-67. pdf</li> <li>Hoffmann, Jordan, et al. \"Training Compute-Optimal Large Language Models.\" arXiv preprint arXiv:2203.15556 (2022). pdf</li> </ul>"},{"location":"transformers.html#vision-transformers","title":"Vision Transformers","text":"<ul> <li>Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020). pdf</li> <li>ViT - Papers with code</li> <li>Wightman, Ross, Hugo Touvron, and Herv\u00e9 J\u00e9gou. \"Resnet strikes back: An improved training procedure in timm.\" arXiv preprint arXiv:2110.00476 (2021). pdf</li> <li>Liu, Zhuang, et al. \"A convnet for the 2020s.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. pdf</li> </ul>"},{"location":"vision.html","title":"Deep Learning for Computer Vision","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>Class materials here</p>"},{"location":"vision.html#deep-learning-for-computer-vision-practical-session-at-isae-supaero","title":"Deep Learning for Computer Vision , practical session at ISAE-SUPAERO","text":""},{"location":"vision.html#introduction","title":"Introduction","text":"<p>This repository contains the code and documentation of a \"Deep Learning practical session\" given at ISAE-SUPAERO on December 2nd/3rd 2019 and Nov 30/Dec 1st 2020</p> <p>The introduction slides can be accessed at this URL website. It is recommended to read it first as it contains the necessary information to run this from scratch.</p> <p>There are three notebooks at the root of this repository, those as the exercises</p>"},{"location":"vision.html#where-to-run-it","title":"Where to run it ?","text":"<p>This hands on session is based on running code &amp; training using Google Cloud Platform Deep Learning VMs, see <code>gcp/</code> for examples on configuring your own machine. </p> <p>However, this is runnable everywhere since data access is based on public URLs &amp; numpy</p> <p>Should you want to do this at home you can use Google Collaboratory instances - it's even easier than deep learning VMs</p>"},{"location":"vision.html#usage-contribution","title":"Usage &amp; Contribution","text":"<p>No support is guaranteed by the authors beyond the hands-on session.</p> <p>This hands-on session was created by Florient Chouteau and Matthieu Le Goff.</p> <p>See <code>licence.md</code> for licence information.</p>"}]}