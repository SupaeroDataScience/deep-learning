{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Deep Learning","text":"<p>Deep Learning section of the Algorithms in Machine Learning class at ISAE-Supaero</p> <ul> <li>Home</li> <li>Github repository</li> </ul> <p>Adapted from Emmanuel Rachelson's Machine Learning class</p>"},{"location":"index.html#syllabus","title":"Syllabus","text":"<p>This class covers deep learning from a theoretical basis to example applications. We start with simple multi-layer perceptrons, backpropogation, and gradient descent, exploring at the fundamental aspects of deep learning in depth. We cover a wide range of deep learning topics, from Natural Language Processing to Generative Adversarial Networks; the full schedule is below. The goal is that students understand the capacities of deep learning, the current state of the field, and the challenges of using and developing deep learning algorithms. By the end of this class, we expect students that students will be able to understand recent literature in deep learning, implement novel neural network architectures, use and understand the PyTorch library in many ways, and apply deep learning to different domains.</p>"},{"location":"index.html#2023-schedule","title":"2023 Schedule","text":"Schedule 28/11 Artificial Neural Networks ANNs, backpropagation, Stochastic Gradient Descent 29/11 Deep Learning layers, convolution, architectures, training 05/12 Deep Learning for Computer Vision, pt 1 Convolutional Neural Networks, satellite imagery 05/12 Deep Learning for Computer Vision, pt 2 12/12 GANs VAEs, GANs, and Diffusion Models 19/12 RNNs Recurrent Neural Networks, LSTM, GRU 19/12 NLP Natural Language Processing, Transformers 09/01 Dimensionality Reduction Autoencoders, t-SNE"},{"location":"ANN.html","title":"Artificial Neural Networks","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class gives a biological and historical overview of artificial neural networks and walks through the theoretical foundations of backpropagation and gradient descent, using an example in numpy.</p> <p>Notebook [source] [Colab]</p> <p>As an additional exercise, this notebook runs the provided backpropagation code, visualizing neural activations and error at each step.</p> <p>Visualizing backprop</p>"},{"location":"ANN.html#additional-resources","title":"Additional Resources","text":"<p>In class we present a Universal Approximation Theorem for single-layer networks with sigmoid activation functions. The slides for that proof are here: Universal Approximation Theorem</p> <p>This online book has a nice interactive visualization of this property of neural networks.</p> <p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here</p> <p>Stanford CS229 Lectures 11 and 12 introduce ANNs, backpropagation, and gradient descent, with lecture 12 going further in depth on how to resolve common training problems in ANNs.</p>"},{"location":"DR.html","title":"Dimensionality Reduction","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class goes deeper in the mechanism and usage of Dimensionality Reduction. You will learn to exploit the effects of dimensionality on your machine learning models, with illustrated intuitions, and practical examples. You will start with an already very known technique in a linear setup, Principal Components Analysis, to move progressively towards non-linear manifold learning with t-SNE, to end up with Deep Learning techniques to reduce the dimension. Through concrete application of Autoencoders you will know what are their main interests and pitfalls, to make the best architecture choice given your problem to solve.</p> <p>Lecture notes</p> <p>Notebook for class exercises (colab)</p> <p>Solutions (colab)</p>"},{"location":"GAN.html","title":"Generative Models","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>In this class, we cover three types of generative models: Variational Autoencoders, Generative Adversarial Networks, and Score-based models (also called diffusion models).</p> <p>VAEs, Colab version</p> <p>GANs, Colab version</p> <p>Score-based models, Colab version</p>"},{"location":"GAN.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Intuitively Understanding Variational Autoencoders</li> <li>Understanding Variational Autoencoders (VAEs)</li> <li>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 20 covers generative models and section 20.10.4 specifically covers GANs.</li> <li>Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \"Score-Based Generative Modeling through Stochastic Differential Equations.\" International Conference on Learning Representations, 2021.</li> <li>Jonathan Ho, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in Neural Information Processing Systems. 2020.</li> <li>Yang Song, and Stefano Ermon. \"Generative modeling by estimating gradients of the data distribution.\" Advances in Neural Information Processing Systems. 2019.</li> </ul>"},{"location":"NLP.html","title":"Natural Language Processing","text":"<ul> <li>Home</li> <li> <p>Github repository</p> </li> <li> <p>Introduction to NLP slides</p> </li> <li> <p>Sequence to Sequence Learning, version colab</p> </li> <li> <p>Attention in Seq2Seq, version colab</p> </li> <li> <p>Large Language Models and Generative AI slides</p> </li> </ul>"},{"location":"NLP.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Text Processing and Classification, version colab</li> <li>Entity Recognition and Chatbots, version colab</li> <li>Word2Vec Exercise, version colab</li> <li>MOOC of 10-20h on linguistics</li> <li>blog of Jay Alamar</li> <li>blog of Sebastian Ruder</li> <li>Explosion.ai, creators of spacy</li> <li>Rasa blog</li> </ul>"},{"location":"RNN.html","title":"Recurrent Neural Networks","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>Introduction</p> <ul> <li>Notebook</li> <li>Colab</li> </ul> <p>Time-series Forecasting</p> <ul> <li>Notebook</li> <li>Colab</li> </ul>"},{"location":"RNN.html#additional-resources","title":"Additional Resources","text":"<p>Time series forecasting tutorial in Tensorflow.</p> <p>Stanford's CS231n lecture on Recurrent Neural Networks, covering RNNs, backpropagation through time, and LSTMs.</p> <p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 10 covers recurrent neural networks.</p>"},{"location":"deep.html","title":"Deep Learning","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>In this class, we construct simple deep ANNs using the PyTorch library on the Fashion MNIST example.</p> <p>Notebook [source] [Colab]</p> <p>Neural Architecture Slides</p> <p>Instead of calculating backpropagation by hand as in the first class, this class uses automatic differentiation built in to PyTorch. This is built on the autograd package which has its own tutorial.</p> <p>One important concept is convolutional neural network layers. The Stanford CS231N class has a good interactive demonstration of convolution. This page shows demonstrations of stride, padding, and dilation.</p> <p>To facilitate the use of Torch, we introduce ignite at the end of class. An example of using ignite is given in this notebook</p>"},{"location":"deep.html#additional-resources","title":"Additional Resources","text":"<p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 9 specifically covers convolutional neural networks.</p>"},{"location":"transformers.html","title":"Transformers","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class introduces attention mechanisms in detail and presents the Transformer architecture, with an example in NLP. It builds on the NLP class.</p> <p>Attention notebook source</p> <p>Attention notebook on Colab</p> <p>Transformer notebook source</p> <p>Transformer notebook on Colab</p>"},{"location":"transformers.html#additional-resources","title":"Additional Resources","text":"<ul> <li>The Illustrated Transformer</li> <li>Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems 30, 2017. pdf</li> <li>Brown, Tom, et al. \"Language models are few-shot learners.\" Advances in neural information processing systems 33 (2020): 1877-1901. pdf</li> <li>Raffel, Colin, et al. \"Exploring the limits of transfer learning with a unified text-to-text transformer.\" J. Mach. Learn. Res. 21.140 (2020): 1-67. pdf</li> <li>t5 on HuggingFace</li> <li>c4 dataset</li> <li>Hoffmann, Jordan, et al. \"Training Compute-Optimal Large Language Models.\" arXiv preprint arXiv:2203.15556 (2022). pdf</li> <li>Taylor, Ross, et al. \"Galactica: A large language model for science.\" arXiv preprint arXiv:2211.09085 (2022). pdf</li> <li>ChatGPT</li> </ul>"},{"location":"transformers.html#vision-transformers","title":"Vision Transformers","text":"<ul> <li>Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020). pdf</li> <li>ViT - Papers with code</li> <li>Wightman, Ross, Hugo Touvron, and Herv\u00e9 J\u00e9gou. \"Resnet strikes back: An improved training procedure in timm.\" arXiv preprint arXiv:2110.00476 (2021). pdf</li> <li>Liu, Zhuang, et al. \"A convnet for the 2020s.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. pdf</li> </ul>"},{"location":"vision.html","title":"Deep Learning for Computer Vision","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>Class materials here</p>"},{"location":"vision.html#deep-learning-for-computer-vision-practical-session-at-isae-supaero","title":"Deep Learning for Computer Vision , practical session at ISAE-SUPAERO","text":""},{"location":"vision.html#introduction","title":"Introduction","text":"<p>This repository contains the code and documentation of a \"Deep Learning practical session\" given at ISAE-SUPAERO on December 2nd/3rd 2019 and Nov 30/Dec 1st 2020</p> <p>The introduction slides can be accessed at this URL website. It is recommended to read it first as it contains the necessary information to run this from scratch.</p> <p>There are three notebooks at the root of this repository, those as the exercises</p>"},{"location":"vision.html#where-to-run-it","title":"Where to run it ?","text":"<p>This hands on session is based on running code &amp; training using Google Cloud Platform Deep Learning VMs, see <code>gcp/</code> for examples on configuring your own machine. </p> <p>However, this is runnable everywhere since data access is based on public URLs &amp; numpy</p> <p>Should you want to do this at home you can use Google Collaboratory instances - it's even easier than deep learning VMs</p>"},{"location":"vision.html#usage-contribution","title":"Usage &amp; Contribution","text":"<p>No support is guaranteed by the authors beyond the hands-on session.</p> <p>This hands-on session was created by Florient Chouteau and Matthieu Le Goff.</p> <p>See <code>licence.md</code> for licence information.</p>"}]}