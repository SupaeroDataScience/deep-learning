{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Deep Learning \ud83d\udd17 Deep Learning section of the Algorithms in Machine Learning class at ISAE-Supaero Home Github repository Adapted from Emmanuel Rachelson's Machine Learning class Syllabus \ud83d\udd17 This class covers deep learning in a total of 24 hours over 5 weeks. We start with simple multi-layer perceptrons, backpropogation, and gradient descent, exploring at the fundamental aspects of deep learning in depth. We cover a wide range of deep learning topics, from Natural Language Processing to Generative Adversarial Networks; the full schedule is below. The goal is that students understand the capacities of deep learning, the current state of the field, and the challenges of using and developing deep learning algorithms. By the end of this class, we expect students that students will be able to understand recent literature in deep learning, implement novel neural network architectures, use and understand the PyTorch library in many ways, and apply deep learning to different domains. 2020 Schedule \ud83d\udd17 Schedule 17/11 Artificial Neural Networks ANNs, backpropagation, Stochastic Gradient Descent 23/11 Deep Learning layers, convolution, architectures, training 30/11 Deep Learning for Computer Vision, pt 1 Convolutional Neural Networks, satellite imagery 01/12 Deep Learning for Computer Vision, pt 2 07/12 RNNs Recurrent Neural Networks, LSTM, GRU 08/12 NLP Natural Language Processing, Transformers 15/12 GANs Generative Adversarial Networks, CycleGAN 16/12 Dimensionality Reduction Autoencoders, t-SNE","title":"Deep Learning"},{"location":"index.html#deep-learning","text":"Deep Learning section of the Algorithms in Machine Learning class at ISAE-Supaero Home Github repository Adapted from Emmanuel Rachelson's Machine Learning class","title":"Deep Learning"},{"location":"index.html#syllabus","text":"This class covers deep learning in a total of 24 hours over 5 weeks. We start with simple multi-layer perceptrons, backpropogation, and gradient descent, exploring at the fundamental aspects of deep learning in depth. We cover a wide range of deep learning topics, from Natural Language Processing to Generative Adversarial Networks; the full schedule is below. The goal is that students understand the capacities of deep learning, the current state of the field, and the challenges of using and developing deep learning algorithms. By the end of this class, we expect students that students will be able to understand recent literature in deep learning, implement novel neural network architectures, use and understand the PyTorch library in many ways, and apply deep learning to different domains.","title":"Syllabus"},{"location":"index.html#2020-schedule","text":"Schedule 17/11 Artificial Neural Networks ANNs, backpropagation, Stochastic Gradient Descent 23/11 Deep Learning layers, convolution, architectures, training 30/11 Deep Learning for Computer Vision, pt 1 Convolutional Neural Networks, satellite imagery 01/12 Deep Learning for Computer Vision, pt 2 07/12 RNNs Recurrent Neural Networks, LSTM, GRU 08/12 NLP Natural Language Processing, Transformers 15/12 GANs Generative Adversarial Networks, CycleGAN 16/12 Dimensionality Reduction Autoencoders, t-SNE","title":"2020 Schedule"},{"location":"ANN.html","text":"Artificial Neural Networks \ud83d\udd17 Home Github repository This class gives a biological and historical overview of artificial neural networks and walks through the theoretical foundations of backpropagation and gradient descent, using an example in numpy. Notebook [source] [Colab] As an additional exercise, this notebook runs the provided backpropagation code, visualizing neural activations and error at each step. Visualizing backprop Additional Resources \ud83d\udd17 In class we present a Universal Approximation Theorem for single-layer networks with sigmoid activation functions. The slides for that proof are here: Universal Approximation Theorem This online book has a nice interactive visualization of this property of neural networks. The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here Stanford CS229 Lectures 11 and 12 introduce ANNs, backpropagation, and gradient descent, with lecture 12 going further in depth on how to resolve common training problems in ANNs.","title":"Artificial Neural Networks"},{"location":"ANN.html#artificial-neural-networks","text":"Home Github repository This class gives a biological and historical overview of artificial neural networks and walks through the theoretical foundations of backpropagation and gradient descent, using an example in numpy. Notebook [source] [Colab] As an additional exercise, this notebook runs the provided backpropagation code, visualizing neural activations and error at each step. Visualizing backprop","title":"Artificial Neural Networks"},{"location":"ANN.html#additional-resources","text":"In class we present a Universal Approximation Theorem for single-layer networks with sigmoid activation functions. The slides for that proof are here: Universal Approximation Theorem This online book has a nice interactive visualization of this property of neural networks. The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here Stanford CS229 Lectures 11 and 12 introduce ANNs, backpropagation, and gradient descent, with lecture 12 going further in depth on how to resolve common training problems in ANNs.","title":"Additional Resources"},{"location":"DR.html","text":"Dimensionality Reduction \ud83d\udd17 Home Github repository This class goes deeper in the mechanism and usage of Dimensionality Reduction. You will learn to exploit the effects of dimensionality on your machine learning models, with illustrated intuitions, and practical examples. You will start with an already very known technique in a linear setup, Principal Components Analysis, to move progressively towards non-linear manifold learning with t-SNE, to end up with Deep Learning techniques to reduce the dimension. Through concrete application of Autoencoders you will know what are their main interests and pitfalls, to make the best architecture choice given your problem to solve. Lecture notes Notebook for class exercises ( colab ) Solutions ( colab )","title":"Dimensionality Reduction"},{"location":"DR.html#dimensionality-reduction","text":"Home Github repository This class goes deeper in the mechanism and usage of Dimensionality Reduction. You will learn to exploit the effects of dimensionality on your machine learning models, with illustrated intuitions, and practical examples. You will start with an already very known technique in a linear setup, Principal Components Analysis, to move progressively towards non-linear manifold learning with t-SNE, to end up with Deep Learning techniques to reduce the dimension. Through concrete application of Autoencoders you will know what are their main interests and pitfalls, to make the best architecture choice given your problem to solve. Lecture notes Notebook for class exercises ( colab ) Solutions ( colab )","title":"Dimensionality Reduction"},{"location":"GAN.html","text":"Generative Adversarial Networks \ud83d\udd17 Home Github repository In this class, we introduce Generative Adversarial Networks, showing an example of GAN training on MNIST. Notebook source Notebook on Colab Additional Resources \ud83d\udd17 DeepMind lecture on GANs which goes further in depth on GAN training, architectures, and applications. 2016 NeurIPS GAN tutorial Unrolling GANs which helps with reducing mode collapse and early convergence. Common Problems in GAN training. DCGAN Pytorch tutorial CycleGAN - Zhu, Jun-Yan, et al. \"Unpaired image-to-image translation using cycle-consistent adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017. code StyleGAN - Karras, Tero, Samuli Laine, and Timo Aila. \"A style-based generator architecture for generative adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2019. This person does not exist - GAN-generated faces. ( cat version ) Everybody Dance Now demonstrates motion transfer in videos. Deepfakes , the application of GANs and other deep ANNs to creating modified or generated media, sometimes for harmful and deceptive purposes. ArtBreeder uses BigGAN and StyleGAN to generate images in an iterative optimization process based on evolutionary algorithms. Ganbreeder is an open-source version. The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here . Chapter 20 covers generative models and section 20.10.4 specifically covers GANs.","title":"GANs"},{"location":"GAN.html#generative-adversarial-networks","text":"Home Github repository In this class, we introduce Generative Adversarial Networks, showing an example of GAN training on MNIST. Notebook source Notebook on Colab","title":"Generative Adversarial Networks"},{"location":"GAN.html#additional-resources","text":"DeepMind lecture on GANs which goes further in depth on GAN training, architectures, and applications. 2016 NeurIPS GAN tutorial Unrolling GANs which helps with reducing mode collapse and early convergence. Common Problems in GAN training. DCGAN Pytorch tutorial CycleGAN - Zhu, Jun-Yan, et al. \"Unpaired image-to-image translation using cycle-consistent adversarial networks.\" Proceedings of the IEEE international conference on computer vision. 2017. code StyleGAN - Karras, Tero, Samuli Laine, and Timo Aila. \"A style-based generator architecture for generative adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2019. This person does not exist - GAN-generated faces. ( cat version ) Everybody Dance Now demonstrates motion transfer in videos. Deepfakes , the application of GANs and other deep ANNs to creating modified or generated media, sometimes for harmful and deceptive purposes. ArtBreeder uses BigGAN and StyleGAN to generate images in an iterative optimization process based on evolutionary algorithms. Ganbreeder is an open-source version. The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here . Chapter 20 covers generative models and section 20.10.4 specifically covers GANs.","title":"Additional Resources"},{"location":"NLP.html","text":"Natural Language Processing \ud83d\udd17 Home Github repository Notebooks: Text Processing and Classification , version colab Entity Recognition and Chatbots , version colab Natural Language Generation , version colab Natural Language Generation in Pytorch , version colab Additional Resources \ud83d\udd17 Concernant les aspects g\u00e9n\u00e9raux et th\u00e9oriques sur la linguistique : MOOC de 10-20h sur la linguistique A envisager si vous \u00eates particuli\u00e8rement int\u00e9ress\u00e9.e par le domaine. Concernant les concepts de la NLP le blog de Jay Alamar avec d'excellentes illustrations et explications des diff\u00e9rents mod\u00e8les: le blog de Sebastian Ruder , r\u00e9f\u00e9rence du domaine avec des tr\u00e8s bonnes explications. (pour info, Ruder vient d'un background en linguistique avant de s'\u00eatre tourn\u00e9 vers l'informatique. On le sent beaucoup dans sa mani\u00e8re de faire r\u00e9f\u00e9rence aux probl\u00e8mes de NLP... en particulier, son dernier article du mois d'Aout parle de l'importance de lier la NLP \u00e0 l'aspect culturel) le site Explosion.ai , cr\u00e9ateurs de spacy, l'une des meilleurs librairies de NLP le blog Rasa , techno opensource pour la cr\u00e9ation de chatbots qui gagne une ampleur consid\u00e9rable ces derniers temps. Je vous invite en particulier \u00e0 regarder le mod\u00e8le DIET, propos\u00e9 par leurs \u00e9quipes au d\u00e9but de l'ann\u00e9e, et qui aborde de mani\u00e8re tr\u00e8s pragmatique les probl\u00e8mes de classification et d'extraction d'entit\u00e9s en NLP. Ethique et IA Livre blanc de la commission europ\u00e9enne sur l'IA et son application dans le domaine bancaire","title":"NLP"},{"location":"NLP.html#natural-language-processing","text":"Home Github repository Notebooks: Text Processing and Classification , version colab Entity Recognition and Chatbots , version colab Natural Language Generation , version colab Natural Language Generation in Pytorch , version colab","title":"Natural Language Processing"},{"location":"NLP.html#additional-resources","text":"Concernant les aspects g\u00e9n\u00e9raux et th\u00e9oriques sur la linguistique : MOOC de 10-20h sur la linguistique A envisager si vous \u00eates particuli\u00e8rement int\u00e9ress\u00e9.e par le domaine. Concernant les concepts de la NLP le blog de Jay Alamar avec d'excellentes illustrations et explications des diff\u00e9rents mod\u00e8les: le blog de Sebastian Ruder , r\u00e9f\u00e9rence du domaine avec des tr\u00e8s bonnes explications. (pour info, Ruder vient d'un background en linguistique avant de s'\u00eatre tourn\u00e9 vers l'informatique. On le sent beaucoup dans sa mani\u00e8re de faire r\u00e9f\u00e9rence aux probl\u00e8mes de NLP... en particulier, son dernier article du mois d'Aout parle de l'importance de lier la NLP \u00e0 l'aspect culturel) le site Explosion.ai , cr\u00e9ateurs de spacy, l'une des meilleurs librairies de NLP le blog Rasa , techno opensource pour la cr\u00e9ation de chatbots qui gagne une ampleur consid\u00e9rable ces derniers temps. Je vous invite en particulier \u00e0 regarder le mod\u00e8le DIET, propos\u00e9 par leurs \u00e9quipes au d\u00e9but de l'ann\u00e9e, et qui aborde de mani\u00e8re tr\u00e8s pragmatique les probl\u00e8mes de classification et d'extraction d'entit\u00e9s en NLP. Ethique et IA Livre blanc de la commission europ\u00e9enne sur l'IA et son application dans le domaine bancaire","title":"Additional Resources"},{"location":"RNN.html","text":"Recurrent Neural Networks \ud83d\udd17 Home Github repository Introduction Notebook Colab Time-series Forecasting Notebook Colab Additional Resources \ud83d\udd17 Time series forecasting tutorial in Tensorflow. Stanford's CS231n lecture on Recurrent Neural Networks, covering RNNs, backpropagation through time, and LSTMs. The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here . Chapter 10 covers recurrent neural networks.","title":"RNNs"},{"location":"RNN.html#recurrent-neural-networks","text":"Home Github repository Introduction Notebook Colab Time-series Forecasting Notebook Colab","title":"Recurrent Neural Networks"},{"location":"RNN.html#additional-resources","text":"Time series forecasting tutorial in Tensorflow. Stanford's CS231n lecture on Recurrent Neural Networks, covering RNNs, backpropagation through time, and LSTMs. The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here . Chapter 10 covers recurrent neural networks.","title":"Additional Resources"},{"location":"deep.html","text":"Deep Learning \ud83d\udd17 Home Github repository In this class, we construct simple deep ANNs using the PyTorch library on the Fashion MNIST example. Notebook [source] [Colab] Neural Architecture Slides Instead of calculating backpropagation by hand as in the first class , this class uses automatic differentiation built in to PyTorch. This is built on the autograd package which has its own tutorial . One important concept is convolutional neural network layers. The Stanford CS231N class has a good interactive demonstration of convolution. This page shows demonstrations of stride, padding, and dilation. To facilitate the use of Torch, we introduce ignite at the end of class. An example of using ignite is given in this notebook Additional Resources \ud83d\udd17 The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here . Chapter 9 specifically covers convolutional neural networks.","title":"Deep Learning"},{"location":"deep.html#deep-learning","text":"Home Github repository In this class, we construct simple deep ANNs using the PyTorch library on the Fashion MNIST example. Notebook [source] [Colab] Neural Architecture Slides Instead of calculating backpropagation by hand as in the first class , this class uses automatic differentiation built in to PyTorch. This is built on the autograd package which has its own tutorial . One important concept is convolutional neural network layers. The Stanford CS231N class has a good interactive demonstration of convolution. This page shows demonstrations of stride, padding, and dilation. To facilitate the use of Torch, we introduce ignite at the end of class. An example of using ignite is given in this notebook","title":"Deep Learning"},{"location":"deep.html#additional-resources","text":"The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here . Chapter 9 specifically covers convolutional neural networks.","title":"Additional Resources"},{"location":"vision.html","text":"Deep Learning for Computer Vision \ud83d\udd17 Home Github repository Class materials here Deep Learning for Computer Vision , practical session at ISAE-SUPAERO \ud83d\udd17 Introduction \ud83d\udd17 This repository contains the code and documentation of a \"Deep Learning practical session\" given at ISAE-SUPAERO on December 2nd/3rd 2019 and Nov 30/Dec 1st 2020 The introduction slides can be accessed at this URL website . It is recommended to read it first as it contains the necessary information to run this from scratch. There are three notebooks at the root of this repository, those as the exercises Where to run it ? \ud83d\udd17 This hands on session is based on running code & training using Google Cloud Platform Deep Learning VMs , see gcp/ for examples on configuring your own machine. However, this is runnable everywhere since data access is based on public URLs & numpy Should you want to do this at home you can use Google Collaboratory instances - it's even easier than deep learning VMs Usage & Contribution \ud83d\udd17 No support is guaranteed by the authors beyond the hands-on session. This hands-on session was created by Florient Chouteau and Matthieu Le Goff. See licence.md for licence information.","title":"Deep Learning for Computer Vision"},{"location":"vision.html#deep-learning-for-computer-vision","text":"Home Github repository Class materials here","title":"Deep Learning for Computer Vision"},{"location":"vision.html#deep-learning-for-computer-vision-practical-session-at-isae-supaero","text":"","title":"Deep Learning for Computer Vision , practical session at ISAE-SUPAERO"},{"location":"vision.html#introduction","text":"This repository contains the code and documentation of a \"Deep Learning practical session\" given at ISAE-SUPAERO on December 2nd/3rd 2019 and Nov 30/Dec 1st 2020 The introduction slides can be accessed at this URL website . It is recommended to read it first as it contains the necessary information to run this from scratch. There are three notebooks at the root of this repository, those as the exercises","title":"Introduction"},{"location":"vision.html#where-to-run-it","text":"This hands on session is based on running code & training using Google Cloud Platform Deep Learning VMs , see gcp/ for examples on configuring your own machine. However, this is runnable everywhere since data access is based on public URLs & numpy Should you want to do this at home you can use Google Collaboratory instances - it's even easier than deep learning VMs","title":"Where to run it ?"},{"location":"vision.html#usage-contribution","text":"No support is guaranteed by the authors beyond the hands-on session. This hands-on session was created by Florient Chouteau and Matthieu Le Goff. See licence.md for licence information.","title":"Usage &amp; Contribution"}]}