{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Deep Learning","text":"<p>Deep Learning section of the Algorithms in Machine Learning class at ISAE-Supaero</p> <ul> <li>Home</li> <li>Github repository</li> </ul> <p>Adapted from Emmanuel Rachelson's Machine Learning class</p>"},{"location":"index.html#syllabus","title":"Syllabus","text":"<p>This class covers deep learning from a theoretical basis to example applications. We start with simple multi-layer perceptrons, backpropogation, and gradient descent, exploring at the fundamental aspects of deep learning in depth. We cover a wide range of deep learning topics, from Natural Language Processing to Generative Adversarial Networks; the full schedule is below. The goal is that students understand the capacities of deep learning, the current state of the field, and the challenges of using and developing deep learning algorithms. By the end of this class, we expect students that students will be able to understand recent literature in deep learning, implement novel neural network architectures, use and understand the PyTorch library in many ways, and apply deep learning to different domains.</p>"},{"location":"index.html#2024-schedule","title":"2024 Schedule","text":"Schedule 12/11 Artificial Neural Networks ANNs, backpropagation, Stochastic Gradient Descent 12/11 Deep Learning layers, convolution, architectures, training 26/11 Deep Learning for Computer Vision, pt 1 Convolutional Neural Networks, satellite imagery 26/11 Deep Learning for Computer Vision, pt 2 03/12 Image generation VAEs, GANs, and Diffusion Models 09/12 RNNs Recurrent Neural Networks, LSTM, GRU 10/12 Transformers Transformers 17/12 Dimensionality Reduction Autoencoders, t-SNE 17/12 LLMs Large Language Models"},{"location":"ANN.html","title":"Artificial Neural Networks","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class gives a biological and historical overview of artificial neural networks and walks through the theoretical foundations of backpropagation and gradient descent, using an example in numpy.</p> <p>Notebook [source] [Colab]</p> <p>As an additional exercise, this notebook runs the provided backpropagation code, visualizing neural activations and error at each step.</p> <p>Visualizing backprop</p>"},{"location":"ANN.html#additional-resources","title":"Additional Resources","text":"<p>In class we present a Universal Approximation Theorem for single-layer networks with sigmoid activation functions. The slides for that proof are here: Universal Approximation Theorem</p> <p>This online book has a nice interactive visualization of this property of neural networks.</p> <p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here</p> <p>Stanford CS229 Lectures 11 and 12 introduce ANNs, backpropagation, and gradient descent, with lecture 12 going further in depth on how to resolve common training problems in ANNs.</p>"},{"location":"DR.html","title":"Dimensionality Reduction","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class goes deeper in the mechanism and usage of Dimensionality Reduction. You will learn to exploit the effects of dimensionality on your machine learning models, with illustrated intuitions, and practical examples. You will start with an already very known technique in a linear setup, Principal Components Analysis, to move progressively towards non-linear manifold learning with t-SNE, to end up with Deep Learning techniques to reduce the dimension. Through concrete application of Autoencoders you will know what are their main interests and pitfalls, to make the best architecture choice given your problem to solve.</p> <p>Lecture notes</p> <p>Notebook for class exercises (colab)</p> <p>Solutions (colab)</p>"},{"location":"GAN.html","title":"Generative Models","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>In this class, we cover three types of generative models: Variational Autoencoders, Generative Adversarial Networks, and Score-based models (also called diffusion models).</p> <p>VAEs, Colab version</p> <p>GANs, Colab version</p> <p>Score-based models, Colab version</p>"},{"location":"GAN.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Intuitively Understanding Variational Autoencoders</li> <li>Understanding Variational Autoencoders (VAEs)</li> <li>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 20 covers generative models and section 20.10.4 specifically covers GANs.</li> <li>Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \"Score-Based Generative Modeling through Stochastic Differential Equations.\" International Conference on Learning Representations, 2021.</li> <li>Jonathan Ho, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in Neural Information Processing Systems. 2020.</li> <li>Yang Song, and Stefano Ermon. \"Generative modeling by estimating gradients of the data distribution.\" Advances in Neural Information Processing Systems. 2019.</li> </ul>"},{"location":"NLP.html","title":"Natural Language Processing","text":"<ul> <li>Home</li> <li> <p>Github repository</p> </li> <li> <p>Introduction to NLP slides</p> </li> <li> <p>Sequence to Sequence Learning, version colab</p> </li> <li> <p>Attention in Seq2Seq, version colab</p> </li> <li> <p>Large Language Models and Generative AI slides</p> </li> </ul>"},{"location":"NLP.html#additional-resources","title":"Additional Resources","text":"<ul> <li>Text Processing and Classification, version colab</li> <li>Entity Recognition and Chatbots, version colab</li> <li>Word2Vec Exercise, version colab</li> <li>MOOC of 10-20h on linguistics</li> <li>blog of Jay Alamar</li> <li>blog of Sebastian Ruder</li> <li>Explosion.ai, creators of spacy</li> <li>Rasa blog</li> </ul>"},{"location":"RNN.html","title":"Recurrent Neural Networks","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>Introduction</p> <ul> <li>Notebook</li> <li>Colab</li> </ul> <p>Time-series Forecasting</p> <ul> <li>Notebook</li> <li>Colab</li> </ul>"},{"location":"RNN.html#additional-resources","title":"Additional Resources","text":"<p>Time series forecasting tutorial in Tensorflow.</p> <p>Stanford's CS231n lecture on Recurrent Neural Networks, covering RNNs, backpropagation through time, and LSTMs.</p> <p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 10 covers recurrent neural networks.</p>"},{"location":"deep.html","title":"Deep Learning","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>In this class, we construct simple deep ANNs using the PyTorch library on the Fashion MNIST example.</p> <p>Notebook [source] [Colab]</p> <p>Neural Architecture Slides</p> <p>Instead of calculating backpropagation by hand as in the first class, this class uses automatic differentiation built in to PyTorch. This is built on the autograd package which has its own tutorial.</p> <p>One important concept is convolutional neural network layers. The Stanford CS231N class has a good interactive demonstration of convolution. This page shows demonstrations of stride, padding, and dilation.</p> <p>To facilitate the use of Torch, we introduce ignite at the end of class. An example of using ignite is given in this notebook</p>"},{"location":"deep.html#additional-resources","title":"Additional Resources","text":"<p>The deep learning book is fully available online and contains many great examples. Notebook versions of those examples are available here. Chapter 9 specifically covers convolutional neural networks.</p>"},{"location":"evaluation.html","title":"Evaluation","text":"<p>Evaluation 2024-2025.</p>"},{"location":"evaluation.html#principe-et-objectif-pedagogique","title":"Principe et objectif p\u00e9dagogique","text":"<p>Le principe est celui de la co-\u00e9valuation et le but est que le temps que vous consacrez \u00e0 l'\u00e9valuation soit un temps o\u00f9 vous continuez \u00e0 d\u00e9couvrir des choses nouvelles et \u00e0 approfondir votre ma\u00eetrise en ML. Le fichier que je vous enverrai tr\u00e8s rapidement vous indique, pour chaque \u00e9tudiant.e, un sujet \u00e0 traiter. Le but de l'exercice est de r\u00e9diger un notebook pr\u00e9sentant le sujet, comme si vous le pr\u00e9sentiez \u00e0 vos pairs (\u00e9tudiant SDD, coll\u00e8gue de travail, client comp\u00e9tent sur le sujet, etc.). On n'apprend jamais aussi bien que quand on explique, c'est donc l'occasion de bien ma\u00eetriser un sujet de plus et de monter en comp\u00e9tence collectivement en portant un regard critique sur nos productions respectives. </p>"},{"location":"evaluation.html#point-de-depart","title":"Point de d\u00e9part","text":"<p>Chaque sujet est associ\u00e9 \u00e0 une ou deux r\u00e9f\u00e9rences en ligne. Ce sont des points de d\u00e9part, pas des contenus \u00e0 r\u00e9sumer ! Il est normal que vous lisiez des documents compl\u00e9mentaires, que vous construisiez votre compr\u00e9hension, que vous fassiez des choix de ce que vous pr\u00e9sentez ou non dans votre notebook (voire dans certains cas, que vous fassiez le choix de traiter sp\u00e9cifiquement un aspect du sujet et pas un autre - dans ce cas l\u00e0, parlez m'en svp).</p>"},{"location":"evaluation.html#consignes-et-criteres-devaluation","title":"Consignes et crit\u00e8res d'\u00e9valuation","text":"<p>Votre notebook doit \u00eatre didactique, agr\u00e9able \u00e0 lire, avoir un bon \u00e9quilibre entre aspects formels et pratiques. Il doit \u00eatre jouable en environ une heure. Selon les sujets, vous aurez plus ou moins \u00e0 faire d'efforts pour illustrer le sujet en pratique ou \u00e0 rendre la th\u00e9orie accessible. La langue de r\u00e9daction est le fran\u00e7ais ou l'anglais, selon votre pr\u00e9f\u00e9rence (et la qualit\u00e9 de la langue contribue \u00e0 des notebooks agr\u00e9ables \u00e0 lire, donc \u00e0 la note). Votre notebook doit \u00eatre rigoureux : ce n'est pas de la vulgarisation scientifique, vous vous devez d'\u00eatre pr\u00e9cis et rigoureux. Ca ne vous oblige pas \u00e0 r\u00e9diger syst\u00e9matiquement des preuves math\u00e9matiques mais \u00e7a n\u00e9cessite de formuler et de discuter des id\u00e9es et r\u00e9sultats de fa\u00e7on pr\u00e9cise et argument\u00e9e. Votre notebook doit \u00e9galement \u00eatre utile et r\u00e9utilisable, comporter des \u00e9l\u00e9ments (dessins, code, texte) qui permettront au lecteur d'\u00eatre rapidement fonctionnel sur le sujet. Votre notebook doit \u00eatre document\u00e9 : les aspects non-abord\u00e9s ou les extensions peuvent pointer vers des ressources en ligne ou des \u00e9l\u00e9ments bibliographiques, les id\u00e9es avanc\u00e9es doivent \u00eatre soutenues par des r\u00e9f\u00e9rences. Vous pouvez joindre des annexes. Important : votre notebook doit \u00eatre anonyme ! Rien ne doit permettre de vous identifier (nom, pseudo github, url, dessin, ou autre). La deadline de rendu des notebooks est le 9 f\u00e9vrier \u00e0 23h59.</p>"},{"location":"evaluation.html#evaluation","title":"Evaluation","text":"<p>Le 10 f\u00e9vrier, nous avons une demi-journ\u00e9e o\u00f9 nous jouerons les notebooks. Le but sera d'\u00e9valuer chaque notebook, en bin\u00f4me, pendant la premi\u00e8re heure, d'avoir le temps de r\u00e9diger une \u00e9valuation (que vous pourrez \u00e9ventuellement corriger plus tard), de prendre une pause, puis de recommencer avec un second notebook. De cette fa\u00e7on, tous les notebooks auront re\u00e7u une \u00e9valuation. On r\u00e9p\u00e9tera cette op\u00e9ration pour deux autres notebooks, \u00e0 la maison. Ce qui donnera une seconde \u00e9valuation \u00e0 chaque notebook. Les bin\u00f4mes d'\u00e9valuateurs seront diff\u00e9rents pour chaque notebook. Jouer et noter les notebooks en bin\u00f4me est important car cela vous permet d'en discuter au fil du notebook. Les \u00e9valuations seront constitu\u00e9es de notes num\u00e9riques et d'\u00e9l\u00e9ments textuels o\u00f9 le bin\u00f4me \u00e9valuateur devra r\u00e9sumer sa compr\u00e9hension du notebook et argumenter sur les points forts et les points faibles. C'est l'occasion de faire un retour constructif et anonyme \u00e0 l'auteur du notebook, sous la forme d'une \u00e9valuation argument\u00e9e et objective.  A l'issue de cette s\u00e9ance, chaque notebook aura donc re\u00e7u 2 \u00e9valuations (descern\u00e9es par un bin\u00f4me chacune) et vous aurez chacun.e d\u00e9couvert 5 nouveaux sujets (en comptant celui que vous aurez r\u00e9dig\u00e9). Vous pourrez modifier vos \u00e9valuations jusqu'au vendredi 14 f\u00e9vrier. Notez que tous les notebooks seront accessibles sous forme anonyme \u00e0 tous les \u00e9valuateurs. Donc si vous \u00eates tr\u00e8s int\u00e9ress\u00e9.e par un sujet particulier, vous pourrez en consulter le notebook et m\u00eame contribuer une \u00e9valuation suppl\u00e9mentaire. A la fin, je compilerai toutes les \u00e9valuations et notes pour en tirer une \u00e9valuation unifi\u00e9e.</p>"},{"location":"evaluation.html#rendu","title":"Rendu","text":"<p>La soumission anonyme des notebooks et leur \u00e9valuation (anonyme aussi) se feront sur https://openreview.net/group?id=supaerodatascience.github.io/SupaeroSDD/2025/Workshop. Openreview est la plate-forme de reviewing (d'\u00e9valuation par les pairs) utilis\u00e9e par certaines des plus grandes conf\u00e9rences et journaux en machine learning (par exemple, ICLR, NeurIPS, ou TMLR utilisent openreview). C'est donc l'occasion de vous y cr\u00e9er un compte d\u00e8s maintenant et d'exp\u00e9rimenter et de participer \u00e0 un processus de review complet.  Je vous demande d\u00e8s maintenant 1) de vous y cr\u00e9er un compte, puis 2) de cr\u00e9er une soumission avec le titre de votre notebook (sans y joindre le notebook que vous n'avez pas encore r\u00e9dig\u00e9). Cela va me permettre d'affecter les reviewers sans attendre la derni\u00e8re minute. Vous pourrez par la suite modifier votre soumission autant que vous le souhaiterez jusqu'\u00e0 la deadline. La deadline (ferme) de soumission est le dimanche 11/02 \u00e0 minuit. Pr\u00e9cision 1 : Dans le champ de soumission, \"author\" c'est vous (pas les auteurs des sources sur lesquelles vous vous appuyez). Mettez votre nom, il sera cach\u00e9 aux reviewers (c'est fait pour). Pr\u00e9cision 2 : L'abstract est celui que vous r\u00e9digez. C'est une r\u00e9sum\u00e9 de votre notebook (pas des sources que vous avez utilis\u00e9es). Pr\u00e9cision 3 : Il vous est demand\u00e9 de soumettre un fichier zip. Cela vous permet de mettre des documents annexe \u00e0 votre notebook (bibliographie, correction d'exercices, mod\u00e8le pr\u00e9-entra\u00een\u00e9, etc.).</p>"},{"location":"evaluation.html#workshop-sdd-et-posters","title":"Workshop SDD et  posters","text":"<p>Une grande nouveaut\u00e9 de cette ann\u00e9e est de donner une suite \u00e0 cet exercice.  Autour du parcours SDD, il y a plein d'anciens (le parcours f\u00eatera ses 10 ans l'an prochain), de vacataires, de chercheurs, d'ing\u00e9nieurs, qui connaissent la formation, y contribuent ou interagissent avec nous plus ou moins directement. Pour la premi\u00e8re fois, nous allons tenter d'organiser un workshop SDD pour r\u00e9unir cette communaut\u00e9 (vous compris). Ce workshop se tiendra la journ\u00e9e du 14 mars, pour clore l'ann\u00e9e, et le programme est en construction. Mais ce programme inclura un temps autour du travail des notebooks.  Le lien avec les notebooks est que chaque auteur aura comme t\u00e2che de reprendre son sujet, et d'exploiter les retours anonymes des \u00e9valuateurs pour l'am\u00e9liorer, le corriger et en faire un poster, que nous imprimerons (au format A0 ou A1, \u00e0 pr\u00e9ciser). La deadline de rendu de ces posters sera le 6 mars, pour empi\u00e9ter le moins possible sur les cours in-depth qui commencent le 5, tout en vous laissant de la flexibilit\u00e9 pour pr\u00e9parer les \u00e9valuations de PIE et en nous laissant le temps de faire toutes les impressions de posters pour le 14. Un cr\u00e9neau de 2h pendant le workshop sera d\u00e9di\u00e9 \u00e0 cette session poster o\u00f9 chacun pourra aller voir les posters pr\u00e9sent\u00e9s par leurs auteurs, discuter et \u00e9changer librement (et si j'y arrive on fournira caf\u00e9 et viennoiseries). Je demanderai \u00e0 certains externes de venir \u00e9valuer les posters, ce qui viendra moduler la note \u00e9tablie le 14 f\u00e9vrier.</p>"},{"location":"evaluation.html#votre-cv","title":"Votre CV","text":"<p>Ces notebooks et ces posters font partie de votre portfolio. Mettez-les sur votre github, mentionnez-les en entretien, valorisez-les quand vous candidatez quelque part. C'est un exercice tr\u00e8s chouette \u00e0 faire pendant l'ann\u00e9e, tr\u00e8s original (assez unique en fait), et c'est un travail compl\u00e8tement valorisable dans votre CV !</p>"},{"location":"evaluation.html#recommandations-de-travail","title":"Recommandations de travail","text":"<p>La r\u00e9daction d'un notebook prend du temps, mais c'est aussi une des meilleures mani\u00e8res d'apprendre en profondeur. Voici un petit timing type que je vous recommande, liss\u00e9 sur 6 \u00e0 7 fois 3h de travail.  S\u00e9ance 1 : recherche de sources et lecture efficace. Chaque notebook dispose d'une indication bibliographique, \u00e0 vous de la lire efficacement et de chercher des sources compl\u00e9mentaires pour mieux comprendre ou apporter un \u00e9clairage diff\u00e9rent. S\u00e9ance 2 : lecture approfondie et exp\u00e9rience pratique du sujet (code, exploration personnelle de la th\u00e9orie). Cette s\u00e9ance demandera peut-\u00eatre \u00e0 \u00eatre doubl\u00e9e. S\u00e9ance 3 : d\u00e9cision sur la trame du notebook et \u00e9bauche de r\u00e9daction. S\u00e9ance 4 : r\u00e9daction. S\u00e9ance 5 : r\u00e9daction. S\u00e9ance 6 : relecture et corrections. Pour nous assurer que vous aurez du temps et suite \u00e0 mon absence de novembre et d\u00e9cembre, nous avons r\u00e9am\u00e9nag\u00e9 l'emploi du temps, d\u00e9cal\u00e9 l'\u00e9valuation (qui \u00e9tait initialement pr\u00e9vue le 28/01) et s\u00e9curis\u00e9 des demi-journ\u00e9es dans votre emploi du temps afin que vous ayez du temps pour pr\u00e9parer vos notebooks. Ainsi, l'apr\u00e8s-midi du 20/01 et la journ\u00e9e compl\u00e8te du 28/01 sont des cr\u00e9neaux lib\u00e9r\u00e9s pour que vous travailliez sur les notebooks. Ces trois demi-journ\u00e9es ne sont pas suffisantes pour bien \u00e9crire un notebook : il faut, en plus, du travail sur votre temps \u00e0 la maison !</p>"},{"location":"evaluation.html#resume-du-calendrier","title":"R\u00e9sum\u00e9 du calendrier","text":"<p>17/01 : consignes et affectation sujets 20/01 : demi-journ\u00e9e lib\u00e9r\u00e9e 28/01 : journ\u00e9e compl\u00e8te lib\u00e9r\u00e9e 09/02 : deadline de rendu des notebooks sur https://openreview.net/group?id=supaerodatascience.github.io/SupaeroSDD/2025/Workshop 10/02 : demi-journ\u00e9e d'\u00e9valuation des notebooks 14/02 : deadline pour rendre les \u00e9valuations des notebooks 06/03 : deadline pour rendre les posters 14/03 : workshop SDD</p>"},{"location":"transformers.html","title":"Transformers","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>This class introduces attention mechanisms in detail and presents the Transformer architecture, with an example in NLP. It builds on the NLP class.</p> <p>Attention notebook source</p> <p>Attention notebook on Colab</p> <p>Transformer notebook source</p> <p>Transformer notebook on Colab</p>"},{"location":"transformers.html#additional-resources","title":"Additional Resources","text":"<ul> <li>The Illustrated Transformer</li> <li>Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems 30, 2017. pdf</li> <li>Brown, Tom, et al. \"Language models are few-shot learners.\" Advances in neural information processing systems 33 (2020): 1877-1901. pdf</li> <li>Raffel, Colin, et al. \"Exploring the limits of transfer learning with a unified text-to-text transformer.\" J. Mach. Learn. Res. 21.140 (2020): 1-67. pdf</li> <li>t5 on HuggingFace</li> <li>c4 dataset</li> <li>Hoffmann, Jordan, et al. \"Training Compute-Optimal Large Language Models.\" arXiv preprint arXiv:2203.15556 (2022). pdf</li> <li>Taylor, Ross, et al. \"Galactica: A large language model for science.\" arXiv preprint arXiv:2211.09085 (2022). pdf</li> <li>ChatGPT</li> </ul>"},{"location":"transformers.html#vision-transformers","title":"Vision Transformers","text":"<ul> <li>Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020). pdf</li> <li>ViT - Papers with code</li> <li>Wightman, Ross, Hugo Touvron, and Herv\u00e9 J\u00e9gou. \"Resnet strikes back: An improved training procedure in timm.\" arXiv preprint arXiv:2110.00476 (2021). pdf</li> <li>Liu, Zhuang, et al. \"A convnet for the 2020s.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. pdf</li> </ul>"},{"location":"vision.html","title":"Deep Learning for Computer Vision","text":"<ul> <li>Home</li> <li>Github repository</li> </ul> <p>Class materials here</p>"},{"location":"vision.html#deep-learning-for-computer-vision-practical-session-at-isae-supaero","title":"Deep Learning for Computer Vision , practical session at ISAE-SUPAERO","text":""},{"location":"vision.html#introduction","title":"Introduction","text":"<p>This repository contains the code and documentation of a \"Deep Learning practical session\" given at ISAE-SUPAERO on December 2nd/3rd 2019 and Nov 30/Dec 1st 2020</p> <p>The introduction slides can be accessed at this URL website. It is recommended to read it first as it contains the necessary information to run this from scratch.</p> <p>There are three notebooks at the root of this repository, those as the exercises</p>"},{"location":"vision.html#where-to-run-it","title":"Where to run it ?","text":"<p>This hands on session is based on running code &amp; training using Google Cloud Platform Deep Learning VMs, see <code>gcp/</code> for examples on configuring your own machine. </p> <p>However, this is runnable everywhere since data access is based on public URLs &amp; numpy</p> <p>Should you want to do this at home you can use Google Collaboratory instances - it's even easier than deep learning VMs</p>"},{"location":"vision.html#usage-contribution","title":"Usage &amp; Contribution","text":"<p>No support is guaranteed by the authors beyond the hands-on session.</p> <p>This hands-on session was created by Florient Chouteau and Matthieu Le Goff.</p> <p>See <code>licence.md</code> for licence information.</p>"}]}