{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ecabda2",
   "metadata": {
    "id": "2iUXCk7tC1x5",
    "tags": []
   },
   "source": [
    "# Session 1 : Training your first aircraft classifier with pytorch\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Florient Chouteau | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b37e03",
   "metadata": {
    "id": "yfn1RtChC1yD",
    "tags": []
   },
   "source": [
    "## Intro\n",
    "\n",
    "The objectives of this session is to apply what you learned during [the previous class on Deep Learning](https://supaerodatascience.github.io/deep-learning/) on a real dataset of satellite images.\n",
    "\n",
    "Most of the vocabulary and concepts of Deep Learning and Convolutionnal Neural Network has been defined on the class linked above so you should refer to it.\n",
    "\n",
    "In this session you will:\n",
    "- Train a basic NN on a basic dataset\n",
    "- Plot ROC curve & confusion matrix to diagnose your dataset\n",
    "\n",
    "During session 2 you will be experimenting with harder datasets\n",
    "\n",
    "If you haven't done so, go to the previous notebooks to get a hands on pytorch and CNNs\n",
    "\n",
    "\n",
    "**First steps**\n",
    "- Activate the GPU runtime in colab\n",
    "- Check using `!nvidia-smi` that you detect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac1284",
   "metadata": {
    "id": "xEo4VpHqC1yF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931d970",
   "metadata": {
    "id": "FG3_sWsWC1yH"
   },
   "outputs": [],
   "source": [
    "# Put your imports here\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428d2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils Definition / Ignore & hide\n",
    "\n",
    "Execute once and hide this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS FUNCTION THIS IS AN UTIL, just execute once\n",
    "# Copyright The PyTorch Lightning team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.hooks import RemovableHandle\n",
    "\n",
    "PARAMETER_NUM_UNITS = [\" \", \"K\", \"M\", \"B\", \"T\"]\n",
    "UNKNOWN_SIZE = \"?\"\n",
    "\n",
    "\n",
    "class LayerSummary:\n",
    "    \"\"\"Summary class for a single layer in a :class:`~torch.nn.Module`.\n",
    "\n",
    "    It collects the following information:\n",
    "\n",
    "    * Type of the layer (e.g. Linear, BatchNorm1d, ...)\n",
    "    * Input shape\n",
    "    * Output shape\n",
    "    * Number of parameters\n",
    "\n",
    "    The input and output shapes are only known after the example input array was\n",
    "    passed through the model.\n",
    "\n",
    "    Example::\n",
    "        >>> model = torch.nn.Conv2d(3, 8, 3)\n",
    "        >>> summary = LayerSummary(model)\n",
    "        >>> summary.num_parameters\n",
    "        224\n",
    "        >>> summary.layer_type\n",
    "        'Conv2d'\n",
    "        >>> output = model(torch.rand(1, 3, 5, 5))\n",
    "        >>> summary.in_size\n",
    "        [1, 3, 5, 5]\n",
    "        >>> summary.out_size\n",
    "        [1, 8, 3, 3]\n",
    "\n",
    "    Args:\n",
    "        module: A module to summarize\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self._module = module\n",
    "        self._hook_handle = self._register_hook()\n",
    "        self._in_size: Optional[Union[str, List]] = None\n",
    "        self._out_size: Optional[Union[str, List]] = None\n",
    "\n",
    "    def __del__(self) -> None:\n",
    "        self.detach_hook()\n",
    "\n",
    "    def _register_hook(self) -> Optional[RemovableHandle]:\n",
    "        \"\"\"Registers a hook that computes the input/output size(s) on the first forward pass.\n",
    "\n",
    "        If the hook is called, it will remove itself from the from the module, meaning that\n",
    "        recursive models will only record their input- and output shapes once.\n",
    "\n",
    "        Registering hooks on :class:`~torch.jit.ScriptModule` is not supported.\n",
    "\n",
    "        Return:\n",
    "            A handle for the installed hook, or ``None`` if registering the hook is not possible.\n",
    "        \"\"\"\n",
    "\n",
    "        def hook(_: nn.Module, inp: Any, out: Any) -> None:\n",
    "            if len(inp) == 1:\n",
    "                inp = inp[0]\n",
    "            self._in_size = parse_batch_shape(inp)\n",
    "            self._out_size = parse_batch_shape(out)\n",
    "            assert self._hook_handle is not None\n",
    "            self._hook_handle.remove()\n",
    "\n",
    "        handle = None\n",
    "        if not isinstance(self._module, torch.jit.ScriptModule):\n",
    "            handle = self._module.register_forward_hook(hook)\n",
    "        return handle\n",
    "\n",
    "    def detach_hook(self) -> None:\n",
    "        \"\"\"Removes the forward hook if it was not already removed in the forward pass.\n",
    "\n",
    "        Will be called after the summary is created.\n",
    "        \"\"\"\n",
    "        if self._hook_handle is not None:\n",
    "            self._hook_handle.remove()\n",
    "\n",
    "    @property\n",
    "    def in_size(self) -> Union[str, List]:\n",
    "        return self._in_size or UNKNOWN_SIZE\n",
    "\n",
    "    @property\n",
    "    def out_size(self) -> Union[str, List]:\n",
    "        return self._out_size or UNKNOWN_SIZE\n",
    "\n",
    "    @property\n",
    "    def layer_type(self) -> str:\n",
    "        \"\"\"Returns the class name of the module.\"\"\"\n",
    "        return str(self._module.__class__.__name__)\n",
    "\n",
    "    @property\n",
    "    def num_parameters(self) -> int:\n",
    "        \"\"\"Returns the number of parameters in this module.\"\"\"\n",
    "        return sum(\n",
    "            np.prod(p.shape) if not _is_lazy_weight_tensor(p) else 0\n",
    "            for p in self._module.parameters()\n",
    "        )\n",
    "\n",
    "\n",
    "class ModelSummary:\n",
    "    \"\"\"Generates a summary of all layers in a :class:`~torch.nn.Module`.\n",
    "\n",
    "    Args:\n",
    "        model: The model to summarize (also referred to as the root module).\n",
    "        max_depth: Maximum depth of modules to show. Use -1 to show all modules or 0 to show no\n",
    "            summary. Defaults to 1.\n",
    "        example_input_array (torch.Tensor): If provided, and example input aray which will be used\n",
    "            to infer the shape of tensors throughout the model.\n",
    "\n",
    "    The string representation of this summary prints a table with columns containing\n",
    "    the name, type and number of parameters for each layer.\n",
    "    The root module may also have an attribute ``example_input_array`` as shown in the example\n",
    "    below.\n",
    "\n",
    "    If present, the root module will be called with it as input to determine the\n",
    "    intermediate input- and output shapes of all layers. Supported are tensors and\n",
    "    nested lists and tuples of tensors. All other types of inputs will be skipped and show as `?`\n",
    "    in the summary table. The summary will also display `?` for layers not used in the forward pass.\n",
    "\n",
    "    Example::\n",
    "        >>> from torch.nn import Module\n",
    "        >>> class LitModel(Module):\n",
    "        ...\n",
    "        ...     def __init__(self):\n",
    "        ...         super().__init__()\n",
    "        ...         self.net = nn.Sequential(nn.Linear(256, 512), nn.BatchNorm1d(512))\n",
    "        ...         self.example_input_array = torch.zeros(10, 256)  # optional\n",
    "        ...\n",
    "        ...     def forward(self, x):\n",
    "        ...         return self.net(x)\n",
    "        ...\n",
    "        >>> model = LitModel()\n",
    "        >>> ModelSummary(model, max_depth=1)  # doctest: +NORMALIZE_WHITESPACE\n",
    "          | Name | Type       | Params | In sizes  | Out sizes\n",
    "        ------------------------------------------------------------\n",
    "        0 | net  | Sequential | 132 K  | [10, 256] | [10, 512]\n",
    "        ------------------------------------------------------------\n",
    "        132 K     Trainable params\n",
    "        0         Non-trainable params\n",
    "        132 K     Total params\n",
    "        0.530     Total estimated model params size (MB)\n",
    "        >>> ModelSummary(model, max_depth=-1)  # doctest: +NORMALIZE_WHITESPACE\n",
    "          | Name  | Type        | Params | In sizes  | Out sizes\n",
    "        --------------------------------------------------------------\n",
    "        0 | net   | Sequential  | 132 K  | [10, 256] | [10, 512]\n",
    "        1 | net.0 | Linear      | 131 K  | [10, 256] | [10, 512]\n",
    "        2 | net.1 | BatchNorm1d | 1.0 K    | [10, 512] | [10, 512]\n",
    "        --------------------------------------------------------------\n",
    "        132 K     Trainable params\n",
    "        0         Non-trainable params\n",
    "        132 K     Total params\n",
    "        0.530     Total estimated model params size (MB)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, max_depth=1, example_input_array=None) -> None:\n",
    "        self._model = model\n",
    "\n",
    "        # temporary mapping from mode to max_depth\n",
    "        if not isinstance(max_depth, int) or max_depth < -1:\n",
    "            raise ValueError(f\"`max_depth` can be -1, 0 or > 0, got {max_depth}.\")\n",
    "\n",
    "        self._max_depth = max_depth\n",
    "        self._example_input_array = example_input_array\n",
    "        self._layer_summary = self.summarize(example_input_array=example_input_array)\n",
    "        # 1 byte -> 8 bits\n",
    "        # TODO: how do we compute precision_megabytes in case of mixed precision?\n",
    "        precision = 32  # Fixme: Get precision from global dtype attribute\n",
    "        self._precision_megabytes = (precision / 8.0) * 1e-6\n",
    "\n",
    "    @property\n",
    "    def named_modules(self) -> List[Tuple[str, nn.Module]]:\n",
    "        mods: List[Tuple[str, nn.Module]]\n",
    "        if self._max_depth == 0:\n",
    "            mods = []\n",
    "        elif self._max_depth == 1:\n",
    "            # the children are the top-level modules\n",
    "            mods = list(self._model.named_children())\n",
    "        else:\n",
    "            mods = self._model.named_modules()\n",
    "            mods = list(mods)[1:]  # do not include root module (LightningModule)\n",
    "        return mods\n",
    "\n",
    "    @property\n",
    "    def layer_names(self) -> List[str]:\n",
    "        return list(self._layer_summary.keys())\n",
    "\n",
    "    @property\n",
    "    def layer_types(self) -> List[str]:\n",
    "        return [layer.layer_type for layer in self._layer_summary.values()]\n",
    "\n",
    "    @property\n",
    "    def in_sizes(self) -> List:\n",
    "        return [layer.in_size for layer in self._layer_summary.values()]\n",
    "\n",
    "    @property\n",
    "    def out_sizes(self) -> List:\n",
    "        return [layer.out_size for layer in self._layer_summary.values()]\n",
    "\n",
    "    @property\n",
    "    def param_nums(self) -> List[int]:\n",
    "        return [layer.num_parameters for layer in self._layer_summary.values()]\n",
    "\n",
    "    @property\n",
    "    def total_parameters(self) -> int:\n",
    "        return sum(\n",
    "            p.numel() if not _is_lazy_weight_tensor(p) else 0\n",
    "            for p in self._model.parameters()\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def trainable_parameters(self) -> int:\n",
    "        return sum(\n",
    "            p.numel() if not _is_lazy_weight_tensor(p) else 0\n",
    "            for p in self._model.parameters()\n",
    "            if p.requires_grad\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def model_size(self) -> float:\n",
    "        # todo: seems it does not work with quantized models - it returns 0.0\n",
    "        return self.total_parameters * self._precision_megabytes\n",
    "\n",
    "    def summarize(self, example_input_array=None) -> Dict[str, LayerSummary]:\n",
    "        summary = OrderedDict(\n",
    "            (name, LayerSummary(module)) for name, module in self.named_modules\n",
    "        )\n",
    "\n",
    "        if example_input_array is not None:\n",
    "            self._forward_example_input(example_input_array)\n",
    "\n",
    "        for layer in summary.values():\n",
    "            layer.detach_hook()\n",
    "\n",
    "        if self._max_depth >= 1:\n",
    "            # remove summary entries with depth > max_depth\n",
    "            for k in [k for k in summary if k.count(\".\") >= self._max_depth]:\n",
    "                del summary[k]\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def _forward_example_input(self, example_input_array) -> None:\n",
    "        \"\"\"Run the example input through each layer to get input- and output sizes.\"\"\"\n",
    "        model = self._model\n",
    "\n",
    "        mode = model.training\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # let the model hooks collect the input- and output shapes\n",
    "            if isinstance(example_input_array, (list, tuple)):\n",
    "                model(*example_input_array)\n",
    "            elif isinstance(example_input_array, dict):\n",
    "                model(**example_input_array)\n",
    "            else:\n",
    "                model(example_input_array)\n",
    "        model.train(mode)  # restore mode of module\n",
    "\n",
    "    def _get_summary_data(self) -> List[Tuple[str, List[str]]]:\n",
    "        \"\"\"Makes a summary listing with:\n",
    "\n",
    "        Layer Name, Layer Type, Number of Parameters, Input Sizes, Output Sizes, Model Size\n",
    "        \"\"\"\n",
    "        arrays = [\n",
    "            (\" \", list(map(str, range(len(self._layer_summary))))),\n",
    "            (\"Name\", self.layer_names),\n",
    "            (\"Type\", self.layer_types),\n",
    "            (\"Params\", list(map(get_human_readable_count, self.param_nums))),\n",
    "        ]\n",
    "        if self._example_input_array is not None:\n",
    "            arrays.append((\"In sizes\", [str(x) for x in self.in_sizes]))\n",
    "            arrays.append((\"Out sizes\", [str(x) for x in self.out_sizes]))\n",
    "\n",
    "        return arrays\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        arrays = self._get_summary_data()\n",
    "\n",
    "        total_parameters = self.total_parameters\n",
    "        trainable_parameters = self.trainable_parameters\n",
    "        model_size = self.model_size\n",
    "\n",
    "        return _format_summary_table(\n",
    "            total_parameters, trainable_parameters, model_size, *arrays\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "def parse_batch_shape(batch: Any) -> Union[str, List]:\n",
    "    if hasattr(batch, \"shape\"):\n",
    "        return list(batch.shape)\n",
    "\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        shape = [parse_batch_shape(el) for el in batch]\n",
    "        return shape\n",
    "\n",
    "    return UNKNOWN_SIZE\n",
    "\n",
    "\n",
    "def _format_summary_table(\n",
    "    total_parameters: int,\n",
    "    trainable_parameters: int,\n",
    "    model_size: float,\n",
    "    *cols: Tuple[str, List[str]],\n",
    ") -> str:\n",
    "    \"\"\"Takes in a number of arrays, each specifying a column in the summary table, and combines them all into one\n",
    "    big string defining the summary table that are nicely formatted.\"\"\"\n",
    "    n_rows = len(cols[0][1])\n",
    "    n_cols = 1 + len(cols)\n",
    "\n",
    "    # Get formatting width of each column\n",
    "    col_widths = []\n",
    "    for c in cols:\n",
    "        col_width = max(len(str(a)) for a in c[1]) if n_rows else 0\n",
    "        col_width = max(col_width, len(c[0]))  # minimum length is header length\n",
    "        col_widths.append(col_width)\n",
    "\n",
    "    # Formatting\n",
    "    s = \"{:<{}}\"\n",
    "    total_width = sum(col_widths) + 3 * n_cols\n",
    "    header = [s.format(c[0], l) for c, l in zip(cols, col_widths)]\n",
    "\n",
    "    # Summary = header + divider + Rest of table\n",
    "    summary = \" | \".join(header) + \"\\n\" + \"-\" * total_width\n",
    "    for i in range(n_rows):\n",
    "        line = []\n",
    "        for c, l in zip(cols, col_widths):\n",
    "            line.append(s.format(str(c[1][i]), l))\n",
    "        summary += \"\\n\" + \" | \".join(line)\n",
    "    summary += \"\\n\" + \"-\" * total_width\n",
    "\n",
    "    summary += \"\\n\" + s.format(get_human_readable_count(trainable_parameters), 10)\n",
    "    summary += \"Trainable params\"\n",
    "    summary += \"\\n\" + s.format(\n",
    "        get_human_readable_count(total_parameters - trainable_parameters), 10\n",
    "    )\n",
    "    summary += \"Non-trainable params\"\n",
    "    summary += \"\\n\" + s.format(get_human_readable_count(total_parameters), 10)\n",
    "    summary += \"Total params\"\n",
    "    summary += \"\\n\" + s.format(get_formatted_model_size(model_size), 10)\n",
    "    summary += \"Total estimated model params size (MB)\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def get_formatted_model_size(total_model_size: float) -> str:\n",
    "    return f\"{total_model_size:,.3f}\"\n",
    "\n",
    "\n",
    "def get_human_readable_count(number: int) -> str:\n",
    "    \"\"\"Abbreviates an integer number with K, M, B, T.\n",
    "\n",
    "    Examples:\n",
    "        >>> get_human_readable_count(123)\n",
    "        '123  '\n",
    "        >>> get_human_readable_count(1234)  # (one thousand)\n",
    "        '1.2 K'\n",
    "        >>> get_human_readable_count(2e6)   # (two million)\n",
    "        '2.0 M'\n",
    "        >>> get_human_readable_count(3e9)   # (three billion)\n",
    "        '3.0 B'\n",
    "        >>> get_human_readable_count(4e14)  # (four hundred trillion)\n",
    "        '400 T'\n",
    "        >>> get_human_readable_count(5e15)  # (more than trillion)\n",
    "        '5,000 T'\n",
    "\n",
    "    Args:\n",
    "        number: a positive integer number\n",
    "\n",
    "    Return:\n",
    "        A string formatted according to the pattern described above.\n",
    "    \"\"\"\n",
    "    assert number >= 0\n",
    "    labels = PARAMETER_NUM_UNITS\n",
    "    num_digits = int(np.floor(np.log10(number)) + 1 if number > 0 else 1)\n",
    "    num_groups = int(np.ceil(num_digits / 3))\n",
    "    num_groups = min(num_groups, len(labels))  # don't abbreviate beyond trillions\n",
    "    shift = -3 * (num_groups - 1)\n",
    "    number = number * (10**shift)\n",
    "    index = num_groups - 1\n",
    "    if index < 1 or number >= 100:\n",
    "        return f\"{int(number):,d} {labels[index]}\"\n",
    "\n",
    "    return f\"{number:,.1f} {labels[index]}\"\n",
    "\n",
    "\n",
    "def _is_lazy_weight_tensor(p: Tensor) -> bool:\n",
    "    from torch.nn.parameter import UninitializedParameter\n",
    "\n",
    "    if isinstance(p, UninitializedParameter):\n",
    "        warnings.warn(\n",
    "            \"A layer with UninitializedParameter was found. \"\n",
    "            \"Thus, the total number of parameters detected may be inaccurate.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def summarize(module, max_depth=None, example_input_array=None) -> ModelSummary:\n",
    "    \"\"\"Summarize a PyTorch module.\n",
    "\n",
    "    Args:\n",
    "        module: The module to summarize.\n",
    "        max_depth: The maximum depth of layer nesting that the summary will include. A value of 0\n",
    "            turns the layer summary off. Default: 1.\n",
    "        example_input_array (torch.Tensor): If provided, and example input aray which will be used\n",
    "            to infer the shape of tensors throughout the model.\n",
    "\n",
    "    Return:\n",
    "        The model summary object\n",
    "    \"\"\"\n",
    "    max_depth = 1 if max_depth is None else max_depth\n",
    "    model_summary = ModelSummary(\n",
    "        module, max_depth=max_depth, example_input_array=example_input_array\n",
    "    )\n",
    "\n",
    "    return model_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64d8c",
   "metadata": {
    "id": "1nb7isjuC1yI"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Récupération et exploration du datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe31ad9",
   "metadata": {
    "id": "XLml82VWC1yK"
   },
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "TOY_DATASET_URL = \"https://storage.googleapis.com/fchouteau-isae-deep-learning/toy_aircraft_dataset.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7db080",
   "metadata": {
    "id": "Shmmb50XC1yK"
   },
   "source": [
    "### Image (reminders)\n",
    "\n",
    "A digital image is an image composed of picture elements, also known as pixels, each with finite, discrete quantities of numeric representation for its intensity or gray level that is an output from its two-dimensional functions fed as input by its spatial coordinates denoted with x, y on the x-axis and y-axis, respectively.\n",
    "\n",
    "We represent images as matrixes,\n",
    "\n",
    "Images are made of pixels, and pixels are made of combinations of primary colors (in our case Red, Green and Blue). In this context, images have chanels that are the grayscale image of the same size as a color image, made of just one of these primary colors. For instance, an image from a standard digital camera will have a red, green and blue channel. A grayscale image has just one channel.\n",
    "\n",
    "In geographic information systems, channels are often referred to as raster bands.\n",
    "\n",
    "![img](https://static.packt-cdn.com/products/9781789613964/graphics/e91171a3-f7ea-411e-a3e1-6d3892b8e1e5.png)\n",
    "\n",
    "\n",
    "For the rest of this workshop we will use the following axis conventions for images\n",
    "\n",
    "![conventions](https://storage.googleapis.com/fchouteau-isae-deep-learning/static/image_coordinates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98ffc1",
   "metadata": {
    "id": "nPa5zHUBC1yN"
   },
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "We will be using [numpy datasources](https://docs.scipy.org/doc/numpy/reference/generated/numpy.DataSource.html?highlight=datasources) to download the dataset. DataSources can be local files or remote files/URLs. The files may also be compressed or uncompressed. DataSource hides some of the low-level details of downloading the file, allowing you to simply pass in a valid file path (or URL) and obtain a file object.\n",
    "\n",
    "The dataset is in npz format which is a packaging format where we store several numpy arrays in key-value format\n",
    "\n",
    "Note:\n",
    "If you get an error with the code below run:\n",
    "```python\n",
    "!gsutil -m cp -r gs://isae-deep-learning/toy_aircraft_dataset.npz /tmp/storage.googleapis.com/isae-deep-learning/toy_aircraft_dataset.npz\n",
    "```\n",
    "in a cell above the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb22fac",
   "metadata": {
    "id": "aPxBx-2-C1yP"
   },
   "outputs": [],
   "source": [
    "ds = np.DataSource(destpath=\"/tmp/\")\n",
    "f = ds.open(TOY_DATASET_URL, \"rb\")\n",
    "\n",
    "toy_dataset = np.load(f)\n",
    "trainval_images = toy_dataset[\"train_images\"]\n",
    "trainval_labels = toy_dataset[\"train_labels\"]\n",
    "test_images = toy_dataset[\"test_images\"]\n",
    "test_labels = toy_dataset[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006bd503",
   "metadata": {
    "id": "dRMdfPRKC1yR"
   },
   "source": [
    "### A bit of data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370be00",
   "metadata": {
    "id": "KLD83Y7vC1yR"
   },
   "source": [
    "**Q1. Labels counting**\n",
    "\n",
    "a. What is the dataset size ?\n",
    "\n",
    "b. How many images representing aircrafts ?\n",
    "\n",
    "c. How many images representing backgrounds ?\n",
    "\n",
    "d. What are the dimensions (height and width) of the images ? What are the number of channels ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d6b2e",
   "metadata": {
    "id": "5xkrtVx-C1yS"
   },
   "source": [
    "**Q2. Can you plot at least 8 examples of each label ? In a 4x4 grid ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f043fb",
   "metadata": {
    "id": "n_fynC7iC1yT"
   },
   "source": [
    "Here are some examples that help you answer this question. Try them and make your own. A well-understandood dataset is the key to an efficient model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56d8dc",
   "metadata": {
    "id": "7XcQrRWKC1yT"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b0288",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7wcKYZBC1yU",
    "outputId": "4068a524-b60a-48ec-f40d-9538e2ea425f"
   },
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\"Not an aircraft\", \"Aircraft\"]\n",
    "\n",
    "print(\"Labels counts :\")\n",
    "for l, c, label in zip(*np.unique(trainval_labels, return_counts=True), LABEL_NAMES):\n",
    "    print(f\" Label: {label} , value: {l}, count: {c}\")\n",
    "\n",
    "for l, label in enumerate(LABEL_NAMES):\n",
    "    print(\n",
    "        f\"Examples shape for label {l} : {trainval_images[trainval_labels == l, ::].shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba283d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArvB0PsXC1yW",
    "outputId": "84db6bb2-22b4-4384-d197-2ddcac18d9fd"
   },
   "outputs": [],
   "source": [
    "LABEL_NAMES = [\"Not an aircraft\", \"Aircraft\"]\n",
    "\n",
    "print(\"Labels counts :\")\n",
    "for l, c, label in zip(*np.unique(test_labels, return_counts=True), LABEL_NAMES):\n",
    "    print(f\" Label: {label} , value: {l}, count: {c}\")\n",
    "\n",
    "for l, label in enumerate(LABEL_NAMES):\n",
    "    print(f\"Examples shape for label {l} : {test_images[test_labels == l, ::].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaadce1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594
    },
    "id": "ol6QpoP6C1yX",
    "outputId": "5d445c41-2ed8-4f75-ed45-5ba1e25a2c8f"
   },
   "outputs": [],
   "source": [
    "grid_size = 4\n",
    "grid = np.zeros((grid_size * 64, grid_size * 64, 3)).astype(np.uint8)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        tile = np.copy(trainval_images[i * grid_size + j])\n",
    "        label = np.copy(trainval_labels[i * grid_size + j])\n",
    "        color = (0, 255, 0) if label == 1 else (255, 0, 0)\n",
    "        tile = cv2.rectangle(tile, (0, 0), (64, 64), color, thickness=2)\n",
    "        grid[i * 64 : (i + 1) * 64, j * 64 : (j + 1) * 64, :] = tile\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8c795",
   "metadata": {
    "id": "gFtNYE6EC1yY"
   },
   "source": [
    "### A bit about train-test\n",
    "\n",
    "You just downloaded a training and a test set.\n",
    "\n",
    "- We use the training set for forward/backward\n",
    "- We use the validation set to tune hyperparameters (optimizers, early stopping)\n",
    "- We use the test set for final metrics on our tuned model\n",
    "\n",
    "![](https://i.stack.imgur.com/osBuF.png)\n",
    "\n",
    "For more information as to why we use train/validation and test refer to these articles:\n",
    "\n",
    "- https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "- https://www.freecodecamp.org/news/what-to-do-when-your-training-and-testing-data-come-from-different-distributions-d89674c6ecd8/\n",
    "- https://kevinzakka.github.io/2016/09/26/applying-deep-learning/\n",
    "\n",
    "We will now create our validation dataset,\n",
    "\n",
    "Since we know the dataset is balanced, we can evenly sample from the dataset without taking too many risks\n",
    "\n",
    "We will do a 80/20 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372406c",
   "metadata": {
    "id": "gHmjoZhLC1yZ"
   },
   "outputs": [],
   "source": [
    "idxs = np.random.permutation(np.arange(trainval_images.shape[0]))\n",
    "\n",
    "train_idxs, val_idxs = idxs[: int(0.8 * len(idxs))], idxs[int(0.8 * len(idxs)) :]\n",
    "\n",
    "train_images = trainval_images[train_idxs]\n",
    "train_labels = trainval_labels[train_idxs]\n",
    "val_images = trainval_images[val_idxs]\n",
    "val_labels = trainval_labels[val_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a167d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cfe6Yu6C1yZ",
    "outputId": "9d35706f-c6b9-4d44-fde4-e7509580b865"
   },
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84391564",
   "metadata": {},
   "source": [
    "What is the mean of our data ? \n",
    "Whats is the standard deviation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dataset statistics in [0.,1.], we're going to use it to normalize our data\n",
    "\n",
    "mean = np.mean(train_images, axis=(0, 1, 2)) / 255.0\n",
    "std = np.std(train_images, axis=(0, 1, 2)) / 255.0\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7813724",
   "metadata": {
    "id": "SZ6VBCvQC1ya"
   },
   "source": [
    "## Preparing our training\n",
    "\n",
    "Remember that training a deep learning model requires:\n",
    "\n",
    "- Defining a model to train\n",
    "- Defining a loss function (cost function) to compute gradients with\n",
    "- Defining an optimizer to update parameters\n",
    "- Putting the model on the accelerator device that trains very fast (GPU, TPU)... You'll learn about GPUs later :)\n",
    "\n",
    "![](https://pbs.twimg.com/media/E_1d06cVIAcYheX?format=jpg)\n",
    "\n",
    "The training loop is \"quite basic\" : We loop over samples of the dataset (in batches) several times over :\n",
    "\n",
    "![](https://pbs.twimg.com/media/E_1d06XVcA8Dhzs?format=jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352362e",
   "metadata": {
    "id": "10ow7xWIC1ya",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a3b1ca",
   "metadata": {
    "id": "AKqUcnCcC1yb"
   },
   "source": [
    "### Defining Dataset & Transforms\n",
    "\n",
    "First, we need to tell pytorch how to load our data.\n",
    "\n",
    "Have a look at : https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "We write our own `torch.data.Dataset` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27bbd9",
   "metadata": {
    "id": "uvFjmzHoC1yb"
   },
   "outputs": [],
   "source": [
    "class NpArrayDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        image_transforms: Callable = None,\n",
    "        label_transforms: Callable = None,\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_transforms = label_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.images[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        if self.image_transforms is not None:\n",
    "            x = self.image_transforms(x)\n",
    "        else:\n",
    "            x = torch.tensor(x)\n",
    "\n",
    "        if self.label_transforms is not None:\n",
    "            y = self.label_transforms(y)\n",
    "        else:\n",
    "            y = torch.tensor(y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb376e6",
   "metadata": {
    "id": "0Z4N5o8AC1yb"
   },
   "source": [
    "Then we need to process our data (images) into \"tensors\" that torch can process, we define \"transforms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641ff8a",
   "metadata": {
    "id": "PahdjhR5C1yc"
   },
   "outputs": [],
   "source": [
    "# transform to convert np array in range [0,255] to torch.Tensor [0.,1.]\n",
    "# then normalize by doing x = (x - mean) / std\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# here we don't have anything to do\n",
    "target_transforms = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886ddc1c",
   "metadata": {
    "id": "p9QH51F-C1yc"
   },
   "source": [
    "Now we put everything together into something to load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3042fff6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR14oNXyC1yd",
    "outputId": "55d52439-7ffc-46a6-d7fb-13c0b173e761"
   },
   "outputs": [],
   "source": [
    "# load the training data\n",
    "train_set = NpArrayDataset(\n",
    "    images=train_images,\n",
    "    labels=train_labels,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=target_transforms,\n",
    ")\n",
    "\n",
    "print(len(train_set))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# load the validation data\n",
    "validation_set = NpArrayDataset(\n",
    "    images=val_images,\n",
    "    labels=val_labels,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=target_transforms,\n",
    ")\n",
    "\n",
    "print(len(validation_set))\n",
    "\n",
    "val_loader = DataLoader(validation_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4539a1",
   "metadata": {
    "id": "0yxUYemIC1yd"
   },
   "source": [
    "### Check that your dataset outputs correct data\n",
    "\n",
    "Always to this as a sanity check to catch bugs in your data processing pipeline\n",
    "\n",
    "Write the inverse transformation by hand to ensure it's ok\n",
    "\n",
    "![andrej](https://storage.googleapis.com/fchouteau-isae-deep-learning/static/andrej_tweet_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a009c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "Ic2sE836C1ye",
    "outputId": "db784726-e3e0-40c3-d9ff-9e0ec597d28e"
   },
   "outputs": [],
   "source": [
    "k = np.random.randint(len(train_set))\n",
    "x, y = train_set[k]\n",
    "\n",
    "# From torch\n",
    "# Inverse transform\n",
    "x = x.numpy()\n",
    "x = x.transpose((1, 2, 0))\n",
    "x = x * std + mean\n",
    "x = x.clip(0.0, 1.0)\n",
    "x = (x * 255.0).astype(np.uint8)\n",
    "\n",
    "print(\"Inverse transform is OK ?\")\n",
    "print(\"Label {}\".format(y))\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(train_set.images[k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dd963",
   "metadata": {
    "id": "4np1A43JC1yf"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5728ca",
   "metadata": {},
   "source": [
    "### On which device will we train ?\n",
    "\n",
    "We will check if we have a GPU and set the \"device\" of pytorch on it so that it trains on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37236066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmV9carLC1yf",
    "outputId": "dba28689-a129-4ae1-facc-41f9d3e55f8e"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919248eb",
   "metadata": {
    "id": "AJ3oVqOHC1yg"
   },
   "source": [
    "### Defining a model and computing the parameters\n",
    "\n",
    "Now we have to define a CNN to train. It's usually called a \"network\", and we define its \"architecture\".\n",
    "\n",
    "Defining a good architecture is a huge field of research (a pandora's box) that takes a lot of time, but we can define \"sane architectures\" easily:\n",
    "\n",
    "Basically, CNN architectures are a stacks of :\n",
    "- Convolution layers + non linearities\n",
    "- Pooling layer\n",
    "- A final \"activation\" layer at the end (for classification) that allows us to output probabilities\n",
    "\n",
    "![](https://cs231n.github.io/assets/cnn/convnet.jpeg)\n",
    "\n",
    "Let's define a model together:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    # A block of 2 convolutions + non linearities & a pooling layers\n",
    "    # IN SHAPE (3,64,64)\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "    # OUT SHAPE (16,62,62)\n",
    "    nn.ReLU(),\n",
    "    # IN SHAPE (16,62,62)\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "    # OUT SHAPE (16,60,60)\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # OUT SHAPE (16,30,30)\n",
    "    # Another stack of these\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "    # OUT SHAPE (?,?,?)\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # OUT SHAPE (?,?,?)\n",
    "    # Another stack of these\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    # OUT SHAPE (?,?,?)\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "    # OUT SHAPE (?,?,?)\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # OUT SHAPE (?,?,?)\n",
    "    # A final classifier\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=4 * 4 * 64, out_features=256), # do you understand why 4 * 4 * 64 ?\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=256, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=64, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "```\n",
    "\n",
    "**Questions**\n",
    "\n",
    "Knowing that the input image size is (3,64,64), go through the model step by step,\n",
    "\n",
    "Can you fill the blanks for the shapes ?\n",
    "\n",
    "Do you understand why ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a5586",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hd06b1EnC1yh",
    "outputId": "039eb3c0-b120-4288-e404-f1e70bb76a48"
   },
   "outputs": [],
   "source": [
    "# Let's test this !\n",
    "\n",
    "some_model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # Another stack of these\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # Another stack of these\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # A final classifier\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=4 * 4 * 64, out_features=256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=256, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=64, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# We define an input of dimensions batch_size, channels, height, width\n",
    "x = torch.rand((16, 3, 64, 64))\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "y = some_model(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "# Let's visualize each shape using our summarize helper\n",
    "print(summarize(some_model, example_input_array=x))\n",
    "\n",
    "# let's delete the model now, we won't need it\n",
    "del some_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff6998",
   "metadata": {
    "id": "YPpPpXwZC1yh"
   },
   "source": [
    "**Let's do it yourself !**\n",
    "\n",
    "About weight init :\n",
    "- https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "- https://www.pyimagesearch.com/2021/05/06/understanding-weight-initialization-for-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432431fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "d5nx-e0VC1yh",
    "outputId": "b4f8293d-996e-4e95-bcbc-0442c991ebbe"
   },
   "outputs": [],
   "source": [
    "# Let's define another model, except this time there are blanks ... it's up to you to fill them\n",
    "\n",
    "\n",
    "def _init_weights(model):\n",
    "    for m in model.modules():\n",
    "        # Initialize all convs\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    model = nn.Sequential(\n",
    "        # A first convolution block\n",
    "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=..., out_channels=16, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # Another stack of these\n",
    "        nn.Conv2d(in_channels=..., out_channels=32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=..., out_channels=..., kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # A final classifier\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=... * ... * ..., out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(in_features=64, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c030c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fn()\n",
    "\n",
    "print(model)\n",
    "\n",
    "x = torch.rand(\n",
    "    (16, 3, 64, 64)\n",
    ")  # We define an input of dimensions batch_size, channels, height, width\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(summarize(model, example_input_array=x))\n",
    "\n",
    "# THIS CELL SHOULD NOT GIVE AN ERROR !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af675a55",
   "metadata": {
    "id": "nxb-JeDnC1yk"
   },
   "source": [
    "Hint: The answer (and there can only be one) is :\n",
    "\n",
    "<details>\n",
    "    <summary>Solution</summary> \n",
    "    \n",
    "```python\n",
    "def _init_weights(model):\n",
    "    # about weight initialization\n",
    "    # https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/\n",
    "    # https://www.pyimagesearch.com/2021/05/06/understanding-weight-initialization-for-neural-networks/\n",
    "    for m in model.modules():\n",
    "        # Initialize all convs\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    model = nn.Sequential(\n",
    "        # A first convolution block\n",
    "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # Another stack of these\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # A final classifier\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=12 * 12 * 32, out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(in_features=64, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    _init_weights(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = model_fn()\n",
    "\n",
    "print(model)\n",
    "\n",
    "x = torch.rand((16, 3, 64, 64))  # We define an input of dimensions batch_size, channels, height, width\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(summarize(model,example_input_array=x))\n",
    "```\n",
    "\n",
    "And outputs this\n",
    "\n",
    "```\n",
    "Sequential(\n",
    "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (1): ReLU()\n",
    "  (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (3): ReLU()\n",
    "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (5): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (6): ReLU()\n",
    "  (7): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (8): ReLU()\n",
    "  (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
    "  (10): ReLU()\n",
    "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (12): Flatten(start_dim=1, end_dim=-1)\n",
    "  (13): Linear(in_features=4608, out_features=64, bias=True)\n",
    "  (14): ReLU()\n",
    "  (15): Dropout(p=0.1, inplace=False)\n",
    "  (16): Linear(in_features=64, out_features=1, bias=True)\n",
    "  (17): Sigmoid()\n",
    ")\n",
    "torch.Size([16, 3, 64, 64])\n",
    "torch.Size([16, 1])\n",
    "   | Name | Type      | Params | In sizes         | Out sizes       \n",
    "--------------------------------------------------------------------------\n",
    "0  | 0    | Conv2d    | 448    | [16, 3, 64, 64]  | [16, 16, 62, 62]\n",
    "1  | 1    | ReLU      | 0      | [16, 16, 62, 62] | [16, 16, 62, 62]\n",
    "2  | 2    | Conv2d    | 2.3 K  | [16, 16, 62, 62] | [16, 16, 60, 60]\n",
    "3  | 3    | ReLU      | 0      | [16, 16, 60, 60] | [16, 16, 60, 60]\n",
    "4  | 4    | MaxPool2d | 0      | [16, 16, 60, 60] | [16, 16, 30, 30]\n",
    "5  | 5    | Conv2d    | 4.6 K  | [16, 16, 30, 30] | [16, 32, 28, 28]\n",
    "6  | 6    | ReLU      | 0      | [16, 32, 28, 28] | [16, 32, 28, 28]\n",
    "7  | 7    | Conv2d    | 9.2 K  | [16, 32, 28, 28] | [16, 32, 26, 26]\n",
    "8  | 8    | ReLU      | 0      | [16, 32, 26, 26] | [16, 32, 26, 26]\n",
    "9  | 9    | Conv2d    | 9.2 K  | [16, 32, 26, 26] | [16, 32, 24, 24]\n",
    "10 | 10   | ReLU      | 0      | [16, 32, 24, 24] | [16, 32, 24, 24]\n",
    "11 | 11   | MaxPool2d | 0      | [16, 32, 24, 24] | [16, 32, 12, 12]\n",
    "12 | 12   | Flatten   | 0      | [16, 32, 12, 12] | [16, 4608]      \n",
    "13 | 13   | Linear    | 294 K  | [16, 4608]       | [16, 64]        \n",
    "14 | 14   | ReLU      | 0      | [16, 64]         | [16, 64]        \n",
    "15 | 15   | Dropout   | 0      | [16, 64]         | [16, 64]        \n",
    "16 | 16   | Linear    | 65     | [16, 64]         | [16, 1]         \n",
    "17 | 17   | Sigmoid   | 0      | [16, 1]          | [16, 1]         \n",
    "--------------------------------------------------------------------------\n",
    "320 K     Trainable params\n",
    "0         Non-trainable params\n",
    "320 K     Total params\n",
    "1.284     Total estimated model params size (MB)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "You should be able to understand this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c573d46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAv9FrjAC1yl",
    "outputId": "6b42440f-8806-41e3-dc97-ecb499de36bd"
   },
   "outputs": [],
   "source": [
    "# moving model to gpu if available\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a2c05",
   "metadata": {
    "id": "LhpN-UNfC1yl"
   },
   "source": [
    "### Defining our loss and optimizer\n",
    "\n",
    "Check the definition of the binary cross entropy:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8240f1",
   "metadata": {
    "id": "6w1BHLnoC1ym"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction=\"mean\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da064779",
   "metadata": {
    "id": "8d1qaMZ8C1ym"
   },
   "source": [
    "## Training with pytorch\n",
    "\n",
    "We will actually train the model, and plot training & validation metrics during training\n",
    "\n",
    "Be careful, if you train several times the same model it will continue optimizing its parameters\n",
    "\n",
    "Its advised to define a new model if you are executing the training loop several times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9d085",
   "metadata": {},
   "source": [
    "### Defining the Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846430b",
   "metadata": {
    "id": "xfP8tBSMC1yn"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader):\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "\n",
    "        # get one batch\n",
    "        x, y_true = batch\n",
    "        x = x.to(DEVICE)\n",
    "        y_true = y_true.to(DEVICE)\n",
    "\n",
    "        # format the y_true so that it is compatible with the loss\n",
    "        y_true = y_true.view((-1, 1)).float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(y_pred, y_true)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # save statistics\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Batch {i}, curr loss = {loss.item():.03f}\")\n",
    "\n",
    "    return np.asarray(epoch_loss).mean()\n",
    "\n",
    "\n",
    "def valid_one_epoch(model, valid_loader):\n",
    "\n",
    "    epoch_loss = []\n",
    "\n",
    "    for i, batch in enumerate(valid_loader):\n",
    "        with torch.no_grad():\n",
    "            # get one batch\n",
    "            x, y_true = batch\n",
    "            x = x.to(DEVICE)\n",
    "            y_true = y_true.to(DEVICE)\n",
    "\n",
    "            # format the y_true so that it is compatible with the loss\n",
    "            y_true = y_true.view((-1, 1)).float()\n",
    "\n",
    "            # forward\n",
    "            y_pred = model(x)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(y_pred, y_true)\n",
    "\n",
    "            # save statistics\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "    return np.asarray(epoch_loss).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f64b97",
   "metadata": {},
   "source": [
    "### Putting everything together to run a training\n",
    "\n",
    "Here we copy paste previous code so that you are sure you have setup everything correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fn()\n",
    "\n",
    "# moving model to gpu if available\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# We define an input of dimensions batch_size, channels, height, width\n",
    "x = torch.rand((16, 3, 64, 64))\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(summarize(model, example_input_array=x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6f3e6",
   "metadata": {},
   "source": [
    "**Hyperparameters**\n",
    "\n",
    "Here we define what we call hyperparameters, the \"meta-parameters\" of the training that you can modify to affect your training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10  # Set number of epochs, example 100\n",
    "LEARNING_RATE = 1e-2\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36699957",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction=\"mean\")\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8dc15",
   "metadata": {},
   "source": [
    "Let's go !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1eaeb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fwFxOOFGQkZ",
    "outputId": "a6da7c86-8233-4257-cf0f-86d6ff13ea88"
   },
   "outputs": [],
   "source": [
    "# Send model to GPU\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_epoch_loss = train_one_epoch(model, train_loader)\n",
    "    model.eval()\n",
    "    valid_epoch_loss = valid_one_epoch(model, val_loader)\n",
    "\n",
    "    print(f\"EPOCH={epoch}, TRAIN={train_epoch_loss}, VAL={valid_epoch_loss}\")\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    valid_losses.append(valid_epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23485fcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "sFTlE66MC1yo",
    "outputId": "3f24cd80-7722-4228-e74e-7b4e2759fcb1"
   },
   "outputs": [],
   "source": [
    "# Plot training / validation loss\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48730ff7",
   "metadata": {},
   "source": [
    "### Training analysis\n",
    "\n",
    "How would you analyze your training ?\n",
    "\n",
    "Is it underfitting ?\n",
    "\n",
    "Is it overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593dafdd",
   "metadata": {},
   "source": [
    "### Model saving\n",
    "\n",
    "There are several ways to save your model :\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "- torch.save: Saves a serialized object to disk. This function uses Python’s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "\n",
    "- torch.load: Uses pickle’s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into (see Saving & Loading Model Across Devices).\n",
    "\n",
    "- torch.nn.Module.load_state_dict: Loads a model’s parameter dictionary using a deserialized state_dict. For more information on state_dict, see What is a state_dict?.\n",
    "\n",
    "- scripting / tracing the model: https://pytorch.org/docs/stable/jit.html\n",
    "\n",
    "The first 2 options require you to import the model definition as it uses pickle\n",
    "The third option requires you to redefine an empty model with the same architecture and load the weights, because we are only saving the \"state\" (e.g. parameters, weights, biases)\n",
    "The fourth option allow to make a \"self-contained\" model that can be used later, but comes with caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c494677",
   "metadata": {},
   "source": [
    "### State dict saving \n",
    "\n",
    "This is the recommended method because it allows to reuse the model with any code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State dict saving\n",
    "with open(\"model.pt\", \"wb\") as f:\n",
    "    torch.save(model.state_dict(), f)\n",
    "\n",
    "# See below for how to reload the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5256f",
   "metadata": {},
   "source": [
    "To reload such a model, you have to instantiate an empty model with the same architecture then load the state dict (the weights)\n",
    "```python\n",
    "# Instantiate a new empty model\n",
    "model = model_fn()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Load state\n",
    "checkpoint_path = \"model.pt\"\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "print(\"Model Loaded\")\n",
    "```\n",
    "\n",
    "This is very nice because you get a model that you can finetune, retrain, modify. However, this means that you have to \"port\" the model definition code to production. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f932d",
   "metadata": {},
   "source": [
    "### Model scripting\n",
    "\n",
    "But for production, in order to avoid shipping the model definition code, we like to have an \"self-contained\" binary that we can deliver to the production team (you will see such a case during our next class together for cloud computing)\n",
    "\n",
    "Here we try to \"script\" the model, meaning that we compile the graph to a static version of itself\n",
    "\n",
    "https://pytorch.org/docs/stable/jit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae497a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.jit\n",
    "\n",
    "# Put the model in eval mode\n",
    "model = model.cpu().eval()\n",
    "\n",
    "# Script the model\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Save\n",
    "scripted_model.save(\"scripted_model.pt\")\n",
    "\n",
    "print(scripted_model)\n",
    "\n",
    "# Scripted model reloading (demo)\n",
    "scripted_model = torch.jit.load(\"scripted_model.pt\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b662dd9",
   "metadata": {},
   "source": [
    "### Download the scripted model\n",
    "\n",
    "We are going to download the scripted model to be able to re-use it elsewhere (in another notebook for example), without having to rewrite the model definition function\n",
    "\n",
    "Uncomment this on google colab to download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8576723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# files.download('scripted_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767a313",
   "metadata": {
    "id": "VqMkcDroC1yp"
   },
   "source": [
    "We have finished what we need to do with the model, let's delete it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebc3e9",
   "metadata": {
    "id": "btrb85LmC1yp"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a4e2f",
   "metadata": {
    "id": "QW5XvyyZC1yq"
   },
   "source": [
    "## Testing our models and computing metrics\n",
    "\n",
    "Now that we have a trained network, it is important to measure how well it performs. We do not do that during training because theoretically we try to test on a context closer to how the final model will be used, meaning this can be another pipeline and is usually outside the training engine.\n",
    "\n",
    "You can refer to your ML course or on resources on the web to see how we can measure it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259e268d",
   "metadata": {
    "id": "g0TldNDQC1yq"
   },
   "source": [
    "### Loading saved model\n",
    "\n",
    "State dict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f72ec1",
   "metadata": {
    "id": "Q-kCmgWEC1yr"
   },
   "outputs": [],
   "source": [
    "# Instantiate a new empty model\n",
    "model = model_fn()\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Load state\n",
    "checkpoint_path = \"model.pt\"\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e83ab5",
   "metadata": {
    "id": "a-rbNh7qC1yr"
   },
   "source": [
    "### Inferencing on the test dataset\n",
    "\n",
    "Now we will run predictions on the test dataset using the newly loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33fbcf",
   "metadata": {
    "id": "LjlrKEEOC1yr"
   },
   "outputs": [],
   "source": [
    "test_ds = NpArrayDataset(\n",
    "    images=test_images,\n",
    "    labels=test_labels,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=target_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a79f4",
   "metadata": {
    "id": "Jf3oIRA4C1yr"
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a91b3",
   "metadata": {
    "id": "VWM757ggC1ys"
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Send model to correct device\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Put model in evaluatio mode (very important)\n",
    "model.eval()\n",
    "\n",
    "# Disable all gradients things\n",
    "with torch.no_grad():\n",
    "    for x, y_t in tqdm.tqdm(test_ds, \"predicting\"):\n",
    "        x = x.reshape((-1,) + x.shape)\n",
    "        x = x.to(DEVICE)\n",
    "        y = model.forward(x)\n",
    "        y = y.to(\"cpu\").numpy()\n",
    "\n",
    "        y_t = int(y_t.to(\"cpu\").numpy())\n",
    "\n",
    "        y_pred.append(y)\n",
    "        y_true.append(y_t)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "y_true = np.asarray(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1bac09",
   "metadata": {
    "id": "awHUQ2KxC1ys"
   },
   "outputs": [],
   "source": [
    "print(y_pred.shape)\n",
    "\n",
    "print(y_pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae382b",
   "metadata": {
    "id": "aGqeE1UJC1ys"
   },
   "outputs": [],
   "source": [
    "y_pred_classes = y_pred[:, 0] > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cb185",
   "metadata": {
    "id": "wMuCtss8C1ys"
   },
   "source": [
    "### Confusion matrix\n",
    "Here, we are first computing the [confusion matrix]():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a4778",
   "metadata": {
    "id": "QSq5-t7dC1yt"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "print(\"Confusion matrix\")\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm, display_labels=[\"background\", \"aircraft\"]\n",
    ")\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7addd0",
   "metadata": {
    "id": "Ao41bfOuC1yt",
    "tags": []
   },
   "source": [
    "### ROC curve\n",
    "\n",
    "The next metric we are computing is the [Receiver Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html). A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The method was originally developed for operators of military radar receivers starting in 1941, which led to its name. \n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/512px-Roc_curve.svg.png)\n",
    "\n",
    "![](http://algolytics.com/wp-content/uploads/2018/05/roc1_en.png)\n",
    "\n",
    "It is used to choose a threshold on the output probability in case you are interesting in controling the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33235081",
   "metadata": {
    "id": "sEj4ZBgTC1yt"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and Area Under Curver\n",
    "\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# We round predictions for better readability\n",
    "y_pred_probas = np.round(y_pred[:, 0], 2)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_probas)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c602ca4",
   "metadata": {
    "id": "KGD6ukiMC1yu"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(\n",
    "    fpr, tpr, color=\"darkorange\", lw=lw, label=\"ROC curve (area = %0.2f)\" % roc_auc\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e587d76",
   "metadata": {},
   "source": [
    "### Using the ROC curve to select an optimal threshold\n",
    "\n",
    "The ROC curve can be used to select the best decision threshold for classifying an aircraft as positive.\n",
    "\n",
    "Plot the ROC curve with thresholds assigned to points in the curve (you can round the predictions for a simpler curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209f4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We round predictions every 0.05 for readability\n",
    "y_pred_probas = (y_pred[:, 0] / 0.05).astype(np.int) * 0.05\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_probas)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.step(fpr, tpr, \"bo\", alpha=0.2, where=\"post\")\n",
    "plt.fill_between(fpr, tpr, alpha=0.2, color=\"b\", step=\"post\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\"2-class ROC curve: AUC={:0.2f}\".format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\")\n",
    "\n",
    "for tp, fp, t in zip(tpr, fpr, thresholds):\n",
    "    plt.annotate(\n",
    "        np.round(t, 2),\n",
    "        xy=(fp, tp),\n",
    "        xytext=(fp - 0.05, tp - 0.05),\n",
    "        arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n",
    "    )\n",
    "plt.savefig(\"roc_curve_thresholds.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e95e4d",
   "metadata": {},
   "source": [
    "Now, choose a threshold on the curve where you miss less than 10% of the aircrafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a727f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_threshold = ...\n",
    "\n",
    "print(\"Confusion matrix\")\n",
    "\n",
    "y_pred_classes = y_pred_probas > selected_threshold\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm, display_labels=[\"background\", \"aircraft\"]\n",
    ")\n",
    "\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# How did the confusion matrix evolve ? Does it match your intuition ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc38e6",
   "metadata": {
    "id": "ql5f8eLHC1yu"
   },
   "source": [
    "### Misclassified examples\n",
    "\n",
    "It is always interesting to check mis classified examples.\n",
    "\n",
    "It usually provides tips on how to improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ba3926",
   "metadata": {
    "id": "Z0wvNDzmC1yv"
   },
   "outputs": [],
   "source": [
    "misclassified_idxs = np.where(y_pred_classes != y_true)[0]\n",
    "\n",
    "print(len(misclassified_idxs))\n",
    "\n",
    "print(misclassified_idxs)\n",
    "\n",
    "misclassified_images = test_images[misclassified_idxs]\n",
    "misclassified_true_labels = test_labels[misclassified_idxs]\n",
    "misclassified_pred_labels = y_pred_classes[misclassified_idxs]\n",
    "\n",
    "grid_size = 4\n",
    "grid = np.zeros((grid_size * 64, grid_size * 64, 3)).astype(np.uint8)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        img = np.copy(misclassified_images[i * grid_size + j])\n",
    "        pred = np.copy(misclassified_pred_labels[i * grid_size + j])\n",
    "        color = (0, 255, 0) if pred == 1 else (255, 0, 0)\n",
    "        tile = cv2.rectangle(img, (0, 0), (64, 64), color, thickness=2)\n",
    "        grid[i * 64 : (i + 1) * 64, j * 64 : (j + 1) * 64, :] = img\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65443f1b",
   "metadata": {
    "id": "-DDeFy4cC1yv"
   },
   "source": [
    "## Improving our training / validation loop\n",
    "\n",
    "We will add more advanced features to our training loop for better models\n",
    "\n",
    "Copy the train / valid loop and update it accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff67046",
   "metadata": {},
   "source": [
    "### Computing accuracy during training / validation\n",
    "\n",
    "Update the `valid_one_epoch` to compute accuracy during during the validation loop, and plot its evolution during training\n",
    "\n",
    "Use the ROC curve computation where we compute the pred / true classes as inspiration\n",
    "\n",
    "Here's an example (that needs to be modified)\n",
    "```python\n",
    "\n",
    "correct_pred = 0\n",
    "total_pred = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        predictions = torch.round(outputs)[:,0]\n",
    "        # collect the correct predictions\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred += 1\n",
    "            total_pred += 1\n",
    "            \n",
    "    # print accuracy\n",
    "    accuracy = 100 * (total_pred / total_pred)\n",
    "    print(\"Accuracy is: {:.1f} %\".format(accuracy))\n",
    "\n",
    "```                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2743c0",
   "metadata": {
    "id": "84qzXDMGC1yw"
   },
   "source": [
    "### Early stopping\n",
    "\n",
    "You've seen that it is possible to overfit it you're not careful,\n",
    "\n",
    "**Go back to your previous class and adapt the training loop to add early stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8badc22b",
   "metadata": {
    "id": "NW7rLbdGC1yx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffcab065",
   "metadata": {
    "id": "4XRzekUGC1yy"
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "\n",
    "One technique for training CNNs on images is to put your training data through data augmentation to generate similar-but-different examples to make your network more robust.\n",
    "\n",
    "You can generate \"augmented images\" on the fly or use composition to generate data\n",
    "\n",
    "- We are going to wrap our numpy arrays with `torch.utils.data.Dataset` class\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "\n",
    "- Here is how we use torch Compose to augment data\n",
    "\n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#compose-transforms\n",
    "\n",
    "Note: This step requires a bit of tinkering from numpy arrays to torch datasets, it's fine if you skip it. For the next notebook it may prove a useful way of gaining performance\n",
    "\n",
    "**Remember : We apply data augmentation only during training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb6399",
   "metadata": {
    "id": "ua4UQAZWC1yy"
   },
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "import torch.utils\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741946f",
   "metadata": {
    "id": "em78uFlnC1yy"
   },
   "outputs": [],
   "source": [
    "# Example (very simple) data augmentation to get your started, you can add more transforms to this list\n",
    "\n",
    "train_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52f92c",
   "metadata": {
    "id": "5yPlhQbzC1yy"
   },
   "outputs": [],
   "source": [
    "trainset_augmented = NpArrayDataset(\n",
    "    images=train_images,\n",
    "    labels=train_labels,\n",
    "    image_transforms=train_transform,\n",
    "    label_transforms=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c65b1b",
   "metadata": {
    "id": "MeKVDcHrC1yz"
   },
   "outputs": [],
   "source": [
    "# Get image from dataset. Note: it has been converted as a torch tensor in CHW format in float32 normalized !\n",
    "img, label = trainset_augmented[0]\n",
    "img = img.numpy().transpose((1, 2, 0)) * std + mean\n",
    "img = img.clip(0.0, 1.0)\n",
    "img = (img * 255.0).astype(np.uint8)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# Compare effects of data augmentation\n",
    "img_orig = trainset_augmented.images[0]\n",
    "plt.imshow(img_orig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fe2e0",
   "metadata": {
    "id": "ej7Jb0SNC1yz"
   },
   "outputs": [],
   "source": [
    "# do another training and plot our metrics again. Did we change something ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7685b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Best checkpoint\n",
    "\n",
    "You've seen how to save model checkpoint. However we saved the model at the end of training. What if there is an issue (like overfitting ? or our computer crashes !!!) ? \n",
    "\n",
    "How to keep a good copy of our model at any point ? \n",
    "\n",
    "The idea is that during the training, we always save the checkpoint with the lowest valid loss, then reload it at the end of training\n",
    "\n",
    "**Modify the train loop to keep the best model state dict at any point, then reload it at the end of training**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fd42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b737192",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Food for thoughts: Tooling\n",
    "\n",
    "To conclude this notebook, reflect on the following,\n",
    "\n",
    "You have launched different experiences and obtained different results,\n",
    "\n",
    "Did you feel the notebook you used was sufficient ? Which tools would you like to have in order to properly run your experiments ? (Quick google search or ask someone) Do they already exist ?\n",
    "\n",
    "### **Presentation : High level frameworks**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logo_mixed.svg\" alt=\"ignite\" style=\"width: 400px;\"/>\n",
    "\n",
    "Pytorch ignite is what we call a \"high-level library\" over pytorch, its objectives is to abstract away most of the boilerplate code for training deep neural network.\n",
    "\n",
    "Usually, they make the development process easier by enabling you to focus on what's important instead of writing distributed and optimized training loops and plugging metrics / callbacks. Because we all forgot to call `.backward()` or `.zero_grad()` at least once.\n",
    "\n",
    "Here an overview of the high-level libraries available for pytorch,\n",
    "\n",
    "https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&utm_medium=tweet&utm_campaign=blog-model-training-libraries-pytorch-ecosystem\n",
    "\n",
    "Of these, we would like to highlight three of them:\n",
    "\n",
    "- pytorch-ignite, officially sanctioned by the pytorch team (its repo lives at https://pytorch.org/ignite/), which is developped by [someone from Toulouse](https://twitter.com/vfdev_5) - yes there is a member of the pytorch team living in Toulouse, we are not THAT behind in ML/DL :wishful-thinking:\n",
    "\n",
    "- pytorch-lightning (https://www.pytorchlightning.ai/) which has recently seen its 1.0 milestone and has been developped to a company. It is more \"research oriented\" that pytorch-ignite, and with a lower abstraction level, but seems to enable more use case.\n",
    "\n",
    "- catalyst (https://github.com/catalyst-team/catalyst) \n",
    "\n",
    "- skorch (https://github.com/skorch-dev/skorch). This class was previously written in skorch. Skorch mimics the scikit-learn API and allows bridging the two libraries together. It's a bit less powerful but you write much less code than the two libraries above, and if you are very familiar with scikit-learn, it could be very useful for fast prototyping\n",
    "\n",
    "\n",
    "**Take a look at the [previous class](https://nbviewer.jupyter.org/github/SupaeroDataScience/deep-learning/blob/main/deep/PyTorch%20Ignite.ipynb), the [official examples](https://nbviewer.jupyter.org/github/pytorch/ignite/tree/master/examples/notebooks/) or the [documentation](https://pytorch.org/ignite/) if want to learn about Ignite**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2179d",
   "metadata": {},
   "source": [
    "## **Optional** exercises to run at home\n",
    "\n",
    "If you're done with this, you can explore a little bit more : Now that we have a nice training loop we can do hyperparameter tuning !\n",
    "\n",
    "As you can see, there are a lot of parameters we can choose:\n",
    "\n",
    "- the optimizer\n",
    "- the learning rate\n",
    "- the model architecture\n",
    " \n",
    "etc... !\n",
    "\n",
    "\n",
    "- Try to play with network hyperparameters. The dataset is small and allow fast iterations so use it to have an idea on hyperparameter sensitivity.\n",
    "    number of convolutions, other network structures, learning rates, optimizers,...\n",
    "\n",
    "- Example: Compare again SGD and ADAM\n",
    "\n",
    "- Try to use the ROC curve to select a threshold to filter only negative examples without losing any positive examples\n",
    "\n",
    "When you are done with the warmup, go to the next notebook. But remember that next datasets will be larger and you will not have the time (trainings will take longer ) to experiment on hyperparameters.\n",
    "\n",
    "**You can try more things**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd580c",
   "metadata": {},
   "source": [
    "### Optimizer Changes\n",
    "Change the optimizer from SGD to optim.Adam. Is it better ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954213c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57438d9",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "One of the most used \"layer\" beyond conv / pool / relu is \"batch normalization\",\n",
    "\n",
    "http://d2l.ai/chapter_convolutional-modern/batch-norm.html\n",
    "\n",
    "Try adding it to your network and see what happens !\n",
    "\n",
    "<details>\n",
    "\n",
    "```python\n",
    "def model_fn():\n",
    "    model = nn.Sequential(\n",
    "        # A first convolution block\n",
    "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # Another stack of these\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # A final classifier\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=12 * 12 * 32, out_features=64),\n",
    "        nn.BatchNorm1d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.1),\n",
    "        nn.Linear(in_features=64, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970bb3a",
   "metadata": {
    "id": "xUlVYFuPC1yz"
   },
   "source": [
    "### Trying other models\n",
    "\n",
    "You have seen a class on different model structure,\n",
    "https://supaerodatascience.github.io/deep-learning/slides/2_architectures.html#/\n",
    "\n",
    "Now is the time to try and implement them. \n",
    "\n",
    "For example, try to write a VGG-11 with fewer filters by yourself... or a very small resnet using [this](https://github.com/a-martyn/resnet/blob/master/resnet.py) as inspiration\n",
    "\n",
    "You can also use models from [torchvision](https://pytorch.org/docs/stable/torchvision/models.html#classification) in your loop, or as inspiration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61041a",
   "metadata": {
    "id": "L5vTgr9OC1yz"
   },
   "source": [
    "**Modify the model structure and launch another training... Is it better ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159dbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c35e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LR Scheduling\n",
    "\n",
    "Sometimes it's best to reduce the learning rate if you stop improving, or to reduce learning rate at the end of training\n",
    "\n",
    "Tutorial : https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/#top-basic-learning-rate-schedules\n",
    "\n",
    "- **Modify the train loop to change the learning rate when the validation loss is stagnating**\n",
    "\n",
    "- **Modify the train loop to change the learning rate when the validation loss is stagnating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bcb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b45c0b",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "For usual tasks such as classification or detection, we use \"transfer learning\":\n",
    "\n",
    "    In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n",
    "    \n",
    "Adapt this tutorial to do transfer learning from a network available in torchvision to our use case\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "I advise you to select resnet18\n",
    "\n",
    "The biggest library of pretrained models is available here :\n",
    "\n",
    "https://github.com/rwightman/pytorch-image-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f6380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
