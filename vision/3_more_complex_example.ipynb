{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfccd28a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# Session 2 Part 1: Going Further, Discovering class-imbalance in datasets\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Florient Chouteau | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>\n",
    "\n",
    "Since we have done the most basic training example, got our hands on skorch and on the dataset, we are going to repeat our process using a more realistic use case. This time, our dataset will be severely unbalanced (10% of all data will be images of aircrafts), like in real life (or not even like in real life but it's getting closer).\n",
    "\n",
    "Here, we won't guide you, you will have to use what you learned in the previous notebooks as well as what you learned in previous data science class to try to devise a way to train a good model\n",
    "\n",
    "You are going to:\n",
    "- Do a first \"naive\" run with the full data\n",
    "- Diagnose performance\n",
    "- Try to improve it by tuning several factors:\n",
    "  - The dataset itself\n",
    "  - The optimization parameters\n",
    "  - The network architecture\n",
    "\n",
    "**Remember that \"deep learning\" is still considered somewhat a black art so it's hard to know in advance what will work.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your imports here\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "TRAINVAL_DATASET_URL = \"https://storage.googleapis.com/fchouteau-isae-deep-learning/large_aircraft_dataset_2023.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f81e1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Q0. Downloading & splitting the dataset\n",
    "\n",
    "You will get the following:\n",
    "\n",
    "- 50k images in training which you should use as training & validation\n",
    "- 5k images in test, which you should only use to compute your final metrics on. **Don't ever use this dataset for early stopping / intermediary metrics**\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/pXAfX.png\" alt=\"pokemon\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070d893",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "ds = np.DataSource(\"/tmp/\")\n",
    "f = ds.open(TRAINVAL_DATASET_URL, \"rb\")\n",
    "trainval_dataset = np.load(f)\n",
    "trainval_images = trainval_dataset[\"train_images\"]\n",
    "trainval_labels = trainval_dataset[\"train_labels\"]\n",
    "test_images = trainval_dataset[\"test_images\"]\n",
    "test_labels = trainval_dataset[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainval_images.shape)\n",
    "print(np.unique(trainval_labels, return_counts=True))\n",
    "\n",
    "print(test_images.shape)\n",
    "print(np.unique(test_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5cb725",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### a. Data Exploration\n",
    "\n",
    "a. Can you plot some images ?\n",
    "\n",
    "b. What is the aircraft/background ratio ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfb39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "754c7135",
   "metadata": {},
   "source": [
    "### b. Dataset Splitting\n",
    "\n",
    "Here we will split the trainval_dataset to obtain a training and a validation dataset.\n",
    "\n",
    "For example, try to use 20% of the images as validation\n",
    "\n",
    "You must have seen that the dataset was really unbalanced, so a random sampling will not work...\n",
    "\n",
    "Use stratified sampling to keep the label distribution between training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint to get your started\n",
    "background_indexes = np.where(trainval_labels == 0)[0]\n",
    "foreground_indexes = np.where(trainval_labels == 1)[0]\n",
    "\n",
    "train_bg_indexes = background_indexes[: int(0.8 * len(background_indexes))]\n",
    "valid_bg_indexes = background_indexes[int(0.8 * len(background_indexes)) :]\n",
    "\n",
    "train_fg_indexes = foreground_indexes[: int(0.8 * len(foreground_indexes))]\n",
    "valid_fg_indexes = foreground_indexes[int(0.8 * len(foreground_indexes)) :]\n",
    "\n",
    "train_indexes = list(train_bg_indexes) + list(train_fg_indexes)\n",
    "valid_indexes = list(valid_bg_indexes) + list(valid_fg_indexes)\n",
    "\n",
    "train_images = trainval_images[train_indexes, :, :, :]\n",
    "train_labels = trainval_labels[train_indexes]\n",
    "\n",
    "valid_images = trainval_images[valid_indexes, :, :, :]\n",
    "valid_labels = trainval_labels[valid_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(train_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(valid_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a2f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the dataset statistics in [0.,1.], we're going to use it to normalize our data\n",
    "\n",
    "mean = np.mean(train_images, axis=(0, 1, 2)) / 255.0\n",
    "std = np.std(train_images, axis=(0, 1, 2)) / 255.0\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0fb0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Q1. Training & metrics\n",
    "\n",
    "During Session 1, you learnt how to set up your environment on Colab, train a basic CNN on a small training set and plot metrics. Now let's do it again !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830eb0a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### First run\n",
    "\n",
    "Once you have downloaded & created your training & validation dataset, use the notebook from Session 1 to get:\n",
    "\n",
    "a. Training of the model using steps seen during Session 1\n",
    "\n",
    "b. Compute and plot metrics (confusion matrix, ROC curve) based on this training\n",
    "\n",
    "c. Compare the metrics between this new dataset and the one from Session 1\n",
    "\n",
    "d. What did you expect ? Is your model working well ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f857e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to get you started\n",
    "class NpArrayDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        image_transforms: Callable = None,\n",
    "        label_transforms: Callable = None,\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_transforms = label_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.images[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        if self.image_transforms is not None:\n",
    "            x = self.image_transforms(x)\n",
    "        else:\n",
    "            x = torch.tensor(x)\n",
    "\n",
    "        if self.label_transforms is not None:\n",
    "            y = self.label_transforms(y)\n",
    "        else:\n",
    "            y = torch.tensor(y)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        # Add data augmentation ?\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transforms = None\n",
    "\n",
    "# load the training data\n",
    "train_set = NpArrayDataset(\n",
    "    images=...,\n",
    "    labels=...,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=target_transforms,\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# load the validation data\n",
    "validation_set = NpArrayDataset(\n",
    "    images=...,\n",
    "    labels=...,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=target_transforms,\n",
    ")\n",
    "val_loader = DataLoader(validation_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d75083",
   "metadata": {},
   "source": [
    "define your model, fill the blanks\n",
    "\n",
    "Be careful, this time we are zero padding images so convolutions do not reduce image size !\n",
    "\n",
    "![padding](https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a82d77",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def _init_weights(model):\n",
    "    for m in model.modules():\n",
    "        # Initialize all convs\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    model = nn.Sequential(\n",
    "        # size: 3 x 64 x 64\n",
    "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "        # size: 32 x 64 x 64\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=..., out_channels=32, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # size: 32 x 32 x 32\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        # size: 64 x 32 x 32\n",
    "        nn.MaxPool2d(2),\n",
    "        # size: 64 x ? x ?\n",
    "        nn.Conv2d(in_channels=..., out_channels=128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=..., out_channels=128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # size: ? x ? x ?\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=..., out_channels=128, kernel_size=3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2),\n",
    "        # size: ? x ? x ?\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=..., out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.10),\n",
    "        nn.Linear(in_features=256, out_features=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "    _init_weights(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_name = ...\n",
    "model = model_fn()\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare optimizers and loss\n",
    "optimizer = ...\n",
    "criterion = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03857232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your training and plot your train/val metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a06d6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Test metrics, introduction to PR Curves\n",
    "\n",
    "During the previous notebook you plotted the Receiver Operating Characteristic curve. This is not ideal when dealing with imbalanced dataset since the issue of class imbalance can result in a serious bias towards the majority class, reducing the classification performance and increasing the number of **false positives**. Furthermore, in ROC curve calculation, true negatives don't have such meaning any longer.\n",
    "\n",
    "Instead this time we will plot the Precision Recall curve of our model which uses precision and recall to evaluate models.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/fit/t/1600/480/1*Ub0nZTXYT8MxLzrz0P7jPA.png)\n",
    "\n",
    "![](https://modtools.files.wordpress.com/2020/01/roc_pr-1.png?w=946)\n",
    "\n",
    "Refer here for a tutorial on how to plot such curve:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "\n",
    "More details on PR Curve:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "\n",
    "https://www.datascienceblog.net/post/machine-learning/interpreting-roc-curves-auc/\n",
    "\n",
    "**e. Plot the ROC curve of your model as well as its PR Curve, on the test set, compare them, which is easier to interpret ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf08bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curve\n",
    "\n",
    "# Compute PR Curve\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    PrecisionRecallDisplay,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "# We round predictions for better readability\n",
    "y_pred_probas = np.round(y_pred[:, 0], 2)\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(\n",
    "    y_true, y_pred_probas, pos_label=1\n",
    ")\n",
    "\n",
    "ap = average_precision_score(y_true, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(\n",
    "    recalls, precisions, color=\"darkorange\", lw=lw, label=\"PR Curve (AP = %0.2f)\" % ap\n",
    ")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac4ba2",
   "metadata": {},
   "source": [
    "**f. Can you understand why PR curve may be more useful than ROC curve for diagnosing model performance when dealing with imbalanced data ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55d719",
   "metadata": {},
   "source": [
    "**g. What is Fbeta-Score ? How can it help ? How do you chose beta?**\n",
    "\n",
    "Some reading: https://towardsdatascience.com/on-roc-and-precision-recall-curves-c23e9b63820c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta(precision, recall, beta=1.0):\n",
    "    if p == 0.0 or r == 0.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aff0eb",
   "metadata": {},
   "source": [
    "**h. Can you use the PR curve to choose a threshold ?**\n",
    "\n",
    "The same way you did for the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We round predictions every 0.05 for readability\n",
    "y_pred_probas = (y_pred[:, 0] / 0.05).astype(np.int) * 0.05\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(\n",
    "    y_true, y_pred_probas, pos_label=1\n",
    ")\n",
    "\n",
    "ap = average_precision_score(y_true, y_pred)\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.step(recalls, precisions, \"bo\", alpha=0.2, where=\"post\")\n",
    "plt.fill_between(recalls, precisions, alpha=0.2, color=\"b\", step=\"post\")\n",
    "\n",
    "for r, p, t in zip(recalls, precisions, thresholds):\n",
    "    plt.annotate(\n",
    "        np.round(t, 2),\n",
    "        xy=(r, p),\n",
    "        xytext=(r - 0.05, p - 0.05),\n",
    "        arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\"2-class Precision-Recall curve: AP={:0.2f}\".format(ap))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482c2db",
   "metadata": {},
   "source": [
    "You can also use the fbeta score to find the best threshold, for example to maximise f1 or f2...\n",
    "\n",
    "```python\n",
    "def find_best_threshold(precisions, recalls, thresholds, beta=2.):\n",
    "    best_fb = -np.inf\n",
    "    best_t = None\n",
    "    for t, p, r in zip(thresholds, precisions, recalls):\n",
    "        fb = fbeta(p, r, beta=beta)\n",
    "        if fb > best_fb:\n",
    "            best_t = t\n",
    "            best_fb = fb\n",
    "\n",
    "    return best_fb, best_t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f294f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "006426ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Plot \"hard\" examples\n",
    "\n",
    "- Plot some of the missclassified examples that have true label = 0: Those are false positives\n",
    "- Plot some of the missclassified examples that have true label = 1: those are false negatives (misses)\n",
    "\n",
    "Can you interpret the false positives ?\n",
    "\n",
    "Example for False Positives \n",
    "\n",
    "```python\n",
    "misclassified_idxs = np.where(y_pred_classes == 1 && y_true == 0)[0]\n",
    "\n",
    "\n",
    "print(len(misclassified_idxs))\n",
    "\n",
    "print(misclassified_idxs)\n",
    "\n",
    "misclassified_images = test_images[misclassified_idxs]\n",
    "misclassified_true_labels = test_labels[misclassified_idxs]\n",
    "misclassified_pred_labels = y_pred_classes[misclassified_idxs]\n",
    "\n",
    "grid_size = 4\n",
    "grid = np.zeros((grid_size * 64, grid_size * 64, 3)).astype(np.uint8)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        img = np.copy(misclassified_images[i * grid_size + j])\n",
    "        pred = np.copy(misclassified_pred_labels[i * grid_size + j])\n",
    "        color = (0, 255, 0) if pred == 1 else (255, 0, 0)\n",
    "        tile = cv2.rectangle(img, (0, 0), (64, 64), color, thickness=2)\n",
    "        grid[i * 64 : (i + 1) * 64, j * 64 : (j + 1) * 64, :] = img\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(grid)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ae1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aee444",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Q2. Class Imbalance\n",
    "\n",
    "We will try several things below. Those steps are only indicative and you are free to pursue other means of improving your model.\n",
    "\n",
    "Should you be lost, we refer you to the excellent \"A Recipe for Training Neural Networks\" article : https://karpathy.github.io/2019/04/25/recipe/\n",
    "\n",
    "![image.png](docs/static/img/mlsystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f66ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### a. Tackling the imbalanced data problem\n",
    "\n",
    "Go through your data: is the dataset balanced ? If now, which steps can I do to solve this imbalance problem ?\n",
    "\n",
    "- Which step would you take ?\n",
    "- **Don't forget to apply the same step on you train and validation dataset** but **not on your test set** as your test set should represent the final data distribution\n",
    "\n",
    "Try to decide and a method to modify only the dataset and rerun your training. Did performance improve ?\n",
    "\n",
    "\n",
    "HINT:\n",
    "- It's usually a mix of **oversampling** the minority class and **undersampling** the majority class\n",
    "\n",
    "Some readings:\n",
    "- https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets (very well done)\n",
    "- https://machinelearningmastery.com/framework-for-imbalanced-classification-projects/ (a bigger synthesis)\n",
    "- https://machinelearningmastery.com/category/imbalanced-classification/\n",
    "\n",
    "Hint to get you started\n",
    "```python\n",
    "background_indexes = np.where(trainval_labels == 0)\n",
    "foreground_indexes = np.where(trainval_labels == 1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32bcaa4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Rewrite your data selection to choose the same number of background and foreground classes to put into your training / validation set\n",
    "\n",
    "Does it improve the final performance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd90009",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "029da993",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### b. Hard Example Mining\n",
    "\n",
    "Another solution is called \"hard example mining\" : You could balance your dataset like before, but this time do it \"intelligently\", for example by selecting false positives and false negatives. Those are \"hard examples\",\n",
    "\n",
    "Usually we also put \"easy examples\" otherwise our dataset may be very biased\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTA35C_SgBtMsS1bt_VR7HC2vDaK8zIlIyw9w&usqp=CAU\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "You can see this effect easily on a confusion matrix :\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2102/1*fxiTNIgOyvAombPJx5KGeA.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "If you want to rebalance your dataset by undersampling the 0 class, why not select more false positives than true negatives ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9071c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Rewrite your data selection to ensure there are false positives in your negative selected examples.\n",
    "\n",
    "Does it improve the final performance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a21a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bde574ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "Rewrite your data selection to ensure there are false positives in your negative selected examples.\n",
    "\n",
    "Does it improve the final performance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361dcff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Q3. Transfer learning and Model architecture modification\n",
    "\n",
    "There are no absolute law concerning the structure of your deep Learning model. During the [Deep Learning class](https://github.com/SupaeroDataScience/deep-learning/blob/main/deep/Deep%20Learning.ipynb) you had an overview of existing models\n",
    "\n",
    "You can operate a modification on your structure and observe the effect on final metrics. Of course, remain consistent with credible models, cf Layer Patterns chapter on this \"must view\" course : http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "See here for an introduction to complex CNNs:\n",
    "http://cs231n.stanford.edu/slides/2023/lecture_6.pdf\n",
    "\n",
    "<img src=\"docs/static/img/comparison_architectures.png\" alt=\"pokemon\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "For usual tasks such as classification or detection, we use \"transfer learning\":\n",
    "\n",
    "    In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n",
    "    \n",
    "Adapt this tutorial to do transfer learning from a network available in torchvision to our use case\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "I advise you to select resnet18\n",
    "\n",
    "The biggest library of pretrained models is available here :\n",
    "\n",
    "https://github.com/rwightman/pytorch-image-models\n",
    "\n",
    "You can also use off the shelf architecture provided by torchvision, for example:\n",
    "\n",
    "```python\n",
    "import torchvision.models\n",
    "\n",
    "resnet18 = torchvision.models.resnet18(num_classes=2)\n",
    "```\n",
    "\n",
    "You can also use [transfer learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/) to \"finetune\" already trained features on your dataset\n",
    "\n",
    "You can adapt one of those two tutorials that use either torchvision or timm to take an existing pre-trained CNN and \"finetune int\" for your data, while training only a few parameters\n",
    "\n",
    "A simple option : [torchvision tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#finetuning-the-convnet)\n",
    "\n",
    "A more advanced library : [timm tutorial](https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e) \n",
    "\n",
    "Note that `timm` is a very famous library that stores all the state of the art CNN and Vision Transformer models for your needs\n",
    "\n",
    "**Exercise*** Change your model function for a transfer learning one. Does it improve your performance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba32f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89f390c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Q4. Optimizer and other hyperparameters modifications\n",
    "\n",
    "i ) Now that you have worked on your dataset, it's time to tune your network and your training configuration\n",
    "\n",
    "In Session 1, you tested two different optimizers. What is the effect of its modification? Apply it to your training and compare metrics.\n",
    "\n",
    "ii ) An other important parameter is the learning rate, you can [check its effect on the behavior of your training](https://developers.google.com/machine-learning/crash-course/fitter/graph).\n",
    "\n",
    "You can also try other things such as data augmentation, ...\n",
    "\n",
    "Here is an overview of [possible hyperparameter tuning when training Convolutional Neural Networks](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8)\n",
    "\n",
    "You can try and apply those techniques to your use case.\n",
    "\n",
    "- Does these techniques yield good results ? What about the effort-spent-for-performance ratio ?\n",
    "- Do you find it easy to keep track of your experiments ?\n",
    "- What would you need to have a better overview of the effects of these search ?\n",
    "\n",
    "Don't spend too much time on this part as the next is more important. You can come back to it after you're finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839f41a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q3.a here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd03f05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Q5. Full Test whole dataset & more improvements\n",
    "\n",
    "a. Now that you have optimised your structure for your dataset, you will apply your model to the test dataset to see the final metrics. Plot all your metrics using the full imbalanced test set. Is it good enough ?\n",
    "If you think so, you can apply it to new images using the sliding window technique with the 3rd notebook\n",
    "\n",
    "- Did it bring any improvements ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45278b1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q3a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e973cd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "b. If you're not satisfied with the output of your model, consider the following idea: Training a new model with the failures of your previous model.\n",
    "Try the following:\n",
    "- Get all the images with the \"aircraft\" label\n",
    "- Get all the images with the \"background\" label where your best model was wrong (predicted aircraft), as well as some of the background where it was right.\n",
    "- Train a new model or retrain your existing one on this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae773ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd2bef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "c . **SAVE YOUR MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7037623",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q3c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e56c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Have you saved your model ??** You will need it to relaunch the sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b743f5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Food for thought\n",
    "\n",
    "Reflect on what you just did. What is the most efficient way to gain performance ? data or model / hyperparameters ?\n",
    "\n",
    "Reflect on what is missing from this notebook. Hint : Data preparation !"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
