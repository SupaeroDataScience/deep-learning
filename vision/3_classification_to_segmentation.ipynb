{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f469b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# Session 3: From Image Classification to Image Segmentation\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Florient Chouteau | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a76c43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "In the previous sessions, you learned how to train a CNN for **image classification** - predicting a single label per image (clear vs cloudy).\n",
    "\n",
    "In this session, you will learn about **semantic segmentation** - predicting a label for **every pixel** in an image. This is a fundamental task in computer vision with applications in medical imaging, autonomous driving, satellite imagery analysis, and more.\n",
    "\n",
    "**By the end of this session, you will:**\n",
    "- Understand the difference between classification and dense prediction tasks\n",
    "- Learn why classification CNNs can't directly solve segmentation\n",
    "- Implement encoder-decoder architectures for dense prediction\n",
    "- Understand the importance of skip connections (UNet architecture)\n",
    "- Evaluate segmentation models using IoU and Dice metrics\n",
    "- Apply data augmentation to segmentation tasks\n",
    "\n",
    "**Prerequisites:**\n",
    "- You should be comfortable with CNNs from Session 0\n",
    "- You should have trained a classifier in Session 1\n",
    "\n",
    "**First steps:**\n",
    "- Activate GPU runtime in Colab\n",
    "- Check using `!nvidia-smi` that you detect it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78228f8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Installation script for torchinfo package (if needed)\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b91157",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d74ad4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Put your imports here\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import tqdm\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94b98d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Part 1: Understanding Segmentation - From Classification to Dense Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90fe081",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### What is Semantic Segmentation?\n",
    "\n",
    "**Classification** answers: \"Is there a cloud in this image?\" → **One label** per image\n",
    "\n",
    "**Segmentation** answers: \"Which pixels belong to clouds?\" → **One label per pixel**\n",
    "\n",
    "```\n",
    "Classification:              Segmentation:\n",
    "Image (64x64x3)       →     Image (64x64x3)\n",
    "Model                 →     Model\n",
    "Probability (1,)      →     Probability map (64x64x2)\n",
    "\"cloudy: 0.92\"        →     \"pixel [10,15]: cloud 0.95\"\n",
    "```\n",
    "\n",
    "This is also called **dense prediction** because we predict at every spatial location.\n",
    "\n",
    "![dense](https://fchouteau.github.io/isae-intro-to-cnns/static/img/computervision_tasks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56c9d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9532e12",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "TRAINVAL_DATASET_URL = \"https://storage.googleapis.com/fchouteau-isae-deep-learning/toy_cloud_segmentation_2025.npz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5f273",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Utility Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793753ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def overlay_img_msk(img, msk, color=\"red\"):\n",
    "    \"\"\"Overlay an image with a colored mask\"\"\"\n",
    "    c = color_to_rgb(color)\n",
    "    msk = class_mask_to_color_mask(msk, class_colors=[[0, 0, 0], c])\n",
    "    return overlay_img_rgb_msk(img, msk)\n",
    "\n",
    "\n",
    "def overlay_img_prd(img, prd, cmap=\"viridis\"):\n",
    "    # Create overlays for ground truth and prediction without threshold\n",
    "    cm = plt.get_cmap(cmap)\n",
    "    pred_mask = (cm(prd)[:, :, :3] * 255.0).astype(np.uint8)\n",
    "\n",
    "    # Overlay predicted mask on image\n",
    "    pred_overlay = overlay_img_rgb_msk(img, pred_mask)\n",
    "\n",
    "    return pred_overlay\n",
    "\n",
    "\n",
    "def overlay_img_rgb_msk(img, msk, coefficients=[0.5, 0.5]):\n",
    "    \"\"\"\n",
    "    Overlay a raw IMG and its MSK (0.5 * IMG_In_Visible + 0.5 * Msk)\n",
    "    Args:\n",
    "        img(np.ndarray): Img array (from imread)\n",
    "        msk(np.ndarray): RGB mask\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray containing the overlay (it's a copy)\n",
    "    \"\"\"\n",
    "    tmp = np.copy(img).astype(np.float32)\n",
    "    tmp2 = np.copy(msk).astype(np.float32)\n",
    "    if len(tmp2.shape) == 2:\n",
    "        idxs = tmp2 != 0.0\n",
    "    else:\n",
    "        c = tmp2.shape[2]\n",
    "        idxs = ~np.all(tmp2 == [0.0 for _ in range(c)], axis=-1)\n",
    "\n",
    "    tmp[idxs] = coefficients[0] * tmp[idxs] + coefficients[1] * tmp2[idxs]\n",
    "    tmp = np.clip(tmp, 0.0, 255.0)\n",
    "    tmp = tmp.astype(np.uint8)\n",
    "    del tmp2\n",
    "\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def color_to_rgb(color_str: str):\n",
    "    \"\"\"Convert color name to RGB tuple\"\"\"\n",
    "    if isinstance(color_str, str):\n",
    "        # https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "        color = np.asarray(matplotlib.colors.to_rgb(color_str))\n",
    "        color = (255.0 * color).astype(np.uint8)\n",
    "        color = tuple([int(c) for c in color])\n",
    "        return color\n",
    "    else:\n",
    "        return color_str\n",
    "\n",
    "\n",
    "def class_mask_to_color_mask(msk: np.ndarray, class_colors: List):\n",
    "    \"\"\"Convert class indices to colored mask\"\"\"\n",
    "    if len(msk.shape) == 3:\n",
    "        msk = msk[:, :, 0]\n",
    "    color_mask = np.asarray(class_colors)[msk]\n",
    "    return color_mask\n",
    "\n",
    "\n",
    "def color_image_to_class_mask(msk: np.ndarray, class_colors: List):\n",
    "    \"\"\"Convert colored mask to class indices\"\"\"\n",
    "    h, w = msk.shape[:2]\n",
    "    class_mask = np.zeros((h, w)).astype(np.uint8)\n",
    "    for cls_index, cls_color in enumerate(class_colors):\n",
    "        cls_occ = np.all(msk == cls_color, axis=-1)\n",
    "        class_mask[cls_occ] = cls_index + 1\n",
    "    return class_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151310be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Downloading the Dataset\n",
    "\n",
    "We use the same cloud images as Session 1, but now with **pixel-wise masks** instead of image-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6660b69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "ds = np.lib.npyio.DataSource(destpath=\"/tmp/\")\n",
    "f = ds.open(TRAINVAL_DATASET_URL, \"rb\")\n",
    "\n",
    "toy_dataset = np.load(f)\n",
    "trainval_images = toy_dataset[\"train_images\"]\n",
    "trainval_labels = toy_dataset[\"train_labels\"]\n",
    "test_images = toy_dataset[\"test_images\"]\n",
    "test_labels = toy_dataset[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d0db9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e102e78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training set:\")\n",
    "print(f\"  Images shape: {trainval_images.shape}\")  # (N, H, W, C)\n",
    "print(f\"  Labels shape: {trainval_labels.shape}\")  # (N, H, W) - per-pixel labels!\n",
    "print()\n",
    "print(\"Test set:\")\n",
    "print(f\"  Images shape: {test_images.shape}\")\n",
    "print(f\"  Labels shape: {test_labels.shape}\")\n",
    "print()\n",
    "print(\"Key difference from classification:\")\n",
    "print(f\"  Classification labels: (N,) - one label per image\")\n",
    "print(f\"  Segmentation labels: (N, H, W) - one label per pixel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa0bd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "LABEL_NAMES = [\"Clear\", \"Cloud\"]\n",
    "unique_classes = np.unique(trainval_labels)\n",
    "print(f\"Classes in dataset: {unique_classes}\")\n",
    "print(f\"Class names: {LABEL_NAMES}\")\n",
    "print()\n",
    "\n",
    "# Count pixels per class (more informative than image counts for segmentation)\n",
    "total_pixels = trainval_labels.size\n",
    "for cls_idx, cls_name in enumerate(LABEL_NAMES):\n",
    "    pixel_count = np.sum(trainval_labels == cls_idx)\n",
    "    percentage = 100.0 * pixel_count / total_pixels\n",
    "    print(\n",
    "        f\"Class '{cls_name}' (idx={cls_idx}): {pixel_count:,} pixels ({percentage:.1f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba2d51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Visualizing Segmentation Masks\n",
    "\n",
    "Let's visualize images with their pixel-wise annotations overlaid in red (clouds) on the original satellite images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31d315",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "grid_size = 8\n",
    "grid = np.zeros((grid_size * 64, grid_size * 64, 3)).astype(np.uint8)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        img = trainval_images[i * grid_size + j]\n",
    "        msk = trainval_labels[i * grid_size + j]\n",
    "        tile = overlay_img_msk(img, msk, color=\"red\")\n",
    "        grid[i * 64 : (i + 1) * 64, j * 64 : (j + 1) * 64, :] = tile\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.imshow(grid)\n",
    "ax.set_title(\"Training samples: Images with cloud masks overlaid (red=cloud)\")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f252998",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Observation:** These look like the same satellite images from Session 1, but now we have detailed pixel-level annotations showing exactly where clouds are located, not just whether the image contains clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2160ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Train/Validation Split\n",
    "\n",
    "Just like in Session 1, we split our training data into train and validation sets (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19606ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "idxs = np.random.permutation(np.arange(trainval_images.shape[0]))\n",
    "train_idxs, val_idxs = idxs[: int(0.8 * len(idxs))], idxs[int(0.8 * len(idxs)) :]\n",
    "\n",
    "train_images = trainval_images[train_idxs]\n",
    "train_labels = trainval_labels[train_idxs]\n",
    "val_images = trainval_images[val_idxs]\n",
    "val_labels = trainval_labels[val_idxs]\n",
    "\n",
    "print(f\"Train set: {train_images.shape[0]} samples\")\n",
    "print(f\"Val set: {val_images.shape[0]} samples\")\n",
    "print(f\"Test set: {test_images.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f241d3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Computing Dataset Statistics\n",
    "\n",
    "We'll reuse the normalization from Session 1 (computed on the classification dataset).\n",
    "Since we're using the same underlying images, the statistics are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d01797",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compute mean and std for normalization (in [0, 1] range)\n",
    "mean = np.mean(train_images, axis=(0, 1, 2)) / 255.0\n",
    "std = np.std(train_images, axis=(0, 1, 2)) / 255.0\n",
    "\n",
    "print(f\"Dataset normalization statistics:\")\n",
    "print(f\"  Mean: {mean}\")\n",
    "print(f\"  Std:  {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd1bd0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Part 2: Can We Reuse Classification Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b4779",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### The Challenge: From 1 Output to 64×64 Outputs\n",
    "\n",
    "In Session 1, our classification CNN had this structure:\n",
    "\n",
    "```\n",
    "Input: (B, 3, 64, 64)\n",
    "  ↓ Conv blocks (downsample with pooling)\n",
    "Features: (B, C, 8, 8) - spatial resolution reduced\n",
    "  ↓ Flatten + MLP\n",
    "Output: (B, 1) - single probability\n",
    "```\n",
    "\n",
    "For segmentation, we need:\n",
    "\n",
    "```\n",
    "Input: (B, 3, 64, 64)\n",
    "  ↓ ??? Architecture ???\n",
    "Output: (B, 2, 64, 64) - probability for each pixel\n",
    "```\n",
    "\n",
    "**Key problems:**\n",
    "1. **Flattening destroys spatial information** - we can't recover pixel positions after flatten()\n",
    "2. **Pooling reduces resolution** - we need to upsample back to original size\n",
    "3. **Need to preserve spatial structure** - which pixels are neighbors matters!\n",
    "\n",
    "**Solution:** **Encoder-Decoder** architectures\n",
    "- **Encoder**: Downsample to extract features (like classification CNN)\n",
    "- **Decoder**: Upsample features back to original resolution\n",
    "- Output: Per-pixel predictions via 1×1 convolution\n",
    "\n",
    "![encoderdecoder](https://towardsdatascience.com/wp-content/uploads/2022/11/1XeUglwXyh7967mlMOF20Zw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2516ca9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Understanding Upsampling Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d684ad8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "#### The Upsampling Challenge\n",
    "\n",
    "Before we build our segmentation models, we need to understand **upsampling** - the operation that increases spatial resolution.\n",
    "\n",
    "**Recap: Downsampling (what we know)**\n",
    "- **MaxPool2d**: Reduces spatial size (64×64 → 32×32)\n",
    "- **Strided Conv**: Conv with stride > 1 also downsamples\n",
    "- Non-learnable: no parameters to train\n",
    "\n",
    "**New: Upsampling (what we need)**\n",
    "- Increase spatial size (8×8 → 16×16 → 32×32 → 64×64)\n",
    "- Two approaches:\n",
    "  1. **Non-learnable**: Fixed interpolation (nearest, bilinear)\n",
    "  2. **Learnable**: Transposed convolution (has trainable weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ea1d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Approach 1: Non-Learnable Upsampling\n",
    "\n",
    "**Nearest Neighbor Upsampling:**\n",
    "- Repeat each pixel value\n",
    "- Fast but creates blocky artifacts\n",
    "\n",
    "**Bilinear Upsampling:**\n",
    "- Interpolate between pixels\n",
    "- Smoother but still not learnable\n",
    "\n",
    "```\n",
    "Original (2×2):     Nearest Neighbor (4×4):    Bilinear (4×4):\n",
    "[1  2]              [1  1  2  2]               [1.0  1.3  1.7  2.0]\n",
    "[3  4]              [1  1  2  2]               [1.7  2.0  2.3  2.7]\n",
    "                    [3  3  4  4]               [2.3  2.7  3.0  3.3]\n",
    "                    [3  3  4  4]               [3.0  3.3  3.7  4.0]\n",
    "```\n",
    "\n",
    "![](https://mriquestions.com/uploads/3/4/5/7/34572113/uppooling-methods_orig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8caa2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Approach 2: Transposed Convolution (Learnable Upsampling)\n",
    "\n",
    "**Key Idea:** Like regular convolution, but in reverse - **increases** spatial size while learning optimal upsampling.\n",
    "\n",
    "**Regular Convolution (downsampling):**\n",
    "- Applies kernel to input → smaller output\n",
    "- Example: 4×4 input + 3×3 kernel (stride=2) → 2×2 output\n",
    "\n",
    "**Transposed Convolution (upsampling):**\n",
    "- Applies kernel in a \"reverse\" manner → larger output\n",
    "- Example: 2×2 input + 3×3 kernel (stride=2) → 4×4 output\n",
    "- **Learnable**: The kernel weights are trained via backpropagation!\n",
    "\n",
    "**Why \"transposed\"?**\n",
    "- The operation is mathematically related to the transpose of the convolution matrix\n",
    "\n",
    "![tconv](https://towardsdatascience.com/wp-content/uploads/2022/06/1kv5m8-VXHZ5RzHu70Jt_BQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4573168",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Visual Example: Strided Convolution vs Transposed Convolution\n",
    "\n",
    "Let's see how convolution reduces size and transposed convolution increases it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed714e40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 1: Strided Convolution (Downsampling)\n",
    "print(\"=\" * 60)\n",
    "print(\"STRIDED CONVOLUTION (Downsampling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple 4×4 input\n",
    "input_tensor = torch.arange(1, 17, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "print(\"\\nInput (4×4):\")\n",
    "print(input_tensor.squeeze().numpy())\n",
    "\n",
    "# Define a 3×3 convolution with stride=2 (downsamples)\n",
    "conv_down = nn.Conv2d(\n",
    "    in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=0, bias=False\n",
    ")\n",
    "# Initialize with known weights for clarity\n",
    "conv_down.weight.data = torch.ones(1, 1, 3, 3) * 0.1\n",
    "\n",
    "output_down = conv_down(input_tensor)\n",
    "print(f\"\\nAfter Conv2d(kernel=3, stride=2, padding=0):\")\n",
    "print(f\"Output shape: {output_down.shape} (spatial size reduced: 4×4 → 2×2)\")\n",
    "print(\"Output (2×2):\")\n",
    "print(output_down.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d1442",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example 2: Transposed Convolution (Upsampling)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRANSPOSED CONVOLUTION (Upsampling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Start with the 2×2 output from above\n",
    "input_small = output_down\n",
    "\n",
    "# Define a transposed convolution with stride=2 (upsamples)\n",
    "conv_up = nn.ConvTranspose2d(\n",
    "    in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=0, bias=False\n",
    ")\n",
    "# Initialize with known weights\n",
    "conv_up.weight.data = torch.ones(1, 1, 3, 3) * 0.1\n",
    "\n",
    "output_up = conv_up(input_small)\n",
    "print(f\"\\nAfter ConvTranspose2d(kernel=3, stride=2, padding=0):\")\n",
    "print(f\"Output shape: {output_up.shape} (spatial size increased: 2×2 → 5×5)\")\n",
    "print(\"Output (5×5):\")\n",
    "print(output_up.squeeze().detach().numpy())\n",
    "\n",
    "print(\n",
    "    \"\\n✓ Notice: TransposedConv with stride=2 approximately doubles spatial dimensions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460fdbc3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Controlling Output Size with Transposed Convolution\n",
    "\n",
    "**Common pattern for exact size doubling:**\n",
    "- `kernel_size=2, stride=2, padding=0` → Exactly doubles spatial size\n",
    "- Example: 8×8 → 16×16, 16×16 → 32×32\n",
    "\n",
    "**Formula for output size:**\n",
    "```\n",
    "output_size = (input_size - 1) * stride - 2 * padding + kernel_size\n",
    "```\n",
    "\n",
    "Let's verify this works for our decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ef5a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXACT SIZE DOUBLING FOR DECODER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate decoder upsampling path\n",
    "sizes = [8, 16, 32, 64]\n",
    "for i in range(len(sizes) - 1):\n",
    "    input_size = sizes[i]\n",
    "    target_size = sizes[i + 1]\n",
    "\n",
    "    # Create dummy input\n",
    "    dummy = torch.randn(1, 64, input_size, input_size)\n",
    "\n",
    "    # Transposed conv with kernel=2, stride=2\n",
    "    upconv = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2, padding=0)\n",
    "    output = upconv(dummy)\n",
    "\n",
    "    print(\n",
    "        f\"\\nLevel {i+1}: {input_size}×{input_size} → {output.shape[2]}×{output.shape[3]}\"\n",
    "    )\n",
    "    print(f\"  Using: ConvTranspose2d(kernel=2, stride=2)\")\n",
    "    print(\n",
    "        f\"  Expected: {target_size}×{target_size} | Actual: {output.shape[2]}×{output.shape[3]}\"\n",
    "    )\n",
    "    assert output.shape[2] == target_size, \"Size mismatch!\"\n",
    "\n",
    "print(\"\\n✓ All upsampling steps produce correct sizes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dede50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### How Transposed Convolution Works (Detailed)\n",
    "\n",
    "**Visual intuition for stride=2:**\n",
    "\n",
    "1. **Insert zeros** between input pixels (creates spacing)\n",
    "2. **Apply standard convolution** on the expanded input\n",
    "3. Result: Larger output!\n",
    "\n",
    "```\n",
    "Input (2×2):        After zero insertion:    After convolution:\n",
    "[a  b]              [a  0  b  0]             [... larger output ...]\n",
    "[c  d]              [0  0  0  0]\n",
    "                    [c  0  d  0]\n",
    "                    [0  0  0  0]\n",
    "```\n",
    "\n",
    "**Key differences from regular Conv:**\n",
    "\n",
    "| Property | Conv2d | ConvTranspose2d |\n",
    "|----------|--------|-----------------|\n",
    "| **Purpose** | Extract features, downsample | Upsample, reconstruct spatial info |\n",
    "| **Spatial effect** | Reduces size (stride > 1) | Increases size (stride > 1) |\n",
    "| **Parameters** | Learnable kernel | Learnable kernel |\n",
    "| **Use case** | Encoder | Decoder |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa245a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Comparing Upsampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d6361",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a small feature map to upsample\n",
    "small_feature = torch.randn(1, 1, 4, 4)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(small_feature.squeeze().numpy(), cmap=\"viridis\")\n",
    "axes[0].set_title(\"Original Feature Map (4×4)\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Method 1: Nearest Neighbor\n",
    "upsampled_nearest = F.interpolate(small_feature, scale_factor=2, mode=\"nearest\")\n",
    "axes[1].imshow(upsampled_nearest.squeeze().numpy(), cmap=\"viridis\")\n",
    "axes[1].set_title(\"Nearest Neighbor (8×8)\\nFast but blocky\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Method 2: Bilinear\n",
    "upsampled_bilinear = F.interpolate(\n",
    "    small_feature, scale_factor=2, mode=\"bilinear\", align_corners=False\n",
    ")\n",
    "axes[2].imshow(upsampled_bilinear.squeeze().numpy(), cmap=\"viridis\")\n",
    "axes[2].set_title(\"Bilinear Interpolation (8×8)\\nSmooth but not learnable\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Method 3: Transposed Convolution\n",
    "transposed_conv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, padding=0)\n",
    "upsampled_transposed = transposed_conv(small_feature)\n",
    "axes[3].imshow(upsampled_transposed.squeeze().detach().numpy(), cmap=\"viridis\")\n",
    "axes[3].set_title(\"Transposed Conv (8×8)\\nLearnable upsampling\")\n",
    "axes[3].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visual comparison:\")\n",
    "print(f\"  Nearest:    {'Blocky artifacts, no learning'}\")\n",
    "print(f\"  Bilinear:   {'Smooth, but fixed interpolation'}\")\n",
    "print(f\"  Transposed: {'Learnable kernel adapts to data!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902d3794",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Why Use Transposed Convolution for Segmentation?\n",
    "\n",
    "**Advantages:**\n",
    "1. **Learnable**: Kernel weights adapt to your specific task\n",
    "2. **End-to-end training**: Gradients flow through upsampling layers\n",
    "3. **Feature-aware**: Can learn to upsample different features differently\n",
    "4. **Better reconstruction**: Especially for complex patterns\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Checkerboard artifacts**: Can create unwanted patterns (fixable with careful kernel size)\n",
    "2. **More parameters**: Slightly more memory than interpolation\n",
    "3. **Slower**: More computation than simple interpolation\n",
    "\n",
    "**Bottom line:** For segmentation, the learnable upsampling is worth it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9ef24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Practical Tips for Transposed Convolution\n",
    "\n",
    "**1. Checkerboard Artifacts:**\n",
    "- Problem: `kernel_size` not divisible by `stride` → overlapping outputs\n",
    "- Solution: Use `kernel_size` that is multiple of `stride`\n",
    "- Example: `kernel_size=2, stride=2` or `kernel_size=4, stride=2`\n",
    "\n",
    "**2. Size Matching:**\n",
    "- For exact doubling: `kernel_size=2, stride=2, padding=0`\n",
    "- Alternative: `kernel_size=4, stride=2, padding=1` (reduces artifacts)\n",
    "\n",
    "**3. Combination with Conv:**\n",
    "- Common pattern: `ConvTranspose2d` → `Conv2d` → `ReLU`\n",
    "- The Conv2d after upsampling refines the output and reduces artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9246718",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Summary: Upsampling for Segmentation\n",
    "\n",
    "| Method | Learnable? | Quality | Speed | Use Case |\n",
    "|--------|-----------|---------|-------|----------|\n",
    "| **Nearest** | ❌ | Low (blocky) | Fast | Quick prototyping |\n",
    "| **Bilinear** | ❌ | Medium (smooth) | Fast | Non-critical upsampling |\n",
    "| **Transposed Conv** | ✅ | High (adaptive) | Slower | **Segmentation decoders** |\n",
    "\n",
    "**For the rest of this notebook:**\n",
    "- We'll use `nn.ConvTranspose2d` in our decoder\n",
    "- This allows the model to **learn optimal upsampling** for cloud segmentation\n",
    "- The decoder weights will be trained jointly with the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230dff2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Part 3: Preparing for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb1157",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Dataset Class for Segmentation\n",
    "\n",
    "Similar to Session 1, but now labels are masks (H, W) instead of scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb9649",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for segmentation.\n",
    "    Images: (N, H, W, 3) uint8 arrays\n",
    "    Labels: (N, H, W) uint8 arrays with class indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        image_transforms: Callable = None,\n",
    "        label_transforms: Callable = None,\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_transforms = label_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        x = self.images[index]  # (H, W, 3)\n",
    "        y = self.labels[index]  # (H, W)\n",
    "\n",
    "        if self.image_transforms is not None:\n",
    "            x = self.image_transforms(x)\n",
    "        else:\n",
    "            x = torch.tensor(x).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        if self.label_transforms is not None:\n",
    "            y = self.label_transforms(y)\n",
    "        else:\n",
    "            y = torch.tensor(y).long()  # CrossEntropyLoss expects Long\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f350c76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Transforms for Images and Masks\n",
    "\n",
    "**Important:** For segmentation, we normalize images but keep masks as integer class indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b4410",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Image transforms: convert to tensor and normalize\n",
    "image_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # Converts (H,W,3) -> (3,H,W) and scales to [0,1]\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Label transforms: just convert to tensor (keep as integers)\n",
    "def label_transform(label):\n",
    "    return torch.tensor(label).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672db724",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = SegmentationDataset(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=label_transform,\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    val_images,\n",
    "    val_labels,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=label_transform,\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_images,\n",
    "    test_labels,\n",
    "    image_transforms=image_transforms,\n",
    "    label_transforms=label_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50748e17",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check a sample\n",
    "sample_img, sample_mask = train_dataset[0]\n",
    "print(f\"Image tensor shape: {sample_img.shape}\")  # (3, 64, 64)\n",
    "print(f\"Mask tensor shape: {sample_mask.shape}\")  # (64, 64)\n",
    "print(f\"Mask dtype: {sample_mask.dtype}\")  # torch.int64\n",
    "print(f\"Mask unique values: {torch.unique(sample_mask)}\")  # Should be [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a262057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Verifying Transforms with Inverse Transform\n",
    "\n",
    "**Following Andrej Karpathy's advice:** Always verify your data pipeline by inverse-transforming a sample.\n",
    "This catches bugs like wrong normalization, incorrect channel order, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7134688",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def inverse_transform(img_tensor, mean, std):\n",
    "    \"\"\"\n",
    "    Inverse normalization to visualize transformed images\n",
    "    Args:\n",
    "        img_tensor: (3, H, W) normalized tensor\n",
    "    Returns:\n",
    "        (H, W, 3) uint8 numpy array\n",
    "    \"\"\"\n",
    "    mean_t = torch.tensor(mean).view(3, 1, 1)\n",
    "    std_t = torch.tensor(std).view(3, 1, 1)\n",
    "    img = img_tensor * std_t + mean_t  # Denormalize\n",
    "    img = img.clamp(0, 1)  # Clip to valid range\n",
    "    img = (img * 255).byte()  # Scale to [0, 255]\n",
    "    img = img.permute(1, 2, 0).numpy()  # (3,H,W) -> (H,W,3)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeddd221",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify inverse transform\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(train_images[1])\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# After transforms + inverse\n",
    "sample_img, sample_mask = train_dataset[1]\n",
    "reconstructed = inverse_transform(sample_img, mean, std)\n",
    "axes[1].imshow(reconstructed)\n",
    "axes[1].set_title(\"After Transform → Inverse Transform\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Mask visualization\n",
    "axes[2].imshow(sample_mask.numpy(), cmap=\"gray\")\n",
    "axes[2].set_title(\"Mask (0=clear, 1=cloud)\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Inverse transform looks correct - proceeding with training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca8a55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cea936",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f54ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5714b62a",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠ Using CPU - training will be slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9007a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Part 4: Basic FCN (Fully Convolutional Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503e407",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### FCN Architecture Overview\n",
    "\n",
    "A **Fully Convolutional Network** (FCN) has two parts:\n",
    "\n",
    "**Encoder (Downsampling path):**\n",
    "```\n",
    "(3, 64, 64)\n",
    "  ↓ Conv + Pool\n",
    "(32, 32, 32)\n",
    "  ↓ Conv + Pool\n",
    "(64, 16, 16)\n",
    "  ↓ Conv + Pool\n",
    "(128, 8, 8)  ← Bottleneck\n",
    "```\n",
    "\n",
    "**Decoder (Upsampling path):**\n",
    "```\n",
    "(128, 8, 8)\n",
    "  ↓ TransposedConv (upsample)\n",
    "(64, 16, 16)\n",
    "  ↓ TransposedConv (upsample)\n",
    "(32, 32, 32)\n",
    "  ↓ TransposedConv (upsample)\n",
    "(16, 64, 64)\n",
    "  ↓ 1×1 Conv (classification)\n",
    "(2, 64, 64)  ← Output (2 classes per pixel)\n",
    "```\n",
    "\n",
    "**Key operations:**\n",
    "- `nn.Conv2d`: Extract features\n",
    "- `nn.MaxPool2d`: Downsample (reduce spatial resolution)\n",
    "- `nn.ConvTranspose2d`: Upsample (increase spatial resolution)\n",
    "- Final 1×1 conv: Per-pixel classification\n",
    "\n",
    "![encoderdecoder](https://towardsdatascience.com/wp-content/uploads/2022/11/1XeUglwXyh7967mlMOF20Zw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410acef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fcfec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic convolutional block: Conv → ReLU → Conv → ReLU\n",
    "    Used in encoder path\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07464cab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### FCN Encoder (Feature Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529998bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class FCNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder: Downsample input to extract hierarchical features\n",
    "    64×64 → 32×32 → 16×16 → 8×8\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Level 1: 64×64\n",
    "        self.conv1 = ConvBlock(3, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)  # → 32×32\n",
    "\n",
    "        # Level 2: 32×32\n",
    "        self.conv2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)  # → 16×16\n",
    "\n",
    "        # Level 3: 16×16\n",
    "        self.conv3 = ConvBlock(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)  # → 8×8\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.conv1(x)  # (32, 64, 64)\n",
    "        x = self.pool1(x1)  # (32, 32, 32)\n",
    "\n",
    "        x2 = self.conv2(x)  # (64, 32, 32)\n",
    "        x = self.pool2(x2)  # (64, 16, 16)\n",
    "\n",
    "        x3 = self.conv3(x)  # (128, 16, 16)\n",
    "        x = self.pool3(x3)  # (128, 8, 8)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b896057",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### FCN Decoder (Upsampling)\n",
    "\n",
    "**Exercise:** Complete the decoder that upsamples features back to original resolution.\n",
    "\n",
    "**Hints:**\n",
    "- Use `nn.ConvTranspose2d` with `stride=2` to double spatial dimensions\n",
    "- Use `ConvBlock` after each upsampling to refine features\n",
    "- Final layer should output 1 channel (binary segmentation: cloud probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86242ea6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class FCNDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder: Upsample features back to input resolution\n",
    "    8×8 → 16×16 → 32×32 → 64×64\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Level 1: 8×8 → 16×16\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv1 = ConvBlock(64, 64)\n",
    "\n",
    "        # Level 2: 16×16 → 32×32\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.conv2 = ConvBlock(32, 32)\n",
    "\n",
    "        # Level 3: 32×32 → 64×64\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.conv3 = ConvBlock(16, 16)\n",
    "\n",
    "        # Final layer: 1×1 conv + Sigmoid for binary segmentation\n",
    "        self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decoder path\n",
    "        x = self.upconv1(x)  # (64, 16, 16)\n",
    "        x = self.conv1(x)  # Refine features\n",
    "\n",
    "        x = self.upconv2(x)  # (32, 32, 32)\n",
    "        x = self.conv2(x)  # Refine features\n",
    "\n",
    "        x = self.upconv3(x)  # (16, 64, 64)\n",
    "        x = self.conv3(x)  # Refine features\n",
    "\n",
    "        x = self.final_conv(x)  # (1, 64, 64) - logits\n",
    "        x = self.sigmoid(x)  # (1, 64, 64) - probabilities in [0, 1]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753b1cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Complete FCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4af1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Convolutional Network for binary segmentation\n",
    "    Input: (B, 3, H, W)\n",
    "    Output: (B, 1, H, W) - cloud probability per pixel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = FCNEncoder()\n",
    "        self.decoder = FCNDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        features = self.encoder(x)\n",
    "        # Decode\n",
    "        output = self.decoder(features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f40d37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "fcn_model = FCN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d9e89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify architecture and output shape\n",
    "dummy_input = torch.randn(2, 3, 64, 64).to(device)\n",
    "dummy_output = fcn_model(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {dummy_output.shape}\")\n",
    "print(f\"Output range: [{dummy_output.min():.3f}, {dummy_output.max():.3f}]\")\n",
    "print(f\"✓ Output has correct shape: (batch_size, 1, H, W)\")\n",
    "print(f\"✓ Output is probability in [0, 1] range (Sigmoid applied)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec15595",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "print(\"\\nFCN Architecture:\")\n",
    "torchinfo.summary(fcn_model, input_size=(1, 3, 64, 64), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4bfd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf87ce",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "# Loss function: Binary Cross Entropy for binary segmentation\n",
    "# Input: (B, 1, H, W) probabilities after sigmoid, Target: (B, H, W) binary masks\n",
    "# We use BCELoss since Sigmoid is already applied in the model\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(fcn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467003b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa040a31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch_segmentation(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, masks) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).float()  # Convert to float for BCE\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)  # (B, 1, H, W) probabilities\n",
    "        # BCE expects same shape for predictions and targets\n",
    "        masks_expanded = masks.unsqueeze(1)  # (B, H, W) → (B, 1, H, W)\n",
    "        loss = criterion(outputs, masks_expanded)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def val_epoch_segmentation(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).float()\n",
    "\n",
    "            outputs = model(images)  # (B, 1, H, W)\n",
    "            masks_expanded = masks.unsqueeze(1)  # (B, H, W) → (B, 1, H, W)\n",
    "            loss = criterion(outputs, masks_expanded)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402a6b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "fcn_train_losses = []\n",
    "fcn_val_losses = []\n",
    "\n",
    "print(\"Training FCN...\")\n",
    "for epoch in tqdm.trange(EPOCHS):\n",
    "    train_loss = train_epoch_segmentation(\n",
    "        fcn_model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    val_loss = val_epoch_segmentation(fcn_model, val_loader, criterion, device)\n",
    "\n",
    "    fcn_train_losses.append(train_loss)\n",
    "    fcn_val_losses.append(val_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "print(\"✓ FCN training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c96318",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b220f77",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(fcn_train_losses, label=\"Train Loss\")\n",
    "plt.plot(fcn_val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"FCN Training Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec1acee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Visualizing FCN Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73379192",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def visualize_segmentation_predictions(model, dataset, device, num_samples=8):\n",
    "    \"\"\"Visualize model predictions on validation set\"\"\"\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img_tensor, mask_gt = dataset[i]\n",
    "            img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # Predict\n",
    "            output = model(img_tensor_batch)  # (1, 1, H, W) probabilities\n",
    "            pred_mask = output.squeeze(0).squeeze(0).cpu()  # (H, W) float in [0, 1]\n",
    "\n",
    "            # Inverse transform for visualization\n",
    "            img_np = inverse_transform(img_tensor, mean, std)\n",
    "\n",
    "            # Overlay ground truth mask on image\n",
    "            gt_overlay = overlay_img_msk(img_np, mask_gt.numpy(), color=\"red\")\n",
    "\n",
    "            # Overlay predicted mask on image\n",
    "            pred_overlay = overlay_img_prd(img_np, pred_mask, cmap=\"jet\")\n",
    "\n",
    "            # Display\n",
    "            axes[i, 0].imshow(img_np)\n",
    "            axes[i, 0].set_title(\"Image\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "\n",
    "            axes[i, 1].imshow(gt_overlay)\n",
    "            axes[i, 1].set_title(\"Ground Truth (green=cloud)\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "\n",
    "            axes[i, 2].imshow(pred_overlay)\n",
    "            axes[i, 2].set_title(\"FCN Prediction (red=cloud)\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdb735",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "visualize_segmentation_predictions(fcn_model, val_dataset, device, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627469e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Observation:** The FCN produces reasonable segmentation masks, but boundaries are often **blurry** or **imprecise**. This happens because:\n",
    "1. We lost fine spatial details during downsampling (pooling)\n",
    "2. The decoder can't recover information that was discarded\n",
    "\n",
    "**Solution:** Add skip connections to preserve spatial information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85454644",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Part 5: UNet with Skip Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca487c2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Why Skip Connections?\n",
    "\n",
    "**Problem with FCN:**\n",
    "- Early encoder features (high-resolution) have fine spatial details\n",
    "- Late encoder features (low-resolution) have semantic information\n",
    "- Decoder only sees late features → blurry boundaries\n",
    "\n",
    "**UNet Solution:**\n",
    "- **Concatenate** encoder features directly to decoder at matching resolutions\n",
    "- Decoder gets both: semantic info (from bottleneck) + spatial details (from encoder)\n",
    "- Result: Sharp boundaries!\n",
    "\n",
    "```\n",
    "Encoder:              Decoder:\n",
    "(32, 64, 64) --------→ concat → (64+32, 64, 64)\n",
    "(64, 32, 32) ------→ concat → (128+64, 32, 32)\n",
    "(128, 16, 16) ----→ concat → (256+128, 16, 16)\n",
    "(128, 8, 8) ← Bottleneck\n",
    "```\n",
    "\n",
    "![UNet](https://towardsdatascience.com/wp-content/uploads/2022/11/1LH_JiIJngSllUZ0F8JYcwQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046b3b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### UNet Encoder (with feature storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc29b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class UNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet Encoder: Same as FCN but returns intermediate features for skip connections\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Level 1\n",
    "        self.conv1 = ConvBlock(3, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Level 2\n",
    "        self.conv2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Level 3\n",
    "        self.conv3 = ConvBlock(64, 128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(128, 128)\n",
    "\n",
    "    def forward(self, x) -> Tuple[Tensor, List[Tensor]]:\n",
    "        # Store features for skip connections\n",
    "        x1 = self.conv1(x)  # (32, 64, 64)\n",
    "        x = self.pool1(x1)\n",
    "\n",
    "        x2 = self.conv2(x)  # (64, 32, 32)\n",
    "        x = self.pool2(x2)\n",
    "\n",
    "        x3 = self.conv3(x)  # (128, 16, 16)\n",
    "        x = self.pool3(x3)\n",
    "\n",
    "        x = self.bottleneck(x)  # (128, 8, 8)\n",
    "\n",
    "        # Return bottleneck and skip connection features\n",
    "        return x, (x3, x2, x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d4b19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### UNet Decoder (with skip connections)\n",
    "\n",
    "**Exercise:** Complete the decoder with skip connections.\n",
    "\n",
    "**Hints:**\n",
    "- After upsampling, concatenate with encoder features: `torch.cat([x, skip_feat], dim=1)`\n",
    "- Adjust `ConvBlock` input channels to account for concatenated features\n",
    "- Example: If upsampled feature is 64 channels and skip is 64 channels → input is 128 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b691cc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet Decoder: Upsampling with skip connections from encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Level 1: 8×8 → 16×16\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv1 = ConvBlock(64 + 128, 64)  # +128 from skip connection\n",
    "\n",
    "        # Level 2: 16×16 → 32×32\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.conv2 = ConvBlock(32 + 64, 32)  # +64 from skip connection\n",
    "\n",
    "        # Level 3: 32×32 → 64×64\n",
    "        self.upconv3 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        self.conv3 = ConvBlock(16 + 32, 16)  # +32 from skip connection\n",
    "\n",
    "        # Final layer: 1×1 conv + Sigmoid for binary segmentation\n",
    "        self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: Tensor, skip_features: List[Tensor]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: bottleneck features (B, 128, 8, 8)\n",
    "            skip_features: list of [x3, x2, x1] from encoder\n",
    "        \"\"\"\n",
    "        # Unpack skip connections\n",
    "        x3, x2, x1 = skip_features\n",
    "\n",
    "        # Level 1\n",
    "        x = self.upconv1(x)  # (64, 16, 16)\n",
    "        x = torch.cat([x, x3], dim=1)  # Concatenate skip connection\n",
    "        x = self.conv1(x)  # (64, 16, 16)\n",
    "\n",
    "        # Level 2\n",
    "        x = self.upconv2(x)  # (32, 32, 32)\n",
    "        x = torch.cat([x, x2], dim=1)  # Concatenate skip connection\n",
    "        x = self.conv2(x)  # (32, 32, 32)\n",
    "\n",
    "        # Level 3\n",
    "        x = self.upconv3(x)  # (16, 64, 64)\n",
    "        x = torch.cat([x, x1], dim=1)  # Concatenate skip connection\n",
    "        x = self.conv3(x)  # (16, 64, 64)\n",
    "\n",
    "        # Final classification\n",
    "        x = self.final_conv(x)  # (1, 64, 64) - logits\n",
    "        x = self.sigmoid(x)  # (1, 64, 64) - probabilities\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88584f37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Complete UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbc415",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet for binary segmentation with skip connections\n",
    "    Input: (B, 3, H, W)\n",
    "    Output: (B, 1, H, W) - cloud probability per pixel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = UNetEncoder()\n",
    "        self.decoder = UNetDecoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode (get bottleneck and skip features)\n",
    "        bottleneck, skip_features = self.encoder(x)\n",
    "        # Decode (use skip connections)\n",
    "        output = self.decoder(bottleneck, skip_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9685b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate UNet\n",
    "unet_model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1787d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify architecture\n",
    "dummy_output = unet_model(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {dummy_output.shape}\")\n",
    "print(f\"✓ UNet produces same output shape as FCN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089ceb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "print(\"\\nUNet Architecture:\")\n",
    "torchinfo.summary(unet_model, input_size=(1, 3, 64, 64), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d664b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Training UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafeac8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer for UNet\n",
    "unet_optimizer = optim.Adam(unet_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90d3f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "unet_train_losses = []\n",
    "unet_val_losses = []\n",
    "\n",
    "print(\"Training UNet...\")\n",
    "for epoch in tqdm.trange(EPOCHS):\n",
    "    train_loss = train_epoch_segmentation(\n",
    "        unet_model, train_loader, criterion, unet_optimizer, device\n",
    "    )\n",
    "    val_loss = val_epoch_segmentation(unet_model, val_loader, criterion, device)\n",
    "\n",
    "    unet_train_losses.append(train_loss)\n",
    "    unet_val_losses.append(val_loss)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{EPOCHS}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "print(\"✓ UNet training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95bd4aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.jit\n",
    "\n",
    "# Put the model in eval mode\n",
    "unet_model = unet_model.cpu().eval()\n",
    "\n",
    "# Script the model\n",
    "scripted_model = torch.jit.script(unet_model)\n",
    "\n",
    "# Save\n",
    "scripted_model.save(\"unet_scripted_model.pt\")\n",
    "\n",
    "print(scripted_model)\n",
    "\n",
    "# Scripted model reloading (demo)\n",
    "scripted_model = torch.jit.load(\"unet_scripted_model.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a44ba6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Comparing Training Curves: FCN vs UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9084e",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(fcn_train_losses, label=\"FCN Train\", linestyle=\"--\")\n",
    "axes[0].plot(unet_train_losses, label=\"UNet Train\", linestyle=\"-\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Validation loss\n",
    "axes[1].plot(fcn_val_losses, label=\"FCN Val\", linestyle=\"--\")\n",
    "axes[1].plot(unet_val_losses, label=\"UNet Val\", linestyle=\"-\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Validation Loss\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5774b67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Side-by-Side Comparison: FCN vs UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8182f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def compare_models(fcn_model, unet_model, dataset, device, num_samples=6):\n",
    "    \"\"\"Compare FCN and UNet predictions side-by-side\"\"\"\n",
    "    fcn_model.eval()\n",
    "    unet_model.eval()\n",
    "\n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4 * num_samples))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            img_tensor, mask_gt = dataset[i]\n",
    "            img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            # FCN prediction\n",
    "            fcn_output = fcn_model(img_tensor_batch)  # (1, 1, H, W)\n",
    "            fcn_pred = fcn_output.squeeze(0).squeeze(0).cpu()  # (H, W) float\n",
    "\n",
    "            # UNet prediction\n",
    "            unet_output = unet_model(img_tensor_batch)  # (1, 1, H, W)\n",
    "            unet_pred = unet_output.squeeze(0).squeeze(0).cpu()  # (H, W) float\n",
    "\n",
    "            # Visualize\n",
    "            img_np = inverse_transform(img_tensor, mean, std)\n",
    "\n",
    "            # Create overlays\n",
    "            gt_overlay = overlay_img_msk(img_np, mask_gt.numpy(), color=\"red\")\n",
    "            fcn_overlay = overlay_img_prd(img_np, fcn_pred.numpy(), cmap=\"jet\")\n",
    "            unet_overlay = overlay_img_prd(img_np, unet_pred.numpy(), cmap=\"viridis\")\n",
    "\n",
    "            axes[i, 0].imshow(img_np)\n",
    "            axes[i, 0].set_title(\"Image\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "\n",
    "            axes[i, 1].imshow(gt_overlay)\n",
    "            axes[i, 1].set_title(\"Ground Truth (red)\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "\n",
    "            axes[i, 2].imshow(fcn_overlay)\n",
    "            axes[i, 2].set_title(\"FCN not thresholded (cmap jet)\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "\n",
    "            axes[i, 3].imshow(unet_overlay)\n",
    "            axes[i, 3].set_title(\"UNet not thresholded (cmap viridis)\")\n",
    "            axes[i, 3].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313542fc",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "compare_models(fcn_model, unet_model, val_dataset, device, num_samples=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7ebd7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Key Observation:** UNet predictions have **sharper boundaries** compared to FCN! The skip connections allow the decoder to recover fine spatial details that were lost during downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef19fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Part 6: Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218c5c86",
   "metadata": {},
   "source": [
    "### ROC CURVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befdf9c",
   "metadata": {},
   "source": [
    "First, you can transpose the code that you had for the ROC curve in the classication case to the segmentation case to determine the thresholds.\n",
    "\n",
    "It's the same, except you now get 1 sample per pixel instead of 1 sample per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d2b94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Do it HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87e6b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Segmentation Metrics\n",
    "\n",
    "Unlike classification (accuracy, precision, recall), segmentation uses:\n",
    "\n",
    "**1. Intersection over Union (IoU) / Jaccard Index:**\n",
    "```\n",
    "IoU = (TP) / (TP + FP + FN)\n",
    "    = Area of Overlap / Area of Union\n",
    "```\n",
    "- Range: [0, 1], higher is better\n",
    "- Penalizes both false positives and false negatives\n",
    "\n",
    "**2. Dice Coefficient / F1 Score:**\n",
    "```\n",
    "Dice = 2 * TP / (2 * TP + FP + FN)\n",
    "     = 2 * IoU / (1 + IoU)\n",
    "```\n",
    "- Range: [0, 1], higher is better\n",
    "- More weight on true positives than IoU\n",
    "\n",
    "**3. Pixel Accuracy:**\n",
    "```\n",
    "Pixel Acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "- Can be misleading for imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d5778",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_iou(pred, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute IoU for binary segmentation\n",
    "    Args:\n",
    "        pred: (B, H, W) predicted probabilities (floats in [0, 1])\n",
    "        target: (B, H, W) ground truth binary masks\n",
    "        threshold: threshold to binarize predictions\n",
    "    Returns:\n",
    "        iou: IoU score for cloud class (positive class)\n",
    "    \"\"\"\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    # Binarize predictions\n",
    "    pred_binary = (pred >= threshold).astype(np.uint8)\n",
    "\n",
    "    # Compute IoU for cloud class (class 1)\n",
    "    intersection = np.logical_and(pred_binary, target).sum()\n",
    "    union = np.logical_or(pred_binary, target).sum()\n",
    "\n",
    "    if union == 0:\n",
    "        return float(\"nan\")\n",
    "    else:\n",
    "        return intersection / union\n",
    "\n",
    "\n",
    "def compute_dice(pred, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute Dice coefficient for binary segmentation\n",
    "    \"\"\"\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    # Binarize predictions\n",
    "    pred_binary = (pred >= threshold).astype(np.uint8)\n",
    "\n",
    "    # Compute Dice for cloud class\n",
    "    intersection = np.logical_and(pred_binary, target).sum()\n",
    "    pred_sum = pred_binary.sum()\n",
    "    target_sum = target.sum()\n",
    "\n",
    "    if pred_sum + target_sum == 0:\n",
    "        return float(\"nan\")\n",
    "    else:\n",
    "        return 2 * intersection / (pred_sum + target_sum)\n",
    "\n",
    "\n",
    "def compute_pixel_accuracy(pred, target, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compute overall pixel accuracy\n",
    "    \"\"\"\n",
    "    pred = pred.cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    # Binarize predictions\n",
    "    pred_binary = (pred >= threshold).astype(np.uint8)\n",
    "\n",
    "    correct = (pred_binary == target).sum()\n",
    "    total = pred.size\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a1587",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Evaluating Both Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77a918",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model on entire dataset for binary segmentation\n",
    "    Returns:\n",
    "        mean_iou: IoU score for cloud class\n",
    "        mean_dice: Dice score for cloud class\n",
    "        pixel_acc: overall pixel accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_ious = []\n",
    "    all_dices = []\n",
    "    all_pixel_accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Predict\n",
    "            outputs = model(images)  # (B, 1, H, W) probabilities\n",
    "            preds = outputs.squeeze(1)  # (B, H, W) probabilities\n",
    "\n",
    "            # Compute metrics\n",
    "            iou = compute_iou(preds, masks, threshold)\n",
    "            dice = compute_dice(preds, masks, threshold)\n",
    "            pixel_acc = compute_pixel_accuracy(preds, masks, threshold)\n",
    "\n",
    "            all_ious.append(iou)\n",
    "            all_dices.append(dice)\n",
    "            all_pixel_accs.append(pixel_acc)\n",
    "\n",
    "    # Average across batches (ignore nan values)\n",
    "    mean_iou = np.nanmean(all_ious)\n",
    "    mean_dice = np.nanmean(all_dices)\n",
    "    mean_pixel_acc = np.mean(all_pixel_accs)\n",
    "\n",
    "    return mean_iou, mean_dice, mean_pixel_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873439f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate FCN\n",
    "print(\"Evaluating FCN on test set...\")\n",
    "fcn_iou, fcn_dice, fcn_pixel_acc = evaluate_model(\n",
    "    fcn_model, test_loader, device, threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"\\nFCN Results (threshold=0.5):\")\n",
    "print(f\"  IoU (Cloud class): {fcn_iou:.4f}\")\n",
    "print(f\"  Dice (Cloud class): {fcn_dice:.4f}\")\n",
    "print(f\"  Pixel Accuracy: {fcn_pixel_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4acd70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate UNet\n",
    "print(\"Evaluating UNet on test set...\")\n",
    "unet_iou, unet_dice, unet_pixel_acc = evaluate_model(\n",
    "    unet_model, test_loader, device, threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"\\nUNet Results (threshold=0.5):\")\n",
    "print(f\"  IoU (Cloud class): {unet_iou:.4f}\")\n",
    "print(f\"  Dice (Cloud class): {unet_dice:.4f}\")\n",
    "print(f\"  Pixel Accuracy: {unet_pixel_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbce265",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Metrics Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff151477",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"FCN\", \"UNet\"],\n",
    "        \"IoU (Cloud)\": [fcn_iou, unet_iou],\n",
    "        \"Dice (Cloud)\": [fcn_dice, unet_dice],\n",
    "        \"Pixel Accuracy\": [fcn_pixel_acc, unet_pixel_acc],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUANTITATIVE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "improvement_iou = ((unet_iou - fcn_iou) / fcn_iou) * 100\n",
    "improvement_dice = ((unet_dice - fcn_dice) / fcn_dice) * 100\n",
    "\n",
    "print(f\"\\nUNet improvements over FCN:\")\n",
    "print(f\"  IoU: +{improvement_iou:.1f}%\")\n",
    "print(f\"  Dice: +{improvement_dice:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d355d2e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Visualization: Best and Worst Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5a13c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_best_worst_predictions(model, dataset, device, num_samples=3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Find best and worst predictions based on IoU\n",
    "    Returns indices of best and worst samples\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    sample_ious = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            img_tensor, mask_gt = dataset[i]\n",
    "            img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "            output = model(img_tensor_batch)  # (1, 1, H, W)\n",
    "            pred_mask = output.squeeze(0).squeeze(0)  # (H, W) probabilities\n",
    "\n",
    "            iou = compute_iou(pred_mask.unsqueeze(0), mask_gt.unsqueeze(0), threshold)\n",
    "            sample_ious.append((i, iou))\n",
    "\n",
    "    # Sort by IoU\n",
    "    sample_ious.sort(key=lambda x: x[1])\n",
    "\n",
    "    worst_indices = [idx for idx, _ in sample_ious[:num_samples]]\n",
    "    best_indices = [idx for idx, _ in sample_ious[-num_samples:]]\n",
    "\n",
    "    return best_indices, worst_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ccd186",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find best/worst for UNet\n",
    "best_idx, worst_idx = find_best_worst_predictions(\n",
    "    unet_model, test_dataset, device, num_samples=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d9a61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize best cases\n",
    "print(\"Best UNet Predictions (Highest IoU):\")\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "unet_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(best_idx):\n",
    "        img_tensor, mask_gt = test_dataset[idx]\n",
    "        img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        output = unet_model(img_tensor_batch)  # (1, 1, H, W)\n",
    "        pred_mask = output.squeeze(0).squeeze(0).cpu()  # (H, W) probabilities\n",
    "\n",
    "        img_np = inverse_transform(img_tensor, mean, std)\n",
    "\n",
    "        # Create overlays\n",
    "        gt_overlay = overlay_img_msk(img_np, mask_gt.numpy(), color=\"red\")\n",
    "        pred_overlay = overlay_img_prd(img_np, pred_mask.numpy(), cmap=\"viridis\")\n",
    "\n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(\"Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(gt_overlay)\n",
    "        axes[i, 1].set_title(\"Ground Truth (red)\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(pred_overlay)\n",
    "        axes[i, 2].set_title(\"UNet Prediction (viridis)\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b68273",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize worst cases\n",
    "print(\"Worst UNet Predictions (Lowest IoU):\")\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, idx in enumerate(worst_idx):\n",
    "        img_tensor, mask_gt = test_dataset[idx]\n",
    "        img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "        output = unet_model(img_tensor_batch)  # (1, 1, H, W)\n",
    "        pred_mask = output.squeeze(0).squeeze(0).cpu()  # (H, W) probabilities\n",
    "\n",
    "        img_np = inverse_transform(img_tensor, mean, std)\n",
    "\n",
    "        # Create overlays\n",
    "        gt_overlay = overlay_img_msk(img_np, mask_gt.numpy(), color=\"red\")\n",
    "        pred_overlay = overlay_img_prd(img_np, pred_mask.numpy(), cmap=\"viridis\")\n",
    "\n",
    "        axes[i, 0].imshow(img_np)\n",
    "        axes[i, 0].set_title(\"Image\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        axes[i, 1].imshow(gt_overlay)\n",
    "        axes[i, 1].set_title(\"Ground Truth (red)\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "\n",
    "        axes[i, 2].imshow(pred_overlay)\n",
    "        axes[i, 2].set_title(\"UNet Prediction (viridis)\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afbf27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Congratulations !\n",
    "\n",
    "Now save your precious Unet and go to the next notebook to view your results !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0350e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Part 7: Advanced Topics (Optional Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c4771",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Exercise 1: Data Augmentation for Segmentation\n",
    "\n",
    "**Challenge:** Implement data augmentation that transforms **both image and mask together**.\n",
    "\n",
    "**Important:** Geometric transforms (flip, rotation, crop) must be applied identically to image and mask!\n",
    "\n",
    "**Hints:**\n",
    "- Use `torchvision.transforms.functional` for deterministic transforms\n",
    "- Apply same random parameters to both image and mask\n",
    "- Example: If you flip image horizontally, flip mask horizontally too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59de21e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Exercise 2: Alternative Loss Functions\n",
    "\n",
    "**Challenge:** Implement and compare different loss functions for segmentation.\n",
    "\n",
    "**Try:**\n",
    "1. **Dice Loss:** Directly optimize Dice coefficient\n",
    "   ```python\n",
    "   Dice Loss = 1 - Dice Coefficient\n",
    "   ```\n",
    "\n",
    "2. **Combined Loss:** Mix CrossEntropy and Dice\n",
    "   ```python\n",
    "   Loss = α * CE + (1 - α) * Dice\n",
    "   ```\n",
    "\n",
    "3. **Focal Loss:** Handle class imbalance better\n",
    "\n",
    "**Hint:** Dice loss is differentiable - you can use it as a loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8106f38b",
   "metadata": {
    "editable": true,
    "incorrectly_encoded_metadata": "{\"tags\": [\"solution\"}}",
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for binary segmentation\n",
    "    Works with model output (probabilities after Sigmoid)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: (B, 1, H, W) probabilities after sigmoid [0, 1]\n",
    "            target: (B, 1, H, W) binary ground truth masks\n",
    "        \"\"\"\n",
    "        # Flatten spatial dimensions\n",
    "        pred_flat = pred.view(-1)  # (B*H*W,)\n",
    "        target_flat = target.view(-1)  # (B*H*W,)\n",
    "\n",
    "        # Compute Dice coefficient\n",
    "        intersection = (pred_flat * target_flat).sum()\n",
    "        union = pred_flat.sum() + target_flat.sum()\n",
    "\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "\n",
    "        # Dice loss = 1 - Dice\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95de1b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Try it:**\n",
    "```python\n",
    "dice_loss = DiceLoss()\n",
    "optimizer = optim.SGD(unet_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train with Dice loss instead of CrossEntropy\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch_segmentation(unet_model, train_loader, dice_loss, optimizer, device)\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37aa278",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Exercise 3: Modify UNet Architecture\n",
    "\n",
    "**Challenge:** Experiment with architectural changes:\n",
    "\n",
    "1. **Deeper UNet:** Add more encoder-decoder levels (64×64 → 32×32 → 16×16 → 8×8 → **4×4**)\n",
    "2. **Wider UNet:** Increase channel counts (32 → 64 → 128 → **256**)\n",
    "3. **Different skip connections:**\n",
    "   - Try **addition** instead of concatenation: `x = x + skip_feat`\n",
    "   - Try **attention gates**: Weight skip connections by learned attention\n",
    "\n",
    "**Question:** How does depth vs width affect:\n",
    "- Model parameters?\n",
    "- Training time?\n",
    "- Segmentation quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389474e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Exercise 4: Transfer Learning from Classification\n",
    "\n",
    "**Challenge:** Can we reuse the encoder from Session 1's classification model?\n",
    "\n",
    "**Idea:**\n",
    "1. Train a classifier on large classification dataset (12,800 samples)\n",
    "2. Extract the encoder weights (conv layers before flatten)\n",
    "3. Initialize UNet encoder with these weights\n",
    "4. Fine-tune on small segmentation dataset (256 samples)\n",
    "\n",
    "**Hypothesis:** Pre-trained encoder might learn better features with limited segmentation data!\n",
    "\n",
    "**Hint:** You'll need to match architecture dimensions between classification CNN and UNet encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eeca47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Summary & Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85b8c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### What We Learned\n",
    "\n",
    "**1. Classification vs Segmentation:**\n",
    "- Classification: 1 label per image → (B, 1) output\n",
    "- Binary Segmentation: 1 probability per pixel → (B, 1, H, W) output (after Sigmoid)\n",
    "- Multi-class Segmentation: C probabilities per pixel → (B, C, H, W) output\n",
    "\n",
    "**2. Encoder-Decoder Architecture:**\n",
    "- Encoder: Downsample to extract semantic features (like classification)\n",
    "- Decoder: Upsample to recover spatial resolution (unique to segmentation)\n",
    "- Key operation: `ConvTranspose2d` for learnable upsampling\n",
    "- Output: Per-pixel probabilities via 1×1 Conv + Sigmoid (binary) or Softmax (multi-class)\n",
    "\n",
    "**3. Importance of Skip Connections (UNet):**\n",
    "- FCN: Only uses bottleneck features → blurry boundaries\n",
    "- UNet: Concatenates encoder features → sharp boundaries\n",
    "- Skip connections preserve spatial details lost during pooling\n",
    "- Result: Better localization and finer segmentation quality\n",
    "\n",
    "**4. Segmentation Metrics:**\n",
    "- **IoU (Intersection over Union):** Area of overlap / Area of union\n",
    "- **Dice coefficient:** 2×IoU / (1+IoU), more weight on TP\n",
    "- Pixel accuracy can be misleading for imbalanced classes\n",
    "- Threshold selection (0.5) affects all metrics - can be tuned like ROC in classification\n",
    "\n",
    "**5. When to Use Each Architecture:**\n",
    "\n",
    "| Architecture | Use Case | Trade-offs |\n",
    "|--------------|----------|------------|\n",
    "| **FCN** | Fast prototyping, coarse segmentation | Simpler, faster, but blurry |\n",
    "| **UNet** | Precise boundaries needed | More parameters, sharper results |\n",
    "| **UNet++** | Medical imaging, critical applications | Best quality, most complex |\n",
    "\n",
    "**6. Key Differences from Classification Training:**\n",
    "- Loss: `BCELoss` for binary (Sigmoid in model) or `BCEWithLogitsLoss` (Sigmoid in loss)\n",
    "- Output: (B, 1, H, W) probabilities for binary, (B, C, H, W) for multi-class\n",
    "- More memory intensive (predict every pixel!)\n",
    "- Smaller datasets OK (more labels per sample - 64×64 labels vs 1)\n",
    "- Data augmentation must transform image + mask together (geometric transforms)\n",
    "- Threshold selection important for converting probabilities to binary predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c56400",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Going Further\n",
    "\n",
    "**Advanced Architectures:**\n",
    "- **DeepLabV3+:** Atrous convolutions for multi-scale context\n",
    "- **Mask R-CNN:** Instance segmentation (distinguish individual objects)\n",
    "- **SegFormer:** Transformer-based segmentation\n",
    "- **SAM (Segment Anything):** Zero-shot segmentation with prompts\n",
    "\n",
    "**Applications:**\n",
    "- **Medical imaging:** Tumor segmentation, organ delineation\n",
    "- **Autonomous driving:** Road, pedestrian, vehicle segmentation\n",
    "- **Satellite imagery:** Land cover mapping, change detection\n",
    "- **Agriculture:** Crop health monitoring, weed detection\n",
    "\n",
    "**Resources:**\n",
    "- [UNet paper (2015)](https://arxiv.org/abs/1505.04597) - Original medical imaging paper\n",
    "- [Segmentation Models PyTorch](https://github.com/qubvel/segmentation_models.pytorch) - Pre-trained models\n",
    "- [Albumentations](https://albumentations.ai/) - Fast augmentation library for segmentation\n",
    "- [Papers With Code - Segmentation](https://paperswithcode.com/task/semantic-segmentation) - SOTA models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c8db5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Congratulations! 🎉\n",
    "\n",
    "You've successfully:\n",
    "- ✓ Understood the transition from classification to dense prediction\n",
    "- ✓ Implemented encoder-decoder architectures (FCN)\n",
    "- ✓ Built UNet with skip connections\n",
    "- ✓ Evaluated models with IoU and Dice metrics\n",
    "- ✓ Compared architectures quantitatively and qualitatively\n",
    "\n",
    "**Next steps:** Try the optional exercises above or apply these techniques to your own segmentation problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdd355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py312-isae",
   "language": "python",
   "name": "py312-isae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
