{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4867e5d3",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Florient Chouteau | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>\n",
    "\n",
    "# Session 1 : About Convolutions and CNNs ...\n",
    "\n",
    "Welcome to this BE about applying Deep Learning for Computer Vision\n",
    "\n",
    "We have 6 hours to do an hands on with something that looks like what you could be doing in three months time (but simplified)\n",
    "\n",
    "We have four notebooks to go through during those 6 hours :\n",
    "\n",
    "- One general for to get a better grasp about CNNs (it should be very quick)\n",
    "- One where we will train a small aircraft classifier on a \"simple\" dataset and plot performance curves\n",
    "- One where we will train an aircraft classifier on a more realistic dataset and plot performance curves\n",
    "- The last one we will take our previously trained model and use it for real ;)\n",
    "\n",
    "It is recommended to use Google Colab to run these notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea760ca4",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "A digital image is an image composed of picture elements, also known as pixels, each with finite, discrete quantities of numeric representation for its intensity or gray level that is an output from its two-dimensional functions fed as input by its spatial coordinates denoted with x, y on the x-axis and y-axis, respectively.\n",
    "\n",
    "We represent images as matrixes,\n",
    "\n",
    "Images are made of pixels, and pixels are made of combinations of primary colors (in our case Red, Green and Blue). In this context, images have chanels that are the grayscale image of the same size as a color image, made of just one of these primary colors. For instance, an image from a standard digital camera will have a red, green and blue channel. A grayscale image has just one channel.\n",
    "\n",
    "In remote sensing, channels are often referred to as raster bands.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*icINeO4H7UKe3NlU1fXqlA.jpeg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "For the rest of this workshop we will use the following axis conventions for images\n",
    "\n",
    "![conventions](https://storage.googleapis.com/fchouteau-isae-deep-learning/static/image_coordinates.png)\n",
    "\n",
    "The reference library in python for working with images is https://scikit-image.org/\n",
    "\n",
    "We will just do basic image manipulation, but you [can look at all the examples](https://scikit-image.org/docs/stable/auto_examples/) if you need to get a better grasp of image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a219a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.data\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc42660",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = skimage.data.astronaut()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eef72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the height, width and number of channels of this image ?\n",
    "# In which order is the data represented ? Which dimensions are channels in ?\n",
    "# What is the image \"dtype\" ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the center 128 x 128 pixels on all three bands and plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca50b26",
   "metadata": {},
   "source": [
    "In classical image representation, we use the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model) where the image is represented by three R,G,B channels (in that order).\n",
    "\n",
    "Usually we also use 8bits color depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the difference between the green and the red band\n",
    "# don't forget to convert the image type as the image are in unsigned type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ff296",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c863bb",
   "metadata": {},
   "source": [
    "You've seen this image in the previous class :\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/FjvuN.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "This is a convolution operator.\n",
    "\n",
    "Someone may have told you that CNNs were the \"thing\" that made deep learning for image processing possible. But what are convolutions ?\n",
    "\n",
    "First, remember that you [learnt about convolutions a long time ago üò±](https://fr.wikipedia.org/wiki/Produit_de_convolution)\n",
    "\n",
    "<img src=\"https://betterexplained.com/ColorizedMath/content/img/Convolution.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "So basically, we slide a filter over the signal. In 2D, this means\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "One thing you can notice is that if we slide a filter over an image we \"lose\" pixels at the border. This is actually quite easy to compute : assuming a of size `2*k +1` we loose `k` pixels on each side of the image.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif)\n",
    "\n",
    "If you want to get them back you have to \"pad\" (add values at the border, for examples zeroes) the image\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides.gif)\n",
    "\n",
    "For more information, this website is excellent : https://cs231n.github.io/convolutional-networks/#conv\n",
    "\n",
    "Let's play with convolutions a little bit before actually doing CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0a7ca",
   "metadata": {},
   "source": [
    "### 2D Convolution without \"depth\"\n",
    "\n",
    "First, let's look at basic filtering over grayscale (1 channel) images. We will slide a filter over H,W spatial dimensions and get the result\n",
    "\n",
    "First, the convolution implementation without depth is quite simple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fab7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(img: np.array, kernel: np.array) -> np.array:\n",
    "    k = kernel.shape[0]\n",
    "    h, w = img.shape[:2]\n",
    "    p = int(k // 2)\n",
    "\n",
    "    # 2D array of zeros\n",
    "    kernel = kernel.astype(np.float32)\n",
    "    img = img.astype(np.float32)\n",
    "    convolved_img = np.zeros(shape=(h - 2 * p, w - 2 * p)).astype(np.float32)\n",
    "\n",
    "    # Iterate over the rows\n",
    "    for i in range(h - 2 * p):\n",
    "        # Iterate over the columns\n",
    "        for j in range(w - 2 * p):\n",
    "            # img[i, j] = individual pixel value\n",
    "            # Get the current matrix\n",
    "            mat = img[i : i + k, j : j + k]\n",
    "\n",
    "            # Apply the convolution - element-wise multiplication and summation of the result\n",
    "            # Store the result to i-th row and j-th column of our convolved_img array\n",
    "            convolved_img[i, j] = np.sum(np.multiply(mat, kernel))\n",
    "\n",
    "    convolved_img = convolved_img.clip(0.0, 255.0).astype(np.uint8)\n",
    "\n",
    "    return convolved_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd496c7",
   "metadata": {},
   "source": [
    "What happens if I use this filter as input ?\n",
    "\n",
    "![identity](https://wikimedia.org/api/rest_v1/media/math/render/svg/1fbc763a0af339e3a3ff20af60a8a993c53086a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [[0, 0, 0], [0, 1, 0], [0, 0, 0]]\n",
    "k = np.asarray(k)\n",
    "\n",
    "k.shape\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38da39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = skimage.data.cat()\n",
    "img = img[:, :, 0]\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved_img = convolve(img, k)\n",
    "\n",
    "print(convolved_img.shape)\n",
    "\n",
    "plt.imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Note the loss of 1 pixel... If we wanted to alleviate it we could do something like\n",
    "\n",
    "img = np.pad(img, ((1, 1), (1, 1)))\n",
    "print(f\"before {img.shape}\")\n",
    "convolved_img = convolve(img, k)\n",
    "\n",
    "print(f\"after {convolved_img.shape}\")\n",
    "\n",
    "plt.imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc746131",
   "metadata": {},
   "source": [
    "Too easy ! Let's try another filter\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/91256bfeece3344f8602e288d445e6422c8b8a1c)\n",
    "\n",
    "What does it do ? Take a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.asarray([[1, 1, 1], [1, 1, 1], [1, 1, 1]]).astype(np.float32)\n",
    "k = k / k.sum()\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c327ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve the cat image with this filter and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(convolved_img.shape)\n",
    "\n",
    "plt.imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(img[64:128, 64:128], cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(convolved_img[64:129, 64:128], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72118e06",
   "metadata": {},
   "source": [
    "If we wanted, we could learn the filters in order to do... cat classification !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613b5a3",
   "metadata": {},
   "source": [
    "### Convolutions with depth\n",
    "\n",
    "Let's get back to our GIF\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/FjvuN.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "It's the same as above, except our filter takes all channels of the image as input. So basically a \"Convolution\" layer is a filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9841465",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In classical image processing, we use the (height, width, channels) convention, however in torch we prefer using (channels, height, width) convention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107130e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = skimage.data.astronaut()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To transpose an image, we use\n",
    "img = img.transpose((2, 0, 1))  # change channel order\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d31c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random((1, 3, 3, 3))\n",
    "b = np.random.random((3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should remember this\n",
    "\n",
    "\n",
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = (\n",
    "                    np.sum(\n",
    "                        np.multiply(\n",
    "                            data[:, x : (x + conv_width), y : (y + conv_height)], conv_W[output_channel, :, :, :]\n",
    "                        )\n",
    "                    )\n",
    "                    + conv_b[output_channel]\n",
    "                )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "output = forward_convolution(w, b, img)\n",
    "print(\"Input\", img.shape)\n",
    "print(\"Filter:\\n\", w, w.shape)\n",
    "print(\"Bias:\", b, b.shape)\n",
    "print(\"Input\", output.shape)\n",
    "\n",
    "# Don't forget that matplotlib uses (h,w,c) to plot images !\n",
    "\n",
    "plt.imshow(img.transpose((1, 2, 0)))\n",
    "plt.show()\n",
    "plt.imshow(output.transpose((1, 2, 0)[:,:,0]), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae7ae8",
   "metadata": {},
   "source": [
    "Some useful resources for more information :\n",
    "\n",
    "- The DL class https://github.com/fchouteau/deep-learning/blob/main/deep/Deep%20Learning.ipynb\n",
    "- https://github.com/vdumoulin/conv_arithmetic\n",
    "- https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7d8cf4",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "I shamelessly copy pasted code from this excellent class : https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb\n",
    "\n",
    "Remember, an Artificial Neural Network is a stack of \n",
    "\n",
    "    \"Fully Connected\" layers\n",
    "    Non linearities\n",
    "\n",
    "A Convolutional Neural Network is a stack of\n",
    "- Convolutional Layers aka Filter Banks\n",
    "    - Increase dimensionality\n",
    "    - Projection on overcomplete basis\n",
    "    - Edge detections\n",
    "- Non-linearities\n",
    "    - Sparsification\n",
    "    - Typically Rectified Linear Unit (ReLU): ReLU(x)=max‚Å°(x,0)\\text{ReLU}(x) = \\max(x, 0)ReLU(x)=max(x,0)\n",
    "- Pooling\n",
    "    - Aggregating over a feature map\n",
    "    - Example : Maximum\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/Dgy6hBvOvAWofkrDM8BclOU3E3C2hqb25qBb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da23b8",
   "metadata": {},
   "source": [
    "Why do CNNs works ?\n",
    "\n",
    "To perform well, we need to incorporate some prior knowledge about the problem\n",
    "\n",
    "    Assumptions helps us when they are true\n",
    "    They hurt us when they are not\n",
    "    We want to make just the right amount of assumptions, not more than that\n",
    "    \n",
    "In Deep Learning\n",
    "\n",
    "    Many layers: compositionality\n",
    "    Convolutions: locality + stationarity of images\n",
    "    Pooling: Invariance of object class to translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a66fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a38a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28  # images are 28x28 pixels\n",
    "output_size = 10  # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "    ),\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np = 0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "\n",
    "\n",
    "# Create two models: One ANN vs One CNN\n",
    "\n",
    "\n",
    "class FullyConnected2Layers(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FullyConnected2Layers, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, output_size),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature * 4 * 4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature * 4 * 4)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5194455",
   "metadata": {},
   "source": [
    "Switching between CPU and GPU in PyTorch is controlled via a device string, which will seemlessly determine whether GPU is available, falling back to CPU if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "\n",
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction=\"sum\").item()  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a764c92",
   "metadata": {},
   "source": [
    "### CNNs vs Fully Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793d805",
   "metadata": {},
   "source": [
    "A small FullyConnected ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c3e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8  # number of hidden units\n",
    "\n",
    "model_fnn = FullyConnected2Layers(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65bd60d",
   "metadata": {},
   "source": [
    "A CNN with the same number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702781b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "n_features = 6  # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a700f94",
   "metadata": {},
   "source": [
    "The ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
    "\n",
    "    Use of convolution: Locality and stationarity in images\n",
    "    Pooling: builds in some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68bb656",
   "metadata": {},
   "source": [
    "### What happens when CNNs assumptions are not true ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28 * 28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy())\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43197827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "n_features = 6  # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn, perm)\n",
    "    test(model_cnn, perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8  # number of hidden units\n",
    "\n",
    "model_fnn = FullyConnected2Layers(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn, perm)\n",
    "    test(model_fnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c44bad",
   "metadata": {},
   "source": [
    "\n",
    "The ConvNet's performance drops when we permute the pixels, but the Fully-Connected Network's performance stays the same\n",
    "\n",
    "    ConvNet makes the assumption that pixels lie on a grid and are stationary/local\n",
    "    It loses performance when this assumption is wrong\n",
    "    The fully-connected network does not make this assumption\n",
    "    It does less well when it is true, since it doesn't take advantage of this prior knowledge\n",
    "    But it doesn't suffer when the assumption is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893fb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar((\"NN normal\", \"CNN normal\", \"CNN scrambled\", \"NN scrambled\"), accuracy_list, width=0.4)\n",
    "plt.ylim((min(accuracy_list) - 5, 96))\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "for tick in plt.gca().xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(10)\n",
    "plt.title(\"Performance comparison\");"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py38-isae",
   "language": "python",
   "name": "py38-isae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
