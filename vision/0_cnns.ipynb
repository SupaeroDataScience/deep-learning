{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00111848",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "# Deep Learning for Computer Vision\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Florient Chouteau | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>\n",
    "\n",
    "## Session 1 : About Convolutions and CNNs ...\n",
    "\n",
    "Welcome to this BE about applying Deep Learning for Computer Vision\n",
    "\n",
    "We have 6 hours to do an hands on with something that looks like what you could be doing in three months time (but simplified)\n",
    "\n",
    "We have four notebooks to go through during those 6 hours :\n",
    "\n",
    "- One general for to get a better grasp about CNNs (it should be very quick)\n",
    "- One where we will train a small aircraft classifier on a \"simple\" dataset and plot performance curves\n",
    "- One where we will train an aircraft classifier on a more realistic dataset and plot performance curves\n",
    "- The last one we will take our previously trained model and use it for real ;)\n",
    "\n",
    "It is recommended to use Google Colab to run these notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847672d",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "A digital image is an image composed of picture elements, also known as pixels, each with finite, discrete quantities of numeric representation for its intensity or gray level that is an output from its two-dimensional functions fed as input by its spatial coordinates denoted with x, y on the x-axis and y-axis, respectively.\n",
    "\n",
    "We represent images as matrixes,\n",
    "\n",
    "Images are made of pixels, and pixels are made of combinations of primary colors (in our case Red, Green and Blue). In this context, images have chanels that are the grayscale image of the same size as a color image, made of just one of these primary colors. For instance, an image from a standard digital camera will have a red, green and blue channel. A grayscale image has just one channel.\n",
    "\n",
    "In remote sensing, channels are often referred to as raster bands.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*icINeO4H7UKe3NlU1fXqlA.jpeg\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "For the rest of this workshop we will use the following axis conventions for images\n",
    "\n",
    "![conventions](https://storage.googleapis.com/fchouteau-isae-deep-learning/static/image_coordinates.png)\n",
    "\n",
    "The reference library in python for working with images is https://scikit-image.org/\n",
    "\n",
    "We will just do basic image manipulation, but you [can look at all the examples](https://scikit-image.org/docs/stable/auto_examples/) if you need to get a better grasp of image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3284838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.data\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70921e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load a standard test image (astronaut) to explore image properties and pixel manipulation\n",
    "img = skimage.data.astronaut()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ca167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the height, width and number of channels of this image ?\n",
    "# In which order is the data represented ? Which dimensions are channels in ?\n",
    "# What is the image \"dtype\" ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the center 128 x 128 pixels on all three bands and plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbda2ea",
   "metadata": {},
   "source": [
    "In classical image representation, we use the [RGB color model](https://en.wikipedia.org/wiki/RGB_color_model) where the image is represented by three R,G,B channels (in that order).\n",
    "\n",
    "Usually we also use 8bits color depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dcdf51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot the difference between the green and the red band\n",
    "# don't forget to convert the image type as the image are in unsigned type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67e588",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136c72b",
   "metadata": {},
   "source": [
    "You've seen this image in the previous class :\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SupaeroDataScience/deep-learning/refs/heads/main/vision/FjvuN.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "This is a convolution operator.\n",
    "\n",
    "Someone may have told you that CNNs were the \"thing\" that made deep learning for image processing possible. But what are convolutions ?\n",
    "\n",
    "First, remember that you [learnt about convolutions a long time ago üò±](https://fr.wikipedia.org/wiki/Produit_de_convolution)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "So basically, we slide a filter over the signal. In 2D, this means\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "One thing you can notice is that if we slide a filter over an image we \"lose\" pixels at the border. This is actually quite easy to compute : assuming a of size `2*k +1` we loose `k` pixels on each side of the image.\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif)\n",
    "\n",
    "If you want to get them back you have to \"pad\" (add values at the border, for examples zeroes) the image\n",
    "\n",
    "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides.gif)\n",
    "\n",
    "For more information, this website is excellent : https://cs231n.github.io/convolutional-networks/#conv\n",
    "\n",
    "Let's play with convolutions a little bit before actually doing CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05307fc6",
   "metadata": {},
   "source": [
    "### 2D Convolution without \"depth\"\n",
    "\n",
    "First, let's look at basic filtering over grayscale (1 channel) images. We will slide a filter over H,W spatial dimensions and get the result\n",
    "\n",
    "First, the convolution implementation without depth is quite simple :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b087c22",
   "metadata": {},
   "source": [
    "#### Understanding the Convolution Algorithm\n",
    "\n",
    "The function below implements a basic 2D convolution from scratch. Here's what it does:\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Initialize** an output array with reduced dimensions (we lose `k//2` pixels on each side)\n",
    "2. **Slide** the kernel across the image pixel by pixel\n",
    "3. **Element-wise multiply** the kernel with the corresponding image patch\n",
    "4. **Sum** all the multiplied values to get a single output pixel\n",
    "5. **Clip** values to valid pixel range [0, 255]\n",
    "\n",
    "**Important Notes:**\n",
    "- The kernel size `k` is usually odd (e.g., 3√ó3, 5√ó5) so it has a clear center pixel\n",
    "- Without padding, the output is smaller than the input by `2*p` pixels (where `p = k//2`)\n",
    "- This is a \"valid\" convolution - we only compute where the kernel fully overlaps the image\n",
    "- The nested loops make this slow (real implementations use optimized C/CUDA code)\n",
    "\n",
    "**Practical Tip:** In production, always use library implementations (OpenCV, scipy, PyTorch) which are 100-1000x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a8f4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def convolve(img: np.array, kernel: np.array) -> np.array:\n",
    "    k = kernel.shape[0]\n",
    "    h, w = img.shape[:2]\n",
    "    p = int(k // 2)\n",
    "\n",
    "    # 2D array of zeros\n",
    "    kernel = kernel.astype(np.float32)\n",
    "    img = img.astype(np.float32)\n",
    "    convolved_img = np.zeros(shape=(h - 2 * p, w - 2 * p)).astype(np.float32)\n",
    "\n",
    "    # Iterate over the rows\n",
    "    for i in range(h - 2 * p):\n",
    "        # Iterate over the columns\n",
    "        for j in range(w - 2 * p):\n",
    "            # img[i, j] = individual pixel value\n",
    "            # Get the current matrix\n",
    "            mat = img[i : i + k, j : j + k]\n",
    "\n",
    "            # Apply the convolution - element-wise multiplication and summation of the result\n",
    "            # Store the result to i-th row and j-th column of our convolved_img array\n",
    "            convolved_img[i, j] = np.sum(np.multiply(mat, kernel))\n",
    "\n",
    "    convolved_img = convolved_img.clip(0.0, 255.0).astype(np.uint8)\n",
    "\n",
    "    return convolved_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc550b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "What happens if I use this filter as input ?\n",
    "\n",
    "![identity](https://wikimedia.org/api/rest_v1/media/math/render/svg/1fbc763a0af339e3a3ff20af60a8a993c53086a7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe04605b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "k = [[0, 0, 0], [0, 1, 0], [0, 0, 0]]\n",
    "k = np.asarray(k)\n",
    "\n",
    "k.shape\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84d9c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "img = skimage.data.cat()\n",
    "img = img[:, :, 0]\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde55335",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "convolved_img = convolve(img, k)\n",
    "\n",
    "print(convolved_img.shape)\n",
    "\n",
    "plt.imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "# Note the loss of 1 pixel... If we wanted to alleviate it we could do something like\n",
    "\n",
    "img = np.pad(img, ((1, 1), (1, 1)))\n",
    "print(f\"before {img.shape}\")\n",
    "convolved_img = convolve(img, k)\n",
    "\n",
    "print(f\"after {convolved_img.shape}\")\n",
    "\n",
    "plt.imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f27ff",
   "metadata": {},
   "source": [
    "#### What Just Happened? The Identity Filter Explained\n",
    "\n",
    "The identity filter (center value = 1, all others = 0) **does nothing** - it returns the original image unchanged!\n",
    "\n",
    "**Why is this useful?**\n",
    "- It's a sanity check that our convolution implementation works correctly\n",
    "- It demonstrates that the output pixel is simply a weighted sum of its neighbors\n",
    "- It shows how padding helps preserve image dimensions: without padding we lost 1 pixel per side\n",
    "\n",
    "**The Padding Trade-off:**\n",
    "- **Without padding:** Output is smaller (300√ó300 ‚Üí 298√ó298 for a 3√ó3 kernel)\n",
    "- **With padding:** Output matches input size, but border pixels are less reliable (they include padded zeros)\n",
    "- In practice, most deep learning frameworks offer \"valid\" (no padding) and \"same\" (with padding) options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba6638",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Too easy ! Let's try another filter\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/91256bfeece3344f8602e288d445e6422c8b8a1c)\n",
    "\n",
    "What does it do ? Take a guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8144d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "k = np.asarray([[1, 1, 1], [1, 1, 1], [1, 1, 1]]).astype(np.float32)\n",
    "k = k / k.sum()\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1c65d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convolve the cat image with this filter and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7aa9df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "print(convolved_img.shape)\n",
    "\n",
    "plt.imshow(convolved_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(img[64:128, 64:128], cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(convolved_img[64:129, 64:128], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2b931",
   "metadata": {},
   "source": [
    "#### The Box Blur Filter - Averaging for Smoothing\n",
    "\n",
    "This filter contains all 1s (normalized by dividing by 9), so each output pixel is the **average of its 3√ó3 neighborhood**.\n",
    "\n",
    "**Effect:** Smoothing / Blurring\n",
    "- High-frequency details (edges, noise, texture) are reduced\n",
    "- The image becomes \"softer\" - compare the zoomed regions above!\n",
    "- Each pixel is replaced by the average of itself and its 8 neighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affe5060",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "If we wanted, we could learn the filters in order to do... cat classification !\n",
    "\n",
    "There are many more filters that have been designed to do interesting things, you can find an interesting list here : https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2141203%2F99dba888571cd6284b9b59903061aaa4%2Fko001.png?generation=1591783791920610&alt=media)\n",
    "\n",
    "**Takeaway message** : Kernel filtering (convolution) takes its root from classical image processing !\n",
    "\n",
    "**Why are convolutions useful in CNNs?**\n",
    "- **Not directly!** CNNs *learn* their filter values through backpropagation\n",
    "- But understanding hand-crafted filters helps build intuition\n",
    "- Early CNN layers often learn edge detectors (similar to Sobel filters) automatically\n",
    "- Later layers learn more complex patterns built from these simple features\n",
    "\n",
    "**Classical Computer Vision Filters:**\n",
    "- **Blur:** Averaging (shown here), Gaussian blur (weighted averaging)\n",
    "- **Edge Detection:** Sobel, Prewitt, Laplacian filters\n",
    "- **Sharpening:** Emphasizes high-frequency content (opposite of blur)\n",
    "\n",
    "The key insight: CNNs replace manually designed filters with learned ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de1fcaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Convolutions with depth\n",
    "\n",
    "Let's get back to our GIF\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SupaeroDataScience/deep-learning/refs/heads/main/vision/FjvuN.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "It's the same as above, except our filter takes all channels of the image as input. So basically a \"Convolution\" layer is a filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9625af",
   "metadata": {},
   "source": [
    "#### Why Do We Need Convolutions with Depth?\n",
    "\n",
    "Real images have **multiple channels** (RGB color images have 3, satellite imagery can have 10+):\n",
    "- **Single-channel convolution** (what we did above) only works on grayscale images\n",
    "- **Multi-channel convolution** processes all channels simultaneously:\n",
    "  - The filter has depth matching the input channels (e.g., 3 for RGB)\n",
    "  - We compute element-wise multiplication across ALL channels\n",
    "  - Then sum everything to produce a SINGLE output pixel\n",
    "\n",
    "**Key Insight:** A 3√ó3√ó3 filter on an RGB image:\n",
    "- Has 27 values (3√ó3 spatial √ó 3 channels)\n",
    "- Produces 1 output value per position\n",
    "- Captures relationships between color channels (e.g., \"red AND green but NOT blue\" = yellow detection)\n",
    "\n",
    "**Multiple Output Channels:** CNNs use multiple filters to detect different features:\n",
    "- 64 filters ‚Üí 64 output channels (feature maps)\n",
    "- Each filter learns to detect a different pattern\n",
    "- This is how CNNs build hierarchical representations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4fb57",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Important: Tensor Shape Conventions**\n",
    "\n",
    "In classical image processing, we use the **(height, width, channels)** convention (e.g., 512√ó512√ó3)\n",
    "\n",
    "However, **PyTorch and most deep learning frameworks use (channels, height, width)** (e.g., 3√ó512√ó512)\n",
    "\n",
    "**Why the difference?**\n",
    "- **Computational efficiency:** Memory layout where channels are contiguous is faster for GPU operations\n",
    "- **Consistency:** Batch processing uses (batch, channels, height, width) - keeps related dimensions together\n",
    "- **Convention:** Most DL papers and frameworks adopted channels-first\n",
    "\n",
    "**Practical Impact:** You must transpose images when moving between libraries:\n",
    "- `numpy/scikit-image ‚Üí PyTorch`: use `.transpose((2, 0, 1))`  # (H,W,C) ‚Üí (C,H,W)\n",
    "- `PyTorch ‚Üí matplotlib`: use `.transpose((1, 2, 0))`  # (C,H,W) ‚Üí (H,W,C) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205d03d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "img = skimage.data.astronaut()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1195f3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# To transpose an image, we use\n",
    "img = img.transpose((2, 0, 1))  # change channel order\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b8ba7",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 1,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "w = np.random.random((1, 3, 3, 3))\n",
    "b = np.random.random((3,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb50cfa",
   "metadata": {},
   "source": [
    "#### Understanding Multi-Channel Convolution Implementation\n",
    "\n",
    "The function below implements a **full convolutional layer** with multiple input/output channels.\n",
    "\n",
    "**Function Signature Breakdown:**\n",
    "- `conv_W`: Filter weights with shape **(output_channels, input_channels, kernel_height, kernel_width)**\n",
    "  - Example: (64, 3, 3, 3) means 64 filters, each looking at 3 input channels with 3√ó3 spatial size\n",
    "- `conv_b`: Bias terms with shape **(output_channels,)** - one bias per output channel\n",
    "- `data`: Input image with shape **(input_channels, height, width)** - channels-first format!\n",
    "\n",
    "**Algorithm (Triple Nested Loop):**\n",
    "1. **Outer loop (output_channel):** For each filter (output feature map)\n",
    "2. **Middle loops (x, y):** For each spatial position in the output\n",
    "3. **Implicit inner operation:** Extract patch, multiply with filter, sum ALL values\n",
    "\n",
    "**The Math:** For each output pixel at position (x, y) in channel k:\n",
    "```\n",
    "output[k, x, y] = sum over all (i,j,c): data[c, x+i, y+j] * conv_W[k, c, i, j] + conv_b[k]\n",
    "```\n",
    "Where c iterates over input channels, and (i,j) iterates over the kernel spatial dimensions.\n",
    "\n",
    "**Computational Cost:** For a 3√ó3 kernel, RGB input (3 channels), 64 output channels, 224√ó224 image:\n",
    "- ‚âà 64 √ó 222 √ó 222 √ó 3 √ó 3 √ó 3 ‚âà 850 million multiply-add operations per image!\n",
    "- This is why we need GPUs for deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10089fc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is the general implementation of tensorial convolutions with channels as inputs\n",
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    output_channels is the number of filters in the convolution\n",
    "\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros(\n",
    "        (conv_channels, input_width - conv_width + 1, input_height - conv_height + 1)\n",
    "    )\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = (\n",
    "                    np.sum(\n",
    "                        np.multiply(\n",
    "                            data[:, x : (x + conv_width), y : (y + conv_height)],\n",
    "                            conv_W[output_channel, :, :, :],\n",
    "                        )\n",
    "                    )\n",
    "                    + conv_b[output_channel]\n",
    "                )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e9b5d",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convolve the input with the weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026c42b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Input\", img.shape)\n",
    "print(f\"Filter:\\n {w} \\n {w.shape}\")\n",
    "print(\"Bias:\", b, b.shape)\n",
    "print(\"Input\", output.shape)\n",
    "\n",
    "# Don't forget that matplotlib uses (h,w,c) to plot images !\n",
    "plt.imshow(img.transpose((1, 2, 0)))\n",
    "plt.show()\n",
    "plt.imshow(output.transpose((1, 2, 0))[:, :, 0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec014715",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Some useful resources for more information :\n",
    "\n",
    "- The DL class https://github.com/fchouteau/deep-learning/blob/main/deep/Deep%20Learning.ipynb\n",
    "- https://github.com/vdumoulin/conv_arithmetic\n",
    "- https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a5b36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "## Inductive Biases and Convolutional Neural Networks\n",
    "\n",
    "I shamelessly copy pasted code from this excellent class : https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb\n",
    "\n",
    "Here the objective is to get a grasp of the effect of inductive biases (convolution) that respect the structure of the data.\n",
    "\n",
    "We will go deeper into CNNs, pytorch etc. in the next notebook.\n",
    "\n",
    "Remember, an Artificial Neural Network is a stack of \n",
    "\n",
    "- \"Fully Connected\" layers\n",
    "- Non linearities\n",
    "\n",
    "A Convolutional Neural Network is a stack of\n",
    "- Convolutional Layers aka Filter Banks\n",
    "    - Increase dimensionality\n",
    "    - Projection on overcomplete basis\n",
    "    - Edge detections\n",
    "- Non-linearities\n",
    "    - Sparsification\n",
    "    - Typically Rectified Linear Unit (ReLU): ReLU(x)=max‚Å°(x,0)\\text{ReLU}(x) = \\max(x, 0)ReLU(x)=max(x,0)\n",
    "- Pooling\n",
    "    - Aggregating over a feature map\n",
    "    - Example : Maximum\n",
    "\n",
    "ANN architecture\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/Dgy6hBvOvAWofkrDM8BclOU3E3C2hqb25qBb)\n",
    "\n",
    "Max pooling operations\n",
    "\n",
    "<img src=\"https://nico-curti.github.io/NumPyNet/NumPyNet/images/maxpool.gif\" alt=\"drawing\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05eeb3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Why do CNNs works ?\n",
    "\n",
    "To perform well, we need to incorporate some prior knowledge about the problem\n",
    "\n",
    "    Assumptions helps us when they are true\n",
    "    They hurt us when they are not\n",
    "    We want to make just the right amount of assumptions, not more than that\n",
    "    \n",
    "In Deep Learning\n",
    "\n",
    "    Many layers: compositionality\n",
    "    Convolutions: locality + stationarity of images\n",
    "    Pooling: Invariance of object class to translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a77046",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f2a97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701781e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "input_size = 28 * 28  # images are 28x28 pixels\n",
    "output_size = 10  # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        \"../data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d678f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7233371e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np = 0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36db48e",
   "metadata": {},
   "source": [
    "Here is a Deep Neural network that you've seen last time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cbf72",
   "metadata": {},
   "source": [
    "#### Fully Connected Neural Network Architecture\n",
    "\n",
    "This is a traditional **dense neural network** where every neuron connects to every neuron in the next layer.\n",
    "\n",
    "**Architecture:**\n",
    "- **Input:** 784 values (28√ó28 flattened image) - spatial structure is **destroyed**!\n",
    "- **Layer 1:** Linear(784 ‚Üí n_hidden) + ReLU\n",
    "- **Layer 2:** Linear(n_hidden ‚Üí n_hidden) + ReLU\n",
    "- **Layer 3:** Linear(n_hidden ‚Üí 10) + LogSoftmax (for 10 classes)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **No spatial awareness:** Pixel at position (0,0) and (27,27) are treated identically\n",
    "- **Ignores image structure:** Would work equally well on scrambled pixels (we'll test this!)\n",
    "- **Fully general:** Makes no assumptions about the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec224845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two models: One ANN vs One CNN\n",
    "class FullyConnected2Layers(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FullyConnected2Layers, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, output_size),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f673b",
   "metadata": {},
   "source": [
    "Here is the equivalent with convolutions operations. We will go deeper on how to write and compute a CNN architecture in the next notebook, so take it for granted for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916ef24",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network Architecture - Layer by Layer\n",
    "\n",
    "This CNN **preserves spatial structure** and exploits local patterns through convolutions.\n",
    "\n",
    "**Architecture Breakdown** (for n_feature=6, input 28√ó28 grayscale):\n",
    "\n",
    "1. **Conv2d(1‚Üí6, kernel=5√ó5)**: Applies 6 different 5√ó5 filters\n",
    "   - Input: (1, 28, 28) ‚Üí Output: (6, 24, 24) - loses 4 pixels without padding\n",
    "   - Learns 6 different feature detectors (edges, corners, textures, etc.)\n",
    "   - Parameters: 6 √ó (5√ó5√ó1 + 1 bias) = 156\n",
    "\n",
    "2. **ReLU**: Element-wise activation: max(0, x) - introduces non-linearity\n",
    "\n",
    "3. **MaxPool2d(2)**: Takes max in each 2√ó2 window, stride 2\n",
    "   - Output: (6, 12, 12) - halves spatial dimensions\n",
    "   - Provides translation invariance and reduces computation\n",
    "\n",
    "4. **Conv2d(6‚Üí6, kernel=5√ó5)**: Second convolutional layer\n",
    "   - Output: (6, 8, 8)\n",
    "   - Parameters: 6 √ó (5√ó5√ó6 + 1)\n",
    "\n",
    "5. **ReLU + MaxPool2d(2)**: Same as before ‚Üí Output: (6, 4, 4)\n",
    "\n",
    "6. **Flatten**: Reshapes (6, 4, 4) ‚Üí (96,) - converts to 1D for fully connected layers\n",
    "\n",
    "7. **Linear(96‚Üí50) + ReLU**: Fully connected layer combining spatial features\n",
    "   - Parameters: 96√ó50 + 50\n",
    "\n",
    "8. **Linear(50‚Üí10) + LogSoftmax**: Output layer for 10 classes\n",
    "   - Parameters: 50√ó10 + 10\n",
    "\n",
    "**Key Differences from FC Network:**\n",
    "- **Sparse connectivity:** Each neuron only looks at a local region (5√ó5 patch)\n",
    "- **Parameter sharing:** Same filter is applied across the entire image\n",
    "- **Translation equivariance:** If object moves, feature map shifts accordingly\n",
    "- **Hierarchical features:** Early layers detect edges, later layers detect complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc480b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_feature * 4 * 4, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406df74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Switching between CPU and GPU in PyTorch is controlled via a device string, which will seemlessly determine whether GPU is available, falling back to CPU if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7981d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apple Silicon Support\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "# NVIDIA GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "# Fallback to CPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66bbe0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# store training history\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74108c77",
   "metadata": {},
   "source": [
    "Here is a pytorch training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f27f2",
   "metadata": {},
   "source": [
    "#### Understanding the PyTorch Training Loop\n",
    "\n",
    "The `train()` function implements one epoch of training. Here's the standard deep learning training cycle:\n",
    "\n",
    "**Training Loop Components:**\n",
    "\n",
    "1. **`model.train()`**: Sets model to training mode\n",
    "   - Enables dropout (if present), batch normalization updates, etc.\n",
    "   - Important: Different behavior than `model.eval()`\n",
    "\n",
    "2. **Batch Processing Loop**: Iterate over mini-batches from DataLoader\n",
    "   - `enumerate(train_loader)` gives us batches of (data, target)\n",
    "   - Batch size = 64 images at a time (memory efficient)\n",
    "\n",
    "3. **Move to Device**: `data.to(device)`, `target.to(device)`\n",
    "   - Transfers tensors to GPU/MPS/CPU as configured\n",
    "   - Critical for GPU acceleration!\n",
    "\n",
    "4. **The Optimization Cycle** (repeated for each batch):\n",
    "\n",
    "   a. **`optimizer.zero_grad()`**: Clear previous gradients\n",
    "      - PyTorch accumulates gradients by default - must reset!\n",
    "      - Forgetting this causes incorrect updates\n",
    "\n",
    "   b. **Forward Pass**: `output = model(data)`\n",
    "      - Run input through network\n",
    "      - Compute predictions\n",
    "\n",
    "   c. **Loss Computation**: `loss = F.nll_loss(output, target)`\n",
    "      - Negative log-likelihood loss for classification\n",
    "      - Measures how wrong the predictions are\n",
    "\n",
    "   d. **`loss.backward()`**: Compute gradients via backpropagation\n",
    "      - Automatic differentiation! PyTorch's magic\n",
    "      - Computes ‚àÇloss/‚àÇweight for every parameter\n",
    "\n",
    "   e. **`optimizer.step()`**: Update weights using gradients\n",
    "      - SGD update: weight = weight - learning_rate √ó gradient\n",
    "      - With momentum: smooths updates over time\n",
    "\n",
    "**The `perm` Parameter**: Used for pixel permutation experiment (explained later)\n",
    "- Default: `torch.arange(0, 784)` keeps pixels in order\n",
    "- Experiment: Random permutation to test CNN assumptions\n",
    "\n",
    "**Practical Tips:**\n",
    "- Always check loss is decreasing during training\n",
    "- Print progress every N batches to monitor training\n",
    "- If loss is NaN: learning rate too high or numerical instability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a6d00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(\n",
    "            output, target, reduction=\"sum\"\n",
    "        ).item()  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[\n",
    "            1\n",
    "        ]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    accuracy_list.append(accuracy)\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7fdddf",
   "metadata": {},
   "source": [
    "#### Understanding the Test/Evaluation Function\n",
    "\n",
    "The `test()` function evaluates model performance on unseen data without updating weights.\n",
    "\n",
    "**Key Differences from Training:**\n",
    "\n",
    "1. **`model.eval()`**: Sets model to evaluation mode\n",
    "   - Disables dropout (use all neurons)\n",
    "   - Uses running statistics for batch normalization\n",
    "   - No gradient computation needed (saves memory!)\n",
    "\n",
    "2. **No Gradient Updates**: We only need predictions, not backpropagation\n",
    "   - No `optimizer.zero_grad()`, `loss.backward()`, or `optimizer.step()`\n",
    "   - This makes evaluation much faster\n",
    "\n",
    "3. **Accumulate Metrics**:\n",
    "   - `test_loss`: Sum of all batch losses (then averaged)\n",
    "   - `correct`: Count of correctly classified samples\n",
    "\n",
    "4. **Loss Aggregation**: `reduction=\"sum\"`\n",
    "   - Sums loss over all samples (not average per batch)\n",
    "   - Then divide by total samples for mean loss\n",
    "\n",
    "5. **Prediction Extraction**: `output.data.max(1, keepdim=True)[1]`\n",
    "   - `max(1, ...)`: Find max value along dimension 1 (class dimension)\n",
    "   - `[1]`: Get indices (class labels), not values\n",
    "   - Returns predicted class for each sample\n",
    "\n",
    "6. **Accuracy Calculation**:\n",
    "   - Compare predictions with ground truth: `pred.eq(target)`\n",
    "   - Sum correct predictions and divide by total samples\n",
    "\n",
    "**Best Practice:** Always evaluate on a held-out test set to detect overfitting!\n",
    "- Training accuracy might be high but test accuracy low ‚Üí overfitting\n",
    "- Both low ‚Üí underfitting (need more capacity or training)\n",
    "- Both high ‚Üí good generalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6d4a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### CNNs vs Fully Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26e4eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "A small FullyConnected ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a334b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "n_hidden = 8  # number of hidden units\n",
    "\n",
    "model_fnn = FullyConnected2Layers(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_fnn)))\n",
    "\n",
    "# train for 5 epochs\n",
    "for epoch in range(0, 5):\n",
    "    train(epoch, model_fnn)\n",
    "# test at end of training\n",
    "test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f0953",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "A CNN with the same number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9deacb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "n_features = 6  # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    train(epoch, model_cnn)\n",
    "\n",
    "test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02f418",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "The ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
    "\n",
    "    Use of convolution: Locality and stationarity in images\n",
    "    Pooling: builds in some translation invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88069fa3",
   "metadata": {},
   "source": [
    "### What has our CNN learned ?\n",
    "\n",
    "We will plot the filters of our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "\n",
    "\n",
    "def filters_visualisation(filters, ch=0, allkernels=False, nrow=8, padding=1):\n",
    "    n, c, w, h = filters.shape\n",
    "\n",
    "    if allkernels:\n",
    "        filters = filters.view(n * c, -1, w, h)\n",
    "    elif c != 3:\n",
    "        filters = filters[:, ch, :, :].unsqueeze(dim=1)\n",
    "\n",
    "    rows = np.min((filters.shape[0] // nrow + 1, 64))\n",
    "    grid = utils.make_grid(filters, nrow=nrow, normalize=True, padding=padding)\n",
    "    plt.figure(figsize=(nrow, rows))\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "\n",
    "print(\"Filters visualisation\")\n",
    "for k, module in enumerate(model_cnn.network):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        print(k)\n",
    "        filters = module.weight.clone()\n",
    "        print(filters.shape)\n",
    "        filters_visualisation(filters.cpu(), ch=0, allkernels=True)\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Intermediate data visualisation\")\n",
    "\n",
    "for data, target in test_loader:\n",
    "    inter = data[:1, :, :, :]\n",
    "    print(data.shape)\n",
    "    for k, module in enumerate(model_cnn.network):\n",
    "        if isinstance(module, nn.Flatten):\n",
    "            break\n",
    "        inter = inter.to(device)\n",
    "        print(k, module.type)\n",
    "        inter = module(inter)\n",
    "        print(inter.shape)\n",
    "        filters_visualisation(inter.cpu(), ch=0, allkernels=True)\n",
    "        plt.axis(\"off\")\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ee82f",
   "metadata": {},
   "source": [
    "### Why Does the CNN Win? Understanding Inductive Bias\n",
    "\n",
    "**Inductive Bias** = assumptions built into your model architecture about the problem structure\n",
    "\n",
    "**CNN's Inductive Biases for Images:**\n",
    "1. **Locality**: Nearby pixels are more related than distant ones\n",
    "   - A 5√ó5 convolution only looks at a small neighborhood\n",
    "   - Makes sense for images: edges, corners, textures are local patterns\n",
    "\n",
    "2. **Stationarity/Translation Equivariance**: Patterns can appear anywhere\n",
    "   - Same filter used at every position (parameter sharing)\n",
    "   - A \"cat ear\" detector works regardless of where the ear appears\n",
    "   - This is why CNNs need far fewer parameters!\n",
    "\n",
    "3. **Hierarchical Composition**: Complex patterns built from simple ones\n",
    "   - Layer 1: Edges and corners\n",
    "   - Layer 2: Textures and simple shapes\n",
    "   - Layer 3: Object parts (eyes, wheels, wings)\n",
    "   - Layer 4+: Full objects\n",
    "\n",
    "**Fully Connected Network's Inductive Bias:**\n",
    "- Essentially none! Treats all input dimensions as independent\n",
    "- More flexible, but requires more data to learn the same patterns\n",
    "- Can learn anything (universal approximator), but less efficient\n",
    "\n",
    "**The Trade-off:**\n",
    "- **Strong inductive bias (CNN)**: Fast learning, data-efficient, BUT suffers when assumptions break\n",
    "- **Weak inductive bias (FC)**: Flexible, robust to assumption violations, BUT needs more data/parameters\n",
    "\n",
    "Let's test this trade-off with an experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c87938",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### What happens when CNNs assumptions are not true ?\n",
    "\n",
    "We will deterministically permute pixels so that the content of an image is respected but not its structure\n",
    "\n",
    "Basically we transform some positions into others, so that the spatial relationship between pixels is not respected anymore.\n",
    "\n",
    "We transform it the same way everytime so that it's a deterministic transformation, not a random shuffling : If we transform the same image, we will get the same results.\n",
    "\n",
    "And we will train networks on this : A CNN (convolutional) and a ANN (fully connected)\n",
    "\n",
    "We will then check what results the two different architectures are getting on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6227a19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28 * 28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy())\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c858003",
   "metadata": {},
   "source": [
    "The figure above demonstrate our \"deterministic permutation\" : In the 1st two rows, you see the dataset with the structured data (the numbers). In the second dataset, the content is the same but the pixels are \"randomly scattered\" so there are now structure.\n",
    "\n",
    "Of course the permutation is deterministic, we always permute the pixels the same way, otherwise nothing would work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d684f5f9",
   "metadata": {},
   "source": [
    "#### Understanding the Pixel Permutation Experiment\n",
    "\n",
    "**What we did:**\n",
    "- Created a deterministic permutation: pixel position 0 ‚Üí position 314, position 1 ‚Üí position 621, etc.\n",
    "- Applied the SAME permutation to every image consistently\n",
    "- The permutation is random but fixed (not different for each image!)\n",
    "\n",
    "**Why deterministic?**\n",
    "- If we used different random permutations per image, the task becomes impossible\n",
    "- With a fixed permutation, there's still a consistent mapping to learn\n",
    "- The network can still learn \"if pixel 314 is bright AND pixel 621 is dark, then class 3\"\n",
    "\n",
    "**What's different in the scrambled data?**\n",
    "- **Content preserved**: All the same pixel values exist, just rearranged\n",
    "- **Spatial structure destroyed**: Neighboring pixels in the original are now far apart\n",
    "- **CNN assumptions violated**: Locality no longer valid (nearby pixels in scrambled image are unrelated)\n",
    "- **FC assumptions unchanged**: FC networks don't care about pixel order anyway!\n",
    "\n",
    "**The Hypothesis:**\n",
    "- CNN should perform WORSE on scrambled data (inductive bias doesn't help)\n",
    "- FC should perform the SAME on both (no spatial assumptions to break)\n",
    "\n",
    "Let's see if this hypothesis holds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f134056",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "First, let's train a CNN on the scrambled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f120fc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "n_features = 6  # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    train(epoch, model_cnn, perm)\n",
    "\n",
    "test(model_cnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7d42e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Then, a fully connected neural nets on the same scrambled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed35f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "n_hidden = 8  # number of hidden units\n",
    "\n",
    "model_fnn = FullyConnected2Layers(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print(\"Number of parameters: {}\".format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    train(epoch, model_fnn, perm)\n",
    "\n",
    "test(model_fnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a340c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Let's now compare the accuracies of 4 neural networks :\n",
    "- CNN with image assumption\n",
    "- FC with image assumption\n",
    "- CNN without image assumption\n",
    "- FC with image assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda69ea5",
   "metadata": {},
   "source": [
    "#### Interpreting the Results: The Power and Limits of Inductive Bias\n",
    "\n",
    "The bar chart below reveals a fundamental principle in machine learning architecture design.\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "1. **CNN on Normal Data (Highest)**: ~85-90% accuracy\n",
    "   - Assumptions match reality ‚Üí maximum benefit from inductive bias\n",
    "   - Learns efficiently with few parameters\n",
    "\n",
    "2. **FC on Normal Data (Good)**: ~75-80% accuracy\n",
    "   - No built-in assumptions, must learn spatial structure from scratch\n",
    "   - Requires more data/parameters to match CNN performance\n",
    "\n",
    "3. **FC on Scrambled Data (Same as #2)**: ~75-80% accuracy\n",
    "   - **Key insight**: Performance unchanged!\n",
    "   - No spatial assumptions to violate\n",
    "   - Treats pixels as independent features regardless of order\n",
    "\n",
    "4. **CNN on Scrambled Data (Worst)**: ~60-70% accuracy\n",
    "   - **Critical drop**: Inductive bias now hurts!\n",
    "   - Convolutions assume nearby pixels are related (they're not anymore)\n",
    "   - Parameter sharing means it can't learn independent pixel relationships\n",
    "\n",
    "**The Fundamental Trade-off:**\n",
    "\n",
    "| Architecture | Assumptions | When Good | When Bad |\n",
    "|--------------|-------------|-----------|----------|\n",
    "| **CNN** | Strong spatial priors | Structured grid data (images, audio spectrograms) | Unstructured data, tabular data |\n",
    "| **FC** | No assumptions | Any data type, especially when structure is unknown | Images/sequences (needs more data) |\n",
    "\n",
    "**Practical Takeaways:**\n",
    "\n",
    "1. **Use CNNs for images** - the inductive bias is almost always helpful\n",
    "2. **Don't use CNNs for tabular data** - no spatial structure to exploit\n",
    "3. **Architecture choice encodes assumptions** - choose based on data structure\n",
    "4. **More assumptions ‚â† always better** - assumptions must match reality\n",
    "5. **Data efficiency**: Right assumptions dramatically reduce data needs\n",
    "\n",
    "**Modern Extensions:**\n",
    "- **Vision Transformers (ViT)**: Less inductive bias than CNNs, needs more data but more flexible\n",
    "- **Graph Neural Networks**: Inductive bias for graph-structured data\n",
    "- **Transformers**: Minimal assumptions, sequence order learned from position embeddings\n",
    "\n",
    "This experiment beautifully demonstrates why **architecture matters** in deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231fb72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "**Takeaway messages**\n",
    "\n",
    "The ConvNet's performance drops when we permute the pixels, but the Fully-Connected Network's performance stays the same\n",
    "\n",
    "    ConvNet makes the assumption that pixels lie on a grid and are stationary/local\n",
    "    It loses performance when this assumption is wrong\n",
    "    The fully-connected network does not make this assumption\n",
    "    It does less well when it is true, since it doesn't take advantage of this prior knowledge\n",
    "    But it doesn't suffer when the assumption is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf0880",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(\n",
    "    (\"NN normal\", \"CNN normal\", \"CNN scrambled\", \"NN scrambled\"),\n",
    "    accuracy_list,\n",
    "    width=0.4,\n",
    ")\n",
    "plt.ylim((min(accuracy_list) - 5, 96))\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "for tick in plt.gca().xaxis.get_major_ticks():\n",
    "    tick.label1.set_fontsize(10)\n",
    "plt.title(\"Performance comparison\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ed166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "py312-isae",
   "language": "python",
   "name": "py312-isae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
