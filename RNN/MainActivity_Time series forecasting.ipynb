{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/deep-learning/\">https://supaerodatascience.github.io/deep-learning/</a>\n",
    "Updates by G. Durantin 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series forecasting (EEG data)\n",
    "\n",
    "Today, we'll look at a simple example of time series forecasting. This family of algorithms can be used for a variety of prediction tasks, such as [predicting stock prices](https://arxiv.org/pdf/1911.13288.pdf) and [seizure prediction](https://www.sciencedirect.com/science/article/pii/S001048251830132X). We'll focus on the use of Recurrent Neural Networks in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll use today is a set of EEG readings used to study seizures. You can find the data [here](https://physionet.org/content/chbmit/1.0.0/), and in the `data` directory there is a single patient recording `chb01_chb01_03.edf` which includes a seizure episode. We won't focus on seizure detection today but will instead try to predict neural activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are stored in the European Data Format, https://www.edfplus.info/. We'll use the python library `pyedflib` to read them. To use Colab, uncomment the following lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyedflib\n",
    "#!wget https://github.com/SupaeroDataScience/deep-learning/blob/main/RNN/data/chb01_chb01_03.edf?raw=true\n",
    "#!mkdir data \n",
    "#!mv chb01_chb01_03.edf?raw=true data/chb01_chb01_03.edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pyedflib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading files\n",
    "file_name = os.path.join('data','chb01_chb01_03.edf')\n",
    "f = pyedflib.EdfReader(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = f.signals_in_file\n",
    "labels = f.getSignalLabels()\n",
    "print('%d different signals: %s' % (n, str(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file we are reading contains 23 signals created using the data from electrodes on the scalp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, sharex=True, figsize=(18,5))\n",
    "for i in range(5):\n",
    "    signal = f.readSignal(i)\n",
    "    axs[i].plot(signal[::256])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this single recording contains 23 different signals from different sensors placed on the scalp. We'll focus at predicting a single signal. Our first data processing step is normalizing the data. This is beneficial for recurrent neural network training, but does require domain knowledge. In this case, we know the physical limits of the sensors, so we can use that to normalize the data between $[-1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = f.readSignal(0)\n",
    "signal = 2 * (signal - f.physical_min(0)) / (f.physical_max(0) - f.physical_min(0)) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another preparation often done with timeseries data is to check if it is **stationary**, ie if the mean and variance change over time. To do this, we'll use the [Dickey-Fuller test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test). This test can be very costly to compute, so we'll downsample the data. There are 256 samples per second in this data, so we'll take one sample each second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "p_value = sm.tsa.stattools.adfuller(signal[::256])[1]\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small value means that the timeseries **is stationary**. This isn't a necessary condition for LSTMs, but it will help training. When the timeseries is not stationary (`p_value > 0.05`), it's normal to instead predict the **difference** between timesteps, ie $$y_t - y_{t-1}$$ which can be calculated in numpy using `diff` and in pandas with `shift`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our data into training and test sets, preserving order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_max = 1000\n",
    "data = np.zeros((6, s_max))\n",
    "for i in range(6):\n",
    "    signal = f.readSignal(i)\n",
    "    signal = 2 * (signal - f.physical_min(i)) / (f.physical_max(i) - f.physical_min(i)) - 1.0\n",
    "    data[i, :] = signal[:s_max]\n",
    "train_input = torch.from_numpy(data[:3, :])\n",
    "test_input = torch.from_numpy(data[3:7, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_input.shape)\n",
    "print(test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "for i in range(3):\n",
    "    plt.plot(train_input[i, :500], label='train')\n",
    "for i in range(3):\n",
    "    plt.plot(test_input[i, :500], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a prediction which is\n",
    "+ Univariate: we're using one signal to predict the future of that signal without considering other features. The opposite of this is multivariate, where multiple features are used in prediction (and multiple features can be simultaneously predicted)\n",
    "+ 1-Step: we're predicting one step into the future, which can also be said as having a horizon of one sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll analyse the data and propose some simple baselines. We'll use **iterative prediction**, also known as walk-forward, to predict the next step at each timestep. The first baseline we'll use is called the naive baseline, which simply uses the previous value. We will start with a horizon of 1, meaning we have access to the previous timestep to predict the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the first EEG channel as the signal of interest\n",
    "train_signal = train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation\n",
    "predictions = np.zeros(len(train_signal))\n",
    "for t in range(len(predictions)-horizon):\n",
    "    # make prediction\n",
    "    predictions[t+horizon] = train_signal[t] # naive baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE of the naive baseline :\",\n",
    "      np.sqrt(mean_squared_error(train_signal[horizon:], predictions[:-horizon])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(train_signal[horizon:700], label='target')\n",
    "plt.plot(predictions[:700-horizon], label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a horizon of 1, the naive baseline works well. However, try increasing the horizon beyond 1ms and you'll notice that the performance degrades. Let's see if the historical data beyond one timestep can be useful. We'll start with a windowed approach, using the average of the past `W=5` timesteps as our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros(len(train_signal))\n",
    "w = 50\n",
    "for t in range(w, len(predictions)-horizon):\n",
    "    # make prediction\n",
    "    predictions[t+horizon] = train_signal[t-w:t].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE of the moving average baseline :\",\n",
    "      np.sqrt(mean_squared_error(train_signal[horizon:], predictions[:-horizon])))\n",
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(train_signal[horizon:700], label='target')\n",
    "plt.plot(predictions[:700-horizon], label='prediction')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this captures the general trend of some parts of the data, the naive baseline is still superior in terms of root mean squared error. Another approach is to exponentially decrease the dependency on past predictions, known as exponential smoothing. The rate by which we decrease past predictions is the parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(alpha):\n",
    "    predictions = np.zeros(len(train_signal))\n",
    "    for t in range(len(predictions)-horizon):\n",
    "    # make prediction\n",
    "        predictions[t+horizon] = alpha * train_signal[t] + (1 - alpha) * predictions[t]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_01 = exponential_smoothing(0.1)\n",
    "print(\"MSE of the EMA baseline with alpha=0.1 :\",\n",
    "      np.sqrt(mean_squared_error(train_signal[horizon:], predictions_01[:-horizon])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_05 = exponential_smoothing(0.5)\n",
    "print(\"MSE of the EMA baseline with alpha=0.5 :\",\n",
    "      np.sqrt(mean_squared_error(train_signal[horizon:], predictions_05[:-horizon])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,3))\n",
    "plt.plot(train_signal[horizon:700], label='target')\n",
    "plt.plot(predictions_01[:700-horizon], label='prediction, alpha=0.1')\n",
    "plt.plot(predictions_05[:700-horizon], label='prediction, alpha=0.5')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While lower alpha values help predict the general trend of our data, their RMSE is worse than using very short history. So for this EEG data, we're still struggling to make good use of the historical data to predict future data. Instead of a single parameter for history decay, we'll instead use a recurrent neural network to inform our reliance on memory for prediction, and optimize the network parameters using Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple recurrent neural network layer is very similar to a fully-connected feed-forward neural network layer; it has a set of weights $W_x$ mapping the previous layer $x$ to each neuron of the recurrent layer, a bias term for each neuron, and an activation function. However, a recurrent neural network also has state; specifically, each neuron connects to every other neuron in the same layer with a time delay of 1. This means that a recurrent neural network layer has a second weight matrix $W_s$ of size $n$x$n$, where $n$ is the number of neurons in the recurrent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s_t = \\tanh(W_{x} x + b_{x}  +  W_{s} s_{t-1} + b_{s})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to consider these recurrent connections is by representing the previous activation functions of the recurrent layer as a hidden state, and using that hidden state as input to the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first observe the behavior of a single RNN layer using the `RNNCell` class from PyTorch. The documentation is [here](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNNCell(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signal = torch.randn(1, 1)\n",
    "hidden_state = torch.zeros(1, 10)\n",
    "input_signal, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state = rnn(input_signal, hidden_state)\n",
    "hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output of the `RNNCell` is the new hidden state. In order to predict the next value, we'll use a `Linear` model to map from this hidden state to a single output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn1 = nn.RNNCell(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "        # predict over the different signals\n",
    "        outputs = []\n",
    "        for signal in inp.split(1, dim=1):\n",
    "            hidden = self.rnn1(signal, hidden)\n",
    "            output = self.linear(hidden)\n",
    "            outputs += [output]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(input.size(0), self.hidden_size, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize RNN and its initial recurrent state\n",
    "n_hidden = 16\n",
    "rnn = RNN(1, n_hidden, 1).double()\n",
    "hidden = rnn.init_hidden()\n",
    "\n",
    "#run a prediction over the signal\n",
    "with torch.no_grad():\n",
    "    output, _ = rnn(train_input, hidden)\n",
    "    y = output.detach().numpy()\n",
    "    \n",
    "print(\"MSE of the RNN prediction with random weights :\",\n",
    "      np.sqrt(mean_squared_error(input[0, 1:], y[0, :-1])))\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(input[0, 1:], label='real')\n",
    "plt.plot(y[0, :-1], label='prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't very convincing, but remember that it is random weights. We'll soon get to training, but first let's increase the lookahead of our recurrent neural network. We'll try to predict 5 timesteps ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference with recurrent neural networks is that they depend on the previous state for the current state's computation. Instead of simply prediction $Y = f(x)$ as in feed-forward neural networks, recurrent networks do $Y_1 = f(x_1, f(x_0))$. Here's what an example \"unrolled\" RNN looks like, where the hidden state is carried over to next timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unrolled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 1\n",
    "    \n",
    "Include a look-ahead of `future` timesteps in your RNN model by completing the class below. Make sure to continuously update the `hidden` state.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(3, n_hidden, 3).double()\n",
    "hidden=rnn.init_hidden()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, hidden = rnn(train_input, hidden, future=horizon)\n",
    "    y = output.detach().numpy()\n",
    "\n",
    "print(\"MSE of the RNN prediction with random weights :\",\n",
    "      np.sqrt(mean_squared_error(train_input[0, horizon:], y[0, horizon:-horizon+1])))\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(train_input[0, horizon:], label='real')\n",
    "plt.plot(y[0, horizon:-horizon], label='prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network doesn't do very well, but it's using random weights. In order to train it, we'll need to calcuate the gradient throughout the iterative process. This is known as **backpropagation through time**, and it relies on the computation of not just the current timestep, but all previous timesteps as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bptt.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(3, n_hidden, 3).double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.LBFGS(rnn.parameters(), lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training step through all data\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    hidden = rnn.init_hidden()\n",
    "    out, hidden = rnn(train_input[:, :-horizon], hidden, future=horizon)\n",
    "    loss = criterion(out[:, :-horizon+1], train_input[:, horizon:])\n",
    "    print('loss:', loss.item())\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden=rnn.init_hidden()\n",
    "with torch.no_grad():\n",
    "    output, hidden = rnn(train_input, hidden, future=horizon)\n",
    "    y = output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_input[0, horizon:]\n",
    "print(\"MSE of the RNN prediction after training:\",\n",
    "      np.sqrt(mean_squared_error(target, y[0, :len(target)])))\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(target, label='real')\n",
    "plt.plot(y[0, :len(target)], label='prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden=rnn.init_hidden()\n",
    "with torch.no_grad():\n",
    "    output, hidden = rnn(test_input, hidden, future=horizon)\n",
    "    y = output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = test_input[0, horizon:]\n",
    "print(\"MSE of the RNN prediction after training:\",\n",
    "      np.sqrt(mean_squared_error(target, y[0, :len(target)])))\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(target, label='real')\n",
    "plt.plot(y[0, :len(target)], label='prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 2:\n",
    "\n",
    "Increase the look-ahead of your model during training. Make sure to align the `target` data with the output of your model (ie, advanced by `N` timesteps). How far ahead can your model predict with good accuracy?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex recurrent neural network, which actually predates much of the current era of deep learning, is called the Long Short-Term Memory unit. This unit has multiple different internal states which are transformed to retain only the pertinent information.\n",
    "\n",
    "Gers, Felix A., JÃ¼rgen Schmidhuber, and Fred Cummins. \"Learning to forget: Continual prediction with LSTM.\" (1999): 850-855."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/lstm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1}+b_i)\\\\\n",
    "f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1}+b_f)\\\\\n",
    "c_t = f_tc_{t-1}+i_t\\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\\\\\n",
    "o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_{t-1}+b_o)\\\\\n",
    "h_t = o_t\\tanh(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercise 3\n",
    "\n",
    "Modify your model from part exercise 2 to use the PyTorch `LSTMCell` classes. Be aware of the multiple return values from an LSTM - the hidden state and cell state. The documentation is [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html). Compare your test prediction error against the RNN model and against the naive baseline. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm1 = nn.LSTMCell(1, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, inp, h_t, c_t, future=0):\n",
    "        outputs = []\n",
    "        for signal in inp.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(signal, (h_t, c_t))\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "        for i in range(1, future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs, h_t, c_t\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.input_size, self.hidden_size, dtype=torch.double),\n",
    "                torch.zeros(self.input_size, self.hidden_size, dtype=torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(3, n_hidden, 3).double()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.LBFGS(lstm.parameters(), lr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training step through all data\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    h_t, c_t = lstm.init_hidden()\n",
    "    out, h_t, c_t = lstm(train_input[:, :-horizon], h_t, c_t, future=horizon)\n",
    "    loss = criterion(out[:, :-horizon+1], train_input[:, horizon:])\n",
    "    print('loss:', loss.item())\n",
    "    loss.backward()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t, c_t = lstm.init_hidden()\n",
    "with torch.no_grad():\n",
    "    outputs, h_t, c_t = lstm(train_input, h_t, c_t, future=horizon)\n",
    "    y = outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_input[0, horizon:]\n",
    "print(\"MSE of the LSTM prediction after training:\",\n",
    "      np.sqrt(mean_squared_error(target, y[0, :len(target)])))\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(target, label='real')\n",
    "plt.plot(y[0, :len(target)], label='prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t, c_t = lstm.init_hidden()\n",
    "with torch.no_grad():\n",
    "    outputs, h_t, c_t = lstm(test_input, h_t, c_t, future=horizon)\n",
    "    y = outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = test_input[0, horizon:]\n",
    "print(\"MSE of the LSTM prediction after training:\",\n",
    "      np.sqrt(mean_squared_error(target, y[0, :len(target)])))\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(target, label='real')\n",
    "plt.plot(y[0, :len(target)], label='prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Truncated BPTT\n",
    "\n",
    "So far, we've been iterating through the entire dataset each epoch. In order to split the data into batches, what would need to be done? How could the hidden state of your recurrent networks be preserved between batches?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Bonus Exercise\n",
    "\n",
    "Another recurrent layer type is the Gated Recurrent Unit. It was designed to solve the issue of vanishing gradients in LSTMs. [Here](https://arxiv.org/pdf/1412.3555.pdf) is an empirical study of the layer types. Try replacing your LSTM with a GRU to see if it changes your results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to know more about time-series prediction, [this notebook](https://github.com/marcopeix/stock-prediction/blob/master/Stock%20Prediction.ipynb) gives an example of more complex classical prediction models, notably Seasonal ARIMA.\n",
    "\n",
    "In the next class, we'll see one of the most popular uses of LSTMs currently, in the context of [Natural Language Processing](https://arxiv.org/pdf/1810.04805.pdf)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
